DOCUMENT: TOOLS_Consulting_Methods_v1.txt
CATEGORY: Tools
TOPICS: consulting, methods, problem-solving, workshops, communication, quality

CONSULTING TOOLS AND METHODS
Version 1.0 | Enterprise AI Platform
Last Updated: December 2024

OVERVIEW

This document provides consulting tools, methods, and best practices for delivering high-quality strategic analysis and recommendations.

STRUCTURED PROBLEM SOLVING

Issue Tree Development

Purpose: Break complex problems into mutually exclusive, collectively exhaustive components.

Method:
- Define the core question as root
- Break into 3-5 major branches (MECE)
- Further decompose each branch
- Identify data requirements for each leaf
- Prioritize branches by impact and testability

Example Structure for Profitability Question

The core question is: How can we increase profitability?

The first major branch is Increase Revenue. Under Increase Revenue, there are two sub-branches. The first sub-branch is Grow Volume, which further breaks down into New Customers and Existing Customer Frequency. The second sub-branch is Improve Pricing, which breaks down into Price Optimization and Mix Improvement.

The second major branch is Reduce Costs. Under Reduce Costs, there are two sub-branches. The first sub-branch is Variable Costs, which breaks down into COGS and Variable OpEx. The second sub-branch is Fixed Costs, which breaks down into Overhead and Structural costs.

Hypothesis-Driven Approach

Purpose: Focus analysis on testable hypotheses rather than boiling the ocean.

Method:
- Form initial hypothesis based on available information
- Identify what would need to be true for hypothesis to hold
- Design analyses to test each condition
- Revise hypothesis based on findings
- Iterate until confident in conclusion

Example:
Initial Hypothesis: 'Revenue decline is driven by customer churn'
Conditions to Test:
- Customer count has decreased
- Revenue per customer is stable
- Churn rate exceeds historical norms
- Acquisition is not offsetting churn

80/20 Analysis

Purpose: Focus on the vital few rather than trivial many.

Application:
- 80% of revenue from 20% of customers
- 80% of costs from 20% of activities
- 80% of problems from 20% of root causes
- 80% of results from 20% of efforts

Method:
- Rank items by impact metric
- Calculate cumulative percentage
- Identify the 20% driving 80% of impact
- Focus analysis and recommendations on these items

ANALYSIS TECHNIQUES

Sizing and Estimation

Market Sizing (TAM/SAM/SOM):
- TAM: Total Addressable Market (entire market if 100% share)
- SAM: Serviceable Addressable Market (segment you can reach)
- SOM: Serviceable Obtainable Market (realistic near-term share)

Approach Options:
- Top-Down: Start with total market, apply filters
- Bottom-Up: Build from unit economics and capacity
- Value-Based: Calculate based on value delivered

Triangulation: Use multiple approaches and reconcile differences.

Benchmarking

Purpose: Compare performance against relevant reference points.

Types:
- Internal: Across business units, regions, time periods
- Competitive: Against direct competitors
- Functional: Best practices from any industry
- Generic: Industry averages and standards

Process:
- Select metrics to benchmark
- Identify benchmark sources
- Collect and normalize data
- Analyze gaps
- Identify root causes
- Develop action plan

Root Cause Analysis

5 Whys Method:
- State the problem
- Ask 'Why?'
- For each answer, ask 'Why?' again
- Repeat 5 times (or until root cause identified)
- Verify root cause through evidence

Fishbone (Ishikawa) Categories (6Ms):
- Methods: Processes and procedures
- Machines: Equipment and technology
- Materials: Inputs and supplies
- Measurements: Metrics and standards
- Mother Nature: Environmental factors
- Manpower: People and skills

RECOMMENDATION DEVELOPMENT

Prioritization Frameworks

Impact vs. Effort Matrix:
- High Impact, Low Effort: Quick wins (do first)
- High Impact, High Effort: Major projects (plan carefully)
- Low Impact, Low Effort: Fill-ins (do if time permits)
- Low Impact, High Effort: Time sinks (avoid)

ICE Scoring:
- Impact: Expected outcome (1-10)
- Confidence: Certainty of success (1-10)
- Ease: Simplicity of implementation (1-10)
- Score = Impact x Confidence x Ease

RICE Scoring:
- Reach: Number of people/units affected
- Impact: Degree of impact per person (1-3)
- Confidence: Certainty (0.5-1.0)
- Effort: Person-months required
- Score = (Reach x Impact x Confidence) / Effort

Action Specification

SMART Criteria:
- Specific: Clear and unambiguous
- Measurable: Quantifiable outcomes
- Achievable: Realistic given constraints
- Relevant: Aligned to objectives
- Time-bound: Clear deadline

Recommendation Template Elements

For each recommendation, include the following elements. The What element describes the specific action to take. The Why element provides the business case and expected impact. The Who element identifies the responsible owner. The When element specifies the timeline and milestones. The How element outlines the key implementation steps. The Success Metrics element defines how we know it worked.

COMMUNICATION FRAMEWORKS

Pyramid Principle

Structure: Lead with answer, then support with evidence.

Top Level: Main recommendation or conclusion
Second Level: 3-5 supporting arguments
Third Level: Evidence and data for each argument

Benefits:
- Busy executives get the point immediately
- Logical structure builds conviction
- Allows depth based on audience interest

SCQA Framework

Situation: Context the audience knows
Complication: The change or challenge
Question: What should we do?
Answer: The recommendation

Example:
'We launched in three new markets last year (Situation). Sales in all three are below plan by 40% (Complication). Should we double down, adjust strategy, or exit? (Question) We recommend doubling down in Market A, adjusting in B, and exiting C because... (Answer)'

So What Test

For every statement, ask 'So what?'
Continue until you reach actionable insight.

Weak: 'Customer satisfaction declined 5 points'
Better: '...which correlates with increased churn'
Best: '...suggesting we should prioritize service recovery to protect revenue'

WORKSHOP FACILITATION

Workshop Types

Strategy Development workshops are for setting direction. They typically run 1-2 days and produce strategic priorities as the primary output.

Planning Session workshops are for creating action plans. They typically run a half day and produce a detailed roadmap as the primary output.

Ideation and Brainstorming workshops are for generating options. They typically run 2-3 hours and produce prioritized ideas as the primary output.

Assessment Review workshops are for evaluating findings. They typically run a half day and produce aligned conclusions as the primary output.

Problem Solving workshops are for resolving issues. They typically run 2-4 hours and produce a solution and action plan as the primary output.

Training and Enablement workshops are for building capability. Duration varies based on content, and skills transfer is the primary output.

Facilitation Best Practices

Before:
- Define clear objectives and outcomes
- Design agenda with time blocks
- Prepare materials and pre-work
- Ensure right participants invited
- Set up room and technology

During:
- Start with objectives and ground rules
- Manage time actively
- Ensure balanced participation
- Capture decisions and actions
- Park off-topic items
- Summarize frequently

After:
- Distribute notes within 24 hours
- Confirm action items and owners
- Schedule follow-ups
- Gather feedback on session

Brainstorming Techniques

Brainwriting: Silent written ideation before discussion
Round Robin: Each person contributes in turn
Crazy 8s: 8 ideas in 8 minutes
Affinity Grouping: Cluster ideas into themes
Dot Voting: Prioritize with limited votes

QUALITY STANDARDS

Deliverable Quality Checklist

Content:
- Hypothesis is clearly stated
- Logic flow is MECE
- Data is accurate and sourced
- Analysis supports conclusions
- Recommendations are actionable

Presentation:
- Executive summary captures key points
- Charts are properly labeled
- Tables are scannable
- Text is concise
- Formatting is consistent

Confidence Levels

HIGH: Strong data support, proven methodology, multiple corroborating sources. Present with conviction.

MEDIUM: Good support with some assumptions, reasonable extrapolation from known data. Note assumptions.

MEDIUM-LOW: Limited data, inference from related situations. Flag uncertainty and recommend validation.

LOW: Speculation, insufficient data. Recommend further research before acting.

INFORMATION GAPS

Six Options When Information Is Missing:
- Make reasonable assumption (state it clearly)
- Ask clarifying question to client
- Present options for client decision
- Search internal knowledge base
- Conduct web research
- Acknowledge limitation honestly

Prefer gathering information over assumptions when possible.

END OF DOCUMENT
