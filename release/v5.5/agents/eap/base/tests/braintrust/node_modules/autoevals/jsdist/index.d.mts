import { Scorer, ScorerArgs, Score } from '@braintrust/core';
export { Score, Scorer, ScorerArgs } from '@braintrust/core';
import { ChatCompletion, ChatCompletionMessageParam, ChatCompletionTool, ChatCompletionToolChoiceOption } from 'openai/resources';

interface CachedLLMParams {
    model: string;
    messages: ChatCompletionMessageParam[];
    tools?: ChatCompletionTool[];
    tool_choice?: ChatCompletionToolChoiceOption;
    temperature?: number;
    max_tokens?: number;
}
interface ChatCache {
    get(params: CachedLLMParams): Promise<ChatCompletion | null>;
    set(params: CachedLLMParams, response: ChatCompletion): Promise<void>;
}
interface OpenAIAuth {
    openAiApiKey?: string;
    openAiOrganizationId?: string;
    openAiBaseUrl?: string;
    openAiDefaultHeaders?: Record<string, string>;
    openAiDangerouslyAllowBrowser?: boolean;
}
declare global {
    var __inherited_braintrust_wrap_openai: ((openai: any) => any) | undefined;
}

declare const templates: {
    battle: string;
    closed_q_a: string;
    factuality: string;
    humor: string;
    possible: string;
    security: string;
    sql: string;
    summary: string;
    translation: string;
};

interface ScorerWithPartial<Output, Extra> extends Scorer<Output, Extra> {
    partial: <T extends keyof Extra>(args: {
        [K in T]: Extra[K];
    }) => Scorer<Output, Omit<Extra, T> & Partial<Pick<Extra, T>>>;
}
declare function makePartial<Output, Extra>(fn: Scorer<Output, Extra>, name?: string): ScorerWithPartial<Output, Extra>;

type LLMArgs = {
    maxTokens?: number;
    temperature?: number;
} & OpenAIAuth;
declare const DEFAULT_MODEL = "gpt-4o";
declare function buildClassificationTools(useCoT: boolean, choiceStrings: string[]): ChatCompletionTool[];
type OpenAIClassifierArgs<RenderArgs> = {
    name: string;
    model: string;
    messages: ChatCompletionMessageParam[];
    choiceScores: Record<string, number>;
    classificationTools: ChatCompletionTool[];
    cache?: ChatCache;
} & LLMArgs & RenderArgs;
declare function OpenAIClassifier<RenderArgs, Output>(args: ScorerArgs<Output, OpenAIClassifierArgs<RenderArgs>>): Promise<Score>;
type LLMClassifierArgs<RenderArgs> = {
    model?: string;
    useCoT?: boolean;
} & LLMArgs & RenderArgs;
declare function LLMClassifierFromTemplate<RenderArgs>({ name, promptTemplate, choiceScores, model, useCoT: useCoTArg, temperature, }: {
    name: string;
    promptTemplate: string;
    choiceScores: Record<string, number>;
    model?: string;
    useCoT?: boolean;
    temperature?: number;
}): Scorer<string, LLMClassifierArgs<RenderArgs>>;
interface ModelGradedSpec {
    prompt: string;
    choice_scores: Record<string, number>;
    model?: string;
    use_cot?: boolean;
    temperature?: number;
}
declare function LLMClassifierFromSpec<RenderArgs>(name: string, spec: ModelGradedSpec): Scorer<any, LLMClassifierArgs<RenderArgs>>;
declare function LLMClassifierFromSpecFile<RenderArgs>(name: string, templateName: keyof typeof templates): Scorer<any, LLMClassifierArgs<RenderArgs>>;
/**
 * Test whether an output _better_ performs the `instructions` than the original
 * (expected) value.
 */
declare const Battle: ScorerWithPartial<string, LLMClassifierArgs<{
    instructions: string;
}>>;
/**
 * Test whether an output answers the `input` using knowledge built into the model.
 * You can specify `criteria` to further constrain the answer.
 */
declare const ClosedQA: ScorerWithPartial<string, LLMClassifierArgs<{
    input: string;
    criteria: any;
}>>;
/**
 * Test whether an output is funny.
 */
declare const Humor: ScorerWithPartial<string, LLMClassifierArgs<{}>>;
/**
 * Test whether an output is factual, compared to an original (`expected`) value.
 */
declare const Factuality: ScorerWithPartial<string, LLMClassifierArgs<{
    input: string;
    output: string;
    expected?: string | undefined;
}>>;
/**
 * Test whether an output is a possible solution to the challenge posed in the input.
 */
declare const Possible: ScorerWithPartial<string, LLMClassifierArgs<{
    input: string;
}>>;
/**
 * Test whether an output is malicious.
 */
declare const Security: ScorerWithPartial<string, LLMClassifierArgs<{}>>;
/**
 * Test whether a SQL query is semantically the same as a reference (output) query.
 */
declare const Sql: ScorerWithPartial<string, LLMClassifierArgs<{
    input: string;
}>>;
/**
 * Test whether an output is a better summary of the `input` than the original (`expected`) value.
 */
declare const Summary: ScorerWithPartial<string, LLMClassifierArgs<{
    input: string;
}>>;
/**
 * Test whether an `output` is as good of a translation of the `input` in the specified `language`
 * as an expert (`expected`) value.
 */
declare const Translation: ScorerWithPartial<string, LLMClassifierArgs<{
    language: string;
    input: string;
}>>;

/**
 * A simple scorer that uses the Levenshtein distance to compare two strings.
 */
declare const Levenshtein: ScorerWithPartial<string, {}>;
declare const LevenshteinScorer: ScorerWithPartial<string, {}>;
/**
 * A scorer that uses cosine similarity to compare two strings.
 *
 * @param args
 * @param args.prefix A prefix to prepend to the prompt. This is useful for specifying the domain of the inputs.
 * @param args.model The model to use for the embedding distance. Defaults to "text-embedding-ada-002".
 * @param args.expectedMin The minimum expected score. Defaults to 0.7. Values below this will be scored as 0, and
 * values between this and 1 will be scaled linearly.
 * @returns A score between 0 and 1, where 1 is a perfect match.
 */
declare const EmbeddingSimilarity: ScorerWithPartial<string, {
    prefix?: string;
    expectedMin?: number;
    model?: string;
} & OpenAIAuth>;

/**
 * A scorer that semantically evaluates the overlap between two lists of strings. It works by
 * computing the pairwise similarity between each element of the output and the expected value,
 * and then using Linear Sum Assignment to find the best matching pairs.
 */
declare const ListContains: ScorerWithPartial<string[], {
    pairwiseScorer?: Scorer<string, {}>;
    allowExtraEntities?: boolean;
}>;

/**
 * A scorer that uses OpenAI's moderation API to determine if AI response contains ANY flagged content.
 *
 * @param args
 * @param args.threshold Optional. Threshold to use to determine whether content has exceeded threshold. By
 * default, it uses OpenAI's default. (Using `flagged` from the response payload.)
 * @param args.categories Optional. Specific categories to look for. If not set, all categories will
 * be considered.
 * @returns A score between 0 and 1, where 1 means content passed all moderation checks.
 */
declare const Moderation: ScorerWithPartial<string, {
    threshold?: number;
} & OpenAIAuth>;

/**
 * A simple scorer that compares numbers by normalizing their difference.
 */
declare const NumericDiff: ScorerWithPartial<number, {}>;

/**
 * A simple scorer that compares JSON objects, using a customizable comparison method for strings
 * (defaults to Levenshtein) and numbers (defaults to NumericDiff).
 */
declare const JSONDiff: ScorerWithPartial<any, {
    stringScorer?: Scorer<string, {}>;
    numberScorer?: Scorer<number, {}>;
}>;
/**
 * A binary scorer that evaluates the validity of JSON output, optionally validating against a
 * JSON Schema definition (see https://json-schema.org/learn/getting-started-step-by-step#create).
 */
declare const ValidJSON: ScorerWithPartial<string, {
    schema?: any;
}>;

type RagasArgs = {
    input?: string;
    context?: string | string[];
    model?: string;
} & LLMArgs;
/**
 * Estimates context recall by estimating TP and FN using annotated answer and
 * retrieved context.
 */
declare const ContextEntityRecall: ScorerWithPartial<string, RagasArgs & {
    pairwiseScorer?: Scorer<string, {}>;
}>;
declare const ContextRelevancy: ScorerWithPartial<string, RagasArgs>;
declare const ContextRecall: ScorerWithPartial<string, RagasArgs>;
declare const ContextPrecision: ScorerWithPartial<string, RagasArgs>;
/**
 * Measures factual consistency of the generated answer with the given context.
 */
declare const Faithfulness: ScorerWithPartial<string, RagasArgs>;
/**
 * Scores the relevancy of the generated answer to the given question.
 * Answers with incomplete, redundant or unnecessary information are penalized.
 */
declare const AnswerRelevancy: ScorerWithPartial<string, RagasArgs & {
    strictness?: number;
}>;
/**
 * Scores the semantic similarity between the generated answer and ground truth.
 */
declare const AnswerSimilarity: ScorerWithPartial<string, RagasArgs>;
/**
 * Measures answer correctness compared to ground truth using a weighted
 * average of factuality and semantic similarity.
 */
declare const AnswerCorrectness: ScorerWithPartial<string, RagasArgs & {
    factualityWeight?: number;
    answerSimilarityWeight?: number;
    answerSimilarity?: Scorer<string, {}>;
}>;

interface AutoevalMethod {
    method: ScorerWithPartial<any, any>;
    description: string;
}
declare const Evaluators: {
    label: string;
    methods: AutoevalMethod[];
}[];

export { AnswerCorrectness, AnswerRelevancy, AnswerSimilarity, Battle, ClosedQA, ContextEntityRecall, ContextPrecision, ContextRecall, ContextRelevancy, DEFAULT_MODEL, EmbeddingSimilarity, Evaluators, Factuality, Faithfulness, Humor, JSONDiff, type LLMArgs, type LLMClassifierArgs, LLMClassifierFromSpec, LLMClassifierFromSpecFile, LLMClassifierFromTemplate, Levenshtein, LevenshteinScorer, ListContains, type ModelGradedSpec, Moderation, NumericDiff, OpenAIClassifier, type OpenAIClassifierArgs, Possible, type ScorerWithPartial, Security, Sql, Summary, Translation, ValidJSON, buildClassificationTools, makePartial, templates };
