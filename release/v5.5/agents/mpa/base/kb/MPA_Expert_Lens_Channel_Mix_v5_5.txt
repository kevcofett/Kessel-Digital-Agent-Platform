DOCUMENT: MPA_Expert_Lens_Channel_Mix_v5_5.txt
CATEGORY: Expert Lens
TOPICS: channel mix, media mix, channel interdependencies, platform selection, allocation strategy

MPA EXPERT LENS: CHANNEL MIX AND INTERDEPENDENCIES
VERSION: 5.5
DATE: January 2026
STATUS: Production Ready
COMPLIANCE: 6-Rule Compliant
================================================================================
SECTION 1: PURPOSE AND SCOPE
================================================================================

1.1 PURPOSE

This document provides diagnostic reasoning patterns and expert-level guidance for channel mix decisions. The agent MUST use this document to identify issues a novice might miss, surface channel interdependencies, and ask questions that strengthen the media plan.

1.2 SCOPE

This document covers:
- Critical channel considerations that determine success
- Common pitfalls with recommended alternatives
- Diagnostic signals indicating problems
- Channel interdependency mapping
- Questions worth asking that users may not consider
- Benchmark reference points for channel selection

================================================================================
SECTION 2: WHAT MATTERS MOST
================================================================================

2.1 THE FIVE CRITICAL CHANNEL CONSIDERATIONS

CONSIDERATION 1: OBJECTIVE ALIGNMENT
Each channel excels at different objectives. Search captures intent. Video builds awareness. Social drives consideration. Retail media captures purchase.

Why it matters: Misaligned channels waste budget on activities that do not support the goal. A brand awareness objective with 80 percent search budget will underdeliver.

CONSIDERATION 2: CHANNEL INTERDEPENDENCIES
Channels do not operate in isolation. Upper funnel affects lower funnel efficiency. Similar platforms compete for the same audiences. Some channels cannibalize others.

Why it matters: Ignoring interdependencies leads to either overinvestment through double-counting benefits or missed synergies through failing to capture combined effects.

CONSIDERATION 3: MINIMUM VIABLE INVESTMENT
Each channel needs sufficient budget to optimize and generate reliable data. Below-threshold investment wastes money on noise.

Why it matters: Underfunded channels never exit learning phase, cannot optimize, and produce unreliable data for decisions.

CONSIDERATION 4: AUDIENCE OVERLAP
Different platforms reach overlapping audiences. Without coordination, combined frequency becomes excessive or targeting becomes redundant.

Why it matters: Unmanaged overlap wastes budget on repeated reach to the same users while missing incremental audience.

CONSIDERATION 5: MEASUREMENT CAPABILITY
Different channels have different measurement quality. Some allow clean attribution, others require modeling. Some support incrementality testing, others do not.

Why it matters: Channel selection affects measurement reliability. Heavy investment in hard-to-measure channels requires alternative validation approaches.

================================================================================
SECTION 3: COMMON PITFALLS
================================================================================

3.1 PITFALL: CHANNEL SELECTION BY COMFORT

What it looks like:
- Heavy investment in familiar platforms
- Avoidance of newer or less familiar channels
- Mix unchanged from previous campaigns

Why it happens:
- Team expertise concentrated in certain platforms
- Existing relationships with specific vendors
- Risk aversion to new approaches

Why it matters:
- May miss more efficient channels
- Audience behavior evolves faster than habits
- Competitive advantage lost to more adaptive competitors

Better approach:
- Evaluate channels based on audience and objective fit
- Dedicate testing budget to unfamiliar channels
- Build capability in emerging platforms

3.2 PITFALL: CHASING PLATFORM-REPORTED ROAS

What it looks like:
- Budget concentrated in channels with highest reported ROAS
- Neglect of channels with harder attribution
- Mix driven by dashboard metrics

Why it happens:
- ROAS is readily available and easily compared
- Pressure to justify spend with immediate returns
- Lack of incrementality validation

Why it matters:
- Platform-reported ROAS typically overstates true value
- Lower-funnel channels get credit for upper-funnel work
- Mix optimizes for measurement, not outcomes

Better approach:
- Validate ROAS with incrementality testing
- Use MER to evaluate full portfolio
- Weight allocation decisions toward validated channels

3.3 PITFALL: IGNORING CANNIBALIZATION

What it looks like:
- Simultaneous investment in overlapping channels
- No suppression between platforms
- Credit given to multiple channels for same conversion

Why it happens:
- Each channel managed in isolation
- Measurement systems do not talk to each other
- Incrementality testing not implemented

Why it matters:
- Budget efficiency overstated
- True channel contribution unknown
- May be paying multiple times for same outcome

Better approach:
- Map audience overlap between channels
- Implement cross-platform suppression where possible
- Test incrementality of questionable channels

3.4 PITFALL: COPYING COMPETITOR MIX

What it looks like:
- Channel allocation mimics visible competitor activity
- Rationale is they must know something we do not
- No consideration of strategic differences

Why it happens:
- Competitive intelligence is available
- Assumption that success can be replicated
- Uncertainty about own strategy

Why it matters:
- Competitor may be making mistakes
- Their objectives and constraints differ
- Their results are unknown to you

Better approach:
- Understand competitor presence as context, not strategy
- Base mix on your objectives and audience
- Test differentiated approaches for competitive advantage

3.5 PITFALL: ALL AI-OPTIMIZED CAMPAIGNS

What it looks like:
- Entire budget in Performance Max, Advantage Plus, or similar
- No manual campaigns for comparison or control
- Complete reliance on platform algorithms

Why it happens:
- Platform recommendations emphasize AI campaigns
- Early results often look strong
- Reduces manual optimization work

Why it matters:
- Limited transparency into what is working
- Cannot diagnose issues when performance declines
- No benchmark for true AI contribution

Better approach:
- Maintain manual campaigns alongside AI campaigns
- Validate AI performance with incrementality testing
- Keep some channels with full transparency

================================================================================
SECTION 4: DIAGNOSTIC SIGNALS
================================================================================

4.1 IF LOWER FUNNEL EFFICIENCY IS DECLINING

What to check:
- Has upper funnel investment changed
- Is competitor activity increasing
- Has audience targeting narrowed

Questions to ask:
- When did efficiency start declining
- What else changed at that time
- What are brand search and organic traffic trends

Typical implications:
- May indicate demand pool depletion
- Consider increasing upper funnel investment
- Validate with search volume and consideration metrics

4.2 IF NEW-TO-BRAND RATE IS DROPPING

What to check:
- Is campaign over-optimizing to existing customers
- Are AI campaigns finding easy conversions
- Is retargeting pool shrinking

Questions to ask:
- What is the trend over time
- How is new-to-brand defined and measured
- What percentage of budget is prospecting versus retargeting

Typical implications:
- Campaign may be harvesting existing customers
- AI algorithms often optimize for easy wins
- Need to expand targeting or increase prospecting

4.3 IF CREATIVE PERFORMANCE VARIES WILDLY BY CHANNEL

What to check:
- Is creative optimized for each platform
- Are audience expectations different by channel
- Is message-channel fit considered

Questions to ask:
- Was creative developed for specific channels or adapted
- What are the format differences between platforms
- Have you tested channel-specific creative

Typical implications:
- Cross-platform creative often underperforms
- Investment in channel-specific assets improves results
- May need to adjust channel allocation based on creative strength

4.4 IF ONE CHANNEL DOMINATES RESULTS

What to check:
- Is attribution giving full credit appropriately
- Are other channels contributing but unattributed
- Would removing dominant channel maintain results

Questions to ask:
- How confident are you in this channel attribution
- What happens to other channels when this one is paused
- Have you tested this channel incrementality

Typical implications:
- Dominant channel may be getting assist credit
- Other channels may be essential but undervalued
- Incrementality testing needed before major shifts

4.5 IF FREQUENCY IS VERY HIGH OR VERY LOW

Very high frequency signals:
- Audience may be too narrow for budget
- Creative fatigue likely occurring
- May be causing negative brand impact

Very low frequency signals:
- Budget may be too spread across platforms
- Audience may be too broad for impact
- May not be reaching threshold for effect

Questions to ask:
- What is cross-platform frequency
- How is audience defined and sized
- What frequency has performed best historically

Typical implications:
- High: Expand audience or consolidate platforms
- Low: Focus budget or narrow targeting
- Always: Consider cross-platform not just single-platform

================================================================================
SECTION 5: CHANNEL INTERDEPENDENCIES
================================================================================

5.1 INTERDEPENDENCY MATRIX

SEARCH AND BRAND INVESTMENT
- Brand advertising increases branded search volume
- Branded search appears more efficient than it is because it would happen anyway
- Cutting brand investment eventually reduces branded search volume
- Monitor: Brand search volume trends, organic traffic

SEARCH AND SHOPPING
- Often compete for same commercial queries
- Shopping typically wins for product searches
- Text ads capture non-product queries
- Monitor: Impression share by query type

META AND TIKTOK
- Similar audiences, especially younger demographics
- Combined frequency often unmanaged
- Creative approaches differ significantly
- Monitor: Cross-platform frequency if measurable

CTV AND DIGITAL VIDEO
- CTV drives awareness that improves all channels
- Effect is delayed 4-8 weeks typically
- Attribution to CTV is challenging
- Monitor: Brand search lift, consideration metrics

RETAIL MEDIA AND BRAND SEARCH
- Retail media captures bottom-funnel on platform
- May cannibalize brand search for same products
- New-to-brand rate critical for evaluation
- Monitor: NTB percentage, brand search cannibalization

RETARGETING AND EMAIL
- Both target existing engaged users
- Overlap is significant
- Incrementality of retargeting often overstated
- Monitor: Holdout test retargeting against email-only

5.2 ADDING A CHANNEL IMPLICATIONS

Immediate effects:
- Budget dilution across existing channels
- New learning period for added channel
- Complexity increase in management

Cross-channel effects:
- May reach incremental audience
- May increase frequency to overlapping audience
- Attribution becomes more complex

Business effects:
- Potential for new customer segments
- Increased reach and presence
- More data sources for learning

Measurement effects:
- Need to integrate new channel in attribution
- Cross-channel effects harder to isolate
- May need new measurement approaches

5.3 REMOVING A CHANNEL IMPLICATIONS

Immediate effects:
- Budget concentration in remaining channels
- Loss of accumulated learning
- Potential reach reduction

Cross-channel effects:
- May lose audience that other channels cannot reach
- May reduce frequency below effective threshold
- Other channels may see efficiency change

Business effects:
- Reduced presence in removed channel
- May lose competitive position
- Simpler execution and focus

Measurement effects:
- Attribution simpler with fewer channels
- Can better isolate remaining channel effects
- May reveal true contribution of removed channel

================================================================================
SECTION 6: QUESTIONS WORTH ASKING
================================================================================

6.1 FOUNDATION QUESTIONS

These questions establish channel strategy context:

- Which channels have driven your best results historically and why (Reveals learning)
- What channels are you required to include regardless of performance (Reveals constraints)
- How do your customers typically discover and evaluate products like yours (Reveals journey)
- Where do your competitors invest most heavily (Reveals competitive context)

6.2 INTERDEPENDENCY QUESTIONS

These questions surface channel relationships:

- How do you think about brand advertising and performance advertising working together (Reveals philosophy)
- What happens to your performance channels when you pause or reduce awareness spending (Reveals observed effects)
- Do you manage frequency across platforms or just within each platform (Reveals sophistication)
- Have you ever tested turning off a channel to see what happens (Reveals validation)

6.3 MEASUREMENT QUESTIONS

These questions assess attribution quality:

- How do you attribute conversions across channels today (Reveals methodology)
- Which channels do you trust the attribution for and which do you question (Reveals confidence)
- Have you done incrementality testing on any channels (Reveals validation)
- How do you handle multi-touch journeys (Reveals complexity awareness)

6.4 CAPABILITY QUESTIONS

These questions assess execution readiness:

- Which channels does your team have deep expertise in (Reveals strength)
- Are there channels you would like to test but have not had resources for (Reveals interest)
- How much creative variation can you support across channels (Reveals capacity)
- How quickly can you respond to performance signals by channel (Reveals agility)

================================================================================
SECTION 7: BENCHMARKS AND REFERENCE POINTS
================================================================================

7.1 CHANNEL ROLE BY FUNNEL STAGE

AWARENESS STAGE
- Primary: CTV, YouTube, Broad Digital Video, Audio, OOH
- Secondary: Broad programmatic display, High-reach social
- Characteristics: High reach, lower CPMs, harder attribution

CONSIDERATION STAGE
- Primary: Meta, TikTok, YouTube, LinkedIn for B2B, Native
- Secondary: Content marketing, Influencer, Podcast
- Characteristics: Engagement focus, interest-based targeting

CONVERSION STAGE
- Primary: Paid Search, Shopping, Retail Media, Retargeting
- Secondary: Email, SMS, Brand search
- Characteristics: High intent, clearer attribution, higher CPMs

RETENTION STAGE
- Primary: Email, CRM, SMS
- Secondary: 1P programmatic, Loyalty programs
- Characteristics: Highest efficiency, owned channels

7.2 CHANNEL EFFICIENCY PATTERNS

General efficiency patterns vary by vertical. Use channel seed data for specifics.

- Retargeting typically most efficient but limited scale
- Brand search typically high ROAS but low incrementality
- Prospecting social typically middle efficiency, good scale
- Awareness typically lowest short-term ROAS, highest long-term impact

7.3 TESTING ALLOCATION BENCHMARKS

- Emerging channel testing: 5-10 percent of budget
- Creative testing: 10-20 percent per channel
- Audience testing: 10-15 percent per channel
- Incrementality testing: Dedicated budget, typically 5-10 percent

================================================================================
SECTION 8: FALLBACK BEHAVIOR
================================================================================

8.1 IF CHANNEL PREFERENCES ARE STRONG

When client has fixed channel requirements:
- Work within constraints
- Optimize allocation within required channels
- Propose testing for channels not required
- Document rationale for any concerns

8.2 IF HISTORICAL DATA IS UNAVAILABLE

When no past performance exists:
- Use vertical benchmarks as starting point
- Weight toward established channels with more predictable performance
- Build in testing allocation for learning
- Set expectations for uncertainty in projections

8.3 IF INCREMENTALITY IS UNVALIDATED

When no testing has been done:
- Use benchmark incrementality rates from research
- Flag which channels have highest validation need
- Propose testing plan to validate assumptions
- Weight recommendations toward more measurable channels

8.4 IF CHANNEL EXPERTISE IS LIMITED

When team cannot execute on certain channels:
- Acknowledge capability constraint
- Recommend building capability over time
- Propose simpler channel mix for now
- Identify agency or partner support options

================================================================================
SECTION 9: CROSS-REFERENCES
================================================================================

9.1 RELATED DOCUMENTS

- Channel seed data - Platform details and benchmarks
- AI_ADVERTISING_GUIDE_v5_5.txt - AI campaign guidance
- BRAND_PERFORMANCE_FRAMEWORK_v5_5.txt - Funnel strategy
- RETAIL_MEDIA_NETWORKS_v5_5.txt - Retail media specifics
- MPA_Implications_Channel_Shifts_v5_5_v5_5.txt - Detailed shift implications
- MPA_Supporting_Instructions_v5_5.txt - Communication patterns

9.2 WEB SEARCH TRIGGERS

The agent SHOULD use web search when:
- Platform-specific feature updates needed
- Competitive intelligence requested
- Recent benchmark data required
- Emerging channel capabilities unknown

================================================================================
VERSION HISTORY
================================================================================

Version 1.0 - January 2026 - Initial creation
- Established critical channel considerations
- Documented common pitfalls with better approaches
- Added diagnostic signals for channel issues
- Included interdependency mapping
- Added questions worth asking

================================================================================
END OF DOCUMENT
================================================================================
