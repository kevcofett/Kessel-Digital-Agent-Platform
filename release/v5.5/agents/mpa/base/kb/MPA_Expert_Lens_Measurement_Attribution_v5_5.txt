MPA EXPERT LENS: MEASUREMENT AND ATTRIBUTION
VERSION: 5.5
DATE: January 2026
STATUS: Production Ready
COMPLIANCE: 6-Rule Compliant
================================================================================
SECTION 1: PURPOSE AND SCOPE
================================================================================

1.1 PURPOSE

This document provides diagnostic reasoning patterns and expert-level guidance for measurement and attribution decisions. The agent MUST use this document to identify measurement pitfalls, surface attribution limitations, and ensure plans have valid success criteria.

1.2 SCOPE

This document covers:
- Critical measurement considerations that determine success
- Common pitfalls with recommended alternatives
- Diagnostic signals indicating measurement problems
- Attribution methodology comparison
- Questions worth asking that users may not consider
- Benchmark reference points for measurement setup

================================================================================
SECTION 2: WHAT MATTERS MOST
================================================================================

2.1 THE FIVE CRITICAL MEASUREMENT CONSIDERATIONS

CONSIDERATION 1: WHAT YOU MEASURE DETERMINES WHAT YOU OPTIMIZE
Campaigns optimize toward measured outcomes. Flawed metrics produce flawed optimization. If you measure clicks, you get clicks. If you measure value, you get value.

Why it matters: The wrong primary metric can cause campaigns to optimize away from business objectives while appearing successful.

CONSIDERATION 2: ATTRIBUTION HAS KNOWN BIASES
Every attribution model has limitations. Last-click undervalues awareness. Platform attribution overvalues platform touchpoints. No model is truth.

Why it matters: Understanding biases prevents over-reliance on misleading data. Best practice is triangulation across methods.

CONSIDERATION 3: INCREMENTALITY IS THE GOLD STANDARD
Only incremental conversions represent true campaign value. Many attributed conversions would have occurred anyway through organic channels or existing customer behavior.

Why it matters: Scaling based on attributed results without incrementality validation often produces disappointing business outcomes.

CONSIDERATION 4: MEASUREMENT CAPABILITY VARIES BY CHANNEL
Some channels support clean measurement, others do not. CTV is harder to attribute than search. Brand effects are harder to measure than clicks.

Why it matters: Channel selection should consider measurement quality. Heavy investment in unmeasurable channels requires alternative validation.

CONSIDERATION 5: MEASUREMENT DESIGN AFFECTS LEARNING
Good measurement generates learnings that improve future campaigns. Poor measurement produces data but not insight.

Why it matters: Campaigns should teach you something. Measurement design determines whether you learn or just count.

================================================================================
SECTION 3: COMMON PITFALLS
================================================================================

3.1 PITFALL: ROAS AS PRIMARY KPI

What it looks like:
- Campaign success defined by platform-reported ROAS
- Budget decisions based on channel ROAS comparison
- Scaling to channels with highest reported ROAS

Why it happens:
- ROAS is readily available and intuitive
- Appears to measure what matters (return on spend)
- Platforms prominently feature ROAS in reporting

Why it matters:
- Platform ROAS typically overstates true value by 20-50 percent
- Channels that capture existing demand look most efficient
- Lower funnel gets credit for upper funnel work

Better approach:
- Use iROAS (incremental ROAS) validated by testing
- Use MER for portfolio-level efficiency
- Validate high-ROAS channels with incrementality testing before scaling

3.2 PITFALL: LONG ATTRIBUTION WINDOWS WITHOUT VALIDATION

What it looks like:
- 28-day or longer click attribution windows
- High view-through attribution included
- Large volume of conversions attributed

Why it happens:
- Longer windows show more conversions
- Makes campaigns look more successful
- Platform defaults often favor longer windows

Why it matters:
- Long windows capture conversions that would happen anyway
- View-through especially inflates results
- True campaign impact is overstated

Better approach:
- Start with 7-day click attribution
- Test multiple windows to understand sensitivity
- Add view-through only with holdout validation
- Use shorter windows for conservative estimates

3.3 PITFALL: SINGLE-SOURCE ATTRIBUTION

What it looks like:
- Reliance on one platform or tool for all attribution
- No cross-validation of results
- Decisions based on single data source

Why it happens:
- Simplicity and convenience
- Platform tools are readily available
- Limited resources for multiple approaches

Why it matters:
- Every source has inherent biases
- Platform attribution favors the platform
- Errors are not caught without triangulation

Better approach:
- Use multiple measurement methods
- Compare platform attribution to MER trends
- Validate with incrementality testing
- Reconcile discrepancies before making decisions

3.4 PITFALL: MEASURING WHAT IS EASY INSTEAD OF WHAT MATTERS

What it looks like:
- Focus on clicks, impressions, and engagement
- Limited connection to business outcomes
- Proxies substituted for actual objectives

Why it happens:
- Business outcomes are harder to measure
- Proxy metrics are available immediately
- Pressure to show activity

Why it matters:
- Proxy metrics may not correlate with outcomes
- Optimizing for proxies can hurt actual results
- Disconnection between marketing and business goals

Better approach:
- Define business outcome KPIs first
- Connect proxy metrics to outcomes with validation
- Accept measurement latency for accurate data
- Use leading indicators with known correlation

3.5 PITFALL: NO BASELINE OR CONTROL

What it looks like:
- All budget allocated to active campaigns
- No holdout for comparison
- Cannot separate campaign effect from organic

Why it happens:
- Pressure to maximize spend efficiency
- Fear of missing conversions
- Unfamiliarity with test design

Why it matters:
- Cannot measure true incremental impact
- All conversions attributed to campaigns
- Actual effectiveness unknown

Better approach:
- Maintain geographic or audience holdouts
- Use 5-10 percent of budget or audience for control
- Rotate holdouts to validate ongoing
- Accept some conversion loss for measurement accuracy

================================================================================
SECTION 4: DIAGNOSTIC SIGNALS
================================================================================

4.1 IF ATTRIBUTED CONVERSIONS EXCEED BUSINESS RESULTS

What to check:
- Are multiple channels claiming same conversions
- Is attribution window too long
- Are view-through conversions included

Questions to ask:
- How do attributed conversions compare to actual sales
- What percentage of attributed conversions are view-through
- Are channels using consistent attribution settings

Typical implications:
- Attribution is overstating impact
- Need to deduplicate or adjust methodology
- Incrementality testing required for validation

4.2 IF CHANNEL ROAS VARIES DRAMATICALLY

What to check:
- Are channels at different funnel stages
- Are attribution windows consistent
- Is one channel getting credit for another work

Questions to ask:
- What is the customer journey across these channels
- How would lower-funnel perform without upper-funnel
- Have you tested incrementality of high-ROAS channels

Typical implications:
- Lower funnel typically shows higher ROAS but lower incrementality
- Upper funnel contribution is undervalued
- Need holistic view, not channel comparison

4.3 IF PERFORMANCE LOOKS GREAT BUT BUSINESS RESULTS LAG

What to check:
- Are metrics connected to actual business outcomes
- Is measurement capturing right conversions
- Are there delays in outcome realization

Questions to ask:
- What is the relationship between reported metrics and revenue
- How long does it take for conversions to become revenue
- Are we measuring leading or lagging indicators

Typical implications:
- Metrics may not correlate with business outcomes
- Need to validate measurement against business results
- May be optimizing for wrong objectives

4.4 IF INCREMENTALITY RESULTS ARE LOWER THAN ATTRIBUTION

What to check:
- How much was baseline or organic activity
- Which channels have lowest incrementality
- What portion of conversions would happen anyway

Questions to ask:
- What was the difference between test and control
- Which channels showed highest incremental lift
- Should allocation shift based on these results

Typical implications:
- Attribution overstated true impact
- Some channels are capturing not creating demand
- Budget reallocation may improve true efficiency

4.5 IF MEASUREMENT METHODOLOGY CHANGES RESULTS SIGNIFICANTLY

What to check:
- Which methodology is most accurate
- What assumptions drive each approach
- How do biases affect each method

Questions to ask:
- Why do different methods produce different results
- Which method aligns best with business outcomes
- What is the margin of error on each

Typical implications:
- Measurement has significant uncertainty
- Need triangulation to improve confidence
- Decisions should account for range of outcomes

================================================================================
SECTION 5: ATTRIBUTION METHODOLOGY COMPARISON
================================================================================

5.1 LAST-CLICK ATTRIBUTION

How it works:
- 100 percent credit to final touchpoint before conversion
- Simplest and most common default

Strengths:
- Simple and easy to understand
- Provides clear channel credit
- Actionable for direct response

Weaknesses:
- Ignores all prior touchpoints
- Undervalues awareness and consideration
- Favors lower-funnel channels

When to use:
- Short purchase cycles
- Single-channel campaigns
- When simplicity is priority

Cautions:
- Do not use to compare funnel stages
- Will undervalue brand investment

5.2 MULTI-TOUCH ATTRIBUTION

How it works:
- Distributes credit across multiple touchpoints
- Rules-based (linear, time decay, position) or algorithmic

Strengths:
- Recognizes journey complexity
- More balanced than last-click
- Can reveal channel interactions

Weaknesses:
- Requires significant data
- Model assumptions affect results
- Still based on observed conversions only

When to use:
- Longer purchase cycles
- Multi-channel campaigns
- When journey understanding is important

Cautions:
- Different models produce different results
- Algorithmic models are black boxes

5.3 MEDIA MIX MODELING (MMM)

How it works:
- Statistical regression on aggregate data
- Separates marketing contribution from baseline

Strengths:
- Does not require user-level tracking
- Works across all channels including offline
- Accounts for external factors

Weaknesses:
- Requires 2-3 years historical data
- Aggregate level, not user level
- Significant expertise required

When to use:
- Large budgets and long history
- Privacy-constrained environments
- Portfolio-level optimization

Cautions:
- Cannot optimize in real-time
- Model quality varies significantly

5.4 INCREMENTALITY TESTING

How it works:
- Compare test group to control group
- Measure lift attributable to campaign

Strengths:
- Measures true causal impact
- Gold standard for validation
- Works for any channel or tactic

Weaknesses:
- Requires dedicated budget or audience
- Foregoes some conversions for measurement
- Statistical expertise required

When to use:
- Validating channel contribution
- Before major scaling decisions
- Ongoing validation cadence

Cautions:
- Test design affects results
- Sample size requirements

5.5 MEASUREMENT TRIANGULATION

How it works:
- Use multiple methods and compare
- Reconcile differences to build confidence

Strengths:
- Reduces reliance on any single method
- Catches errors and biases
- Builds more complete picture

Weaknesses:
- More complex and resource-intensive
- Requires reconciliation process
- May produce conflicting answers

When to use:
- Always, to some degree
- Especially for major decisions
- When single-source results seem too good

Cautions:
- Do not average conflicting results
- Investigate discrepancies

================================================================================
SECTION 6: QUESTIONS WORTH ASKING
================================================================================

6.1 SUCCESS DEFINITION QUESTIONS

These questions clarify what success means:

- What is the business outcome we are ultimately trying to drive (Reveals true objective)
- How will you know if this campaign was successful (Reveals success criteria)
- What would make this campaign a failure (Reveals floor)
- How does this campaign connect to broader business goals (Reveals context)

6.2 METHODOLOGY QUESTIONS

These questions assess measurement approach:

- How are you currently attributing conversions (Reveals methodology)
- What attribution window are you using and why (Reveals assumptions)
- Have you validated your attribution with incrementality testing (Reveals validation)
- How do you handle conversions that touch multiple channels (Reveals sophistication)

6.3 DATA QUALITY QUESTIONS

These questions assess measurement reliability:

- How confident are you in your conversion tracking (Reveals data quality)
- What percentage of conversions are you able to track (Reveals coverage)
- Are there known gaps in your measurement (Reveals blind spots)
- How often does attributed performance differ from business results (Reveals calibration)

6.4 LEARNING QUESTIONS

These questions ensure measurement generates insight:

- What hypotheses are we testing with this campaign (Reveals learning agenda)
- How will we know if our assumptions were correct (Reveals validation plan)
- What will we do differently based on results (Reveals action orientation)
- How will we capture and apply learnings (Reveals knowledge management)

================================================================================
SECTION 7: BENCHMARKS AND REFERENCE POINTS
================================================================================

7.1 ATTRIBUTION WINDOW BENCHMARKS

Recommended starting points by objective:

- Direct response, short cycle: 7-day click
- E-commerce, consideration purchase: 7-14 day click
- B2B, long sales cycle: 30-day click
- Awareness campaigns: View-through requires validation

7.2 INCREMENTALITY RATE BENCHMARKS

Typical incrementality by channel (percent of attributed that is truly incremental):

- Brand search: 20-40 percent incremental
- Non-brand search: 50-70 percent incremental
- Retargeting: 30-50 percent incremental
- Prospecting social: 40-60 percent incremental
- CTV and video: 60-80 percent incremental
- Display prospecting: 30-50 percent incremental

These are approximate ranges. Actual incrementality varies by advertiser and context.

7.3 TEST DESIGN BENCHMARKS

Recommended parameters for incrementality testing:

- Minimum test duration: 4 weeks
- Control holdout size: 5-10 percent of budget or audience
- Statistical significance target: 90-95 percent confidence
- Minimum detectable effect: Define before test begins

7.4 ROAS VERSUS iROAS BENCHMARKS

Typical relationship between platform ROAS and incremental ROAS:

- Platform ROAS typically 20-50 percent higher than true iROAS
- Gap is largest for lower-funnel channels
- Gap is smallest for upper-funnel channels
- Specific gap should be validated by testing

================================================================================
SECTION 8: FALLBACK BEHAVIOR
================================================================================

8.1 IF INCREMENTALITY TESTING IS NOT POSSIBLE

When testing cannot be conducted:
- Use benchmark incrementality rates to discount attributed results
- Flag which channels have highest validation need
- Recommend testing when resources allow
- Apply conservative estimates for scaling decisions

8.2 IF ATTRIBUTION METHODOLOGY IS UNCLEAR

When client measurement approach is undefined:
- Recommend starting with platform attribution plus MER
- Establish attribution settings consistently across channels
- Plan for incrementality validation over time
- Use multiple data points for decisions

8.3 IF BUSINESS OUTCOME DATA IS UNAVAILABLE

When conversions cannot be tied to business results:
- Define proxy metrics with acknowledged limitations
- Recommend outcome data capture for future
- Use industry benchmarks for conversion to outcome ratios
- Flag uncertainty in projections

8.4 IF MEASUREMENT RESULTS CONFLICT

When different methods produce different results:
- Investigate sources of discrepancy
- Weight toward incrementality testing if available
- Use conservative estimate for decisions
- Plan additional validation to resolve

================================================================================
SECTION 9: CROSS-REFERENCES
================================================================================

9.1 RELATED DOCUMENTS

- Analytics Engine v5.1 - Detailed calculation methodologies
- KPI seed data - Metric definitions and benchmarks
- AI_ADVERTISING_GUIDE_v5_5.txt - AI campaign measurement challenges
- MPA_Supporting_Instructions_v5_5.txt - Communication patterns
- MPA_Implications_Measurement_Choices_v5_5_v5_5.txt - Downstream effects

9.2 WEB SEARCH TRIGGERS

The agent SHOULD use web search when:
- Platform-specific attribution updates needed
- Recent incrementality research requested
- Privacy regulation impacts on measurement
- Emerging measurement methodologies

================================================================================
VERSION HISTORY
================================================================================

Version 1.0 - January 2026 - Initial creation
- Established critical measurement considerations
- Documented common pitfalls with better approaches
- Added diagnostic signals for measurement issues
- Included attribution methodology comparison
- Added questions worth asking

================================================================================
END OF DOCUMENT
================================================================================
