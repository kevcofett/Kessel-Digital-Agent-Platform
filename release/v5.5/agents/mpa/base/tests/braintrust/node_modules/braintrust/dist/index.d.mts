import { GitMetadataSettings, LogFeedbackFullArgs, ExperimentEvent, BackgroundLogEvent, ExperimentLogFullArgs, ExperimentLogPartialArgs, IdField, SpanType, RepoInfo, DEFAULT_IS_LEGACY_DATASET, TRANSACTION_ID_FIELD, TransactionId, SpanObjectTypeV3, DatasetRecord, Score, CommentEvent, InputField, InputsField, LogCommentFullArgs, OtherExperimentLogFields, ParentExperimentIds, ParentProjectLogIds } from '@braintrust/core';
export { CommentEvent, DatasetRecord, ExperimentLogFullArgs, ExperimentLogPartialArgs, IdField, InputField, InputsField, LogCommentFullArgs, LogFeedbackFullArgs, OtherExperimentLogFields, ParentExperimentIds, ParentProjectLogIds } from '@braintrust/core';
import { PromptData, OpenAIMessage, Tools, AnyModelParam, Message, Prompt as Prompt$1, PromptSessionEvent, StreamingMode, FunctionType, IfExists, SavedFunctionId, ModelParams } from '@braintrust/core/typespecs';
import { z } from 'zod';

interface IsoAsyncLocalStorage<T> {
    enterWith(store: T): void;
    run<R>(store: T | undefined, callback: () => R): R;
    getStore(): T | undefined;
}

declare class LazyValue<T> {
    private callable;
    private value;
    constructor(callable: () => Promise<T>);
    get(): Promise<T>;
    get hasComputed(): boolean;
}

type SetCurrentArg = {
    setCurrent?: boolean;
};
type StartSpanEventArgs = ExperimentLogPartialArgs & Partial<IdField>;
type StartSpanArgs = {
    name?: string;
    type?: SpanType;
    spanAttributes?: Record<any, any>;
    startTime?: number;
    parent?: string;
    event?: StartSpanEventArgs;
    propagatedEvent?: StartSpanEventArgs;
};
type EndSpanArgs = {
    endTime?: number;
};
interface Exportable {
    /**
     * Return a serialized representation of the object that can be used to start subspans in other places. See `Span.traced` for more details.
     */
    export(): Promise<string>;
}
/**
 * A Span encapsulates logged data and metrics for a unit of work. This interface is shared by all span implementations.
 *
 * We suggest using one of the various `traced` methods, instead of creating Spans directly.
 *
 * See `Span.traced` for full details.
 */
interface Span extends Exportable {
    /**
     * Row ID of the span.
     */
    id: string;
    /**
     * Incrementally update the current span with new data. The event will be batched and uploaded behind the scenes.
     *
     * @param event: Data to be logged. See `Experiment.log` for full details.
     */
    log(event: ExperimentLogPartialArgs): void;
    /**
     * Add feedback to the current span. Unlike `Experiment.logFeedback` and `Logger.logFeedback`, this method does not accept an id parameter, because it logs feedback to the current span.
     *
     * @param event: Data to be logged. See `Experiment.logFeedback` for full details.
     */
    logFeedback(event: Omit<LogFeedbackFullArgs, "id">): void;
    /**
     * Create a new span and run the provided callback. This is useful if you want to log more detailed trace information beyond the scope of a single log event. Data logged over several calls to `Span.log` will be merged into one logical row.
     *
     * Spans created within `traced` are ended automatically. By default, the span is marked as current, so they can be accessed using `braintrust.currentSpan`.
     *
     * @param callback The function to be run under the span context.
     * @param args.name Optional name of the span. If not provided, a name will be inferred from the call stack.
     * @param args.type Optional type of the span. If not provided, the type will be unset.
     * @param args.span_attributes Optional additional attributes to attach to the span, such as a type name.
     * @param args.start_time Optional start time of the span, as a timestamp in seconds.
     * @param args.setCurrent If true (the default), the span will be marked as the currently-active span for the duration of the callback.
     * @param args.parent Optional parent info string for the span. The string can be generated from `[Span,Experiment,Logger].export`. If not provided, the current span will be used (depending on context). This is useful for adding spans to an existing trace.
     * @param args.event Data to be logged. See `Experiment.log` for full details.
     * @Returns The result of running `callback`.
     */
    traced<R>(callback: (span: Span) => R, args?: StartSpanArgs & SetCurrentArg): R;
    /**
     * Lower-level alternative to `traced`. This allows you to start a span yourself, and can be useful in situations
     * where you cannot use callbacks. However, spans started with `startSpan` will not be marked as the "current span",
     * so `currentSpan()` and `traced()` will be no-ops. If you want to mark a span as current, use `traced` instead.
     *
     * See `traced` for full details.
     *
     * @returns The newly-created `Span`
     */
    startSpan(args?: StartSpanArgs): Span;
    /**
     * Log an end time to the span (defaults to the current time). Returns the logged time.
     *
     * Will be invoked automatically if the span is constructed with `traced`.
     *
     * @param args.endTime Optional end time of the span, as a timestamp in seconds.
     * @returns The end time logged to the span metrics.
     */
    end(args?: EndSpanArgs): number;
    /**
     * Flush any pending rows to the server.
     */
    flush(): Promise<void>;
    /**
     * Alias for `end`.
     */
    close(args?: EndSpanArgs): number;
    /**
     * Set the span's name, type, or other attributes after it's created.
     */
    setAttributes(args: Omit<StartSpanArgs, "event">): void;
    kind: "span";
}
/**
 * A fake implementation of the Span API which does nothing. This can be used as the default span.
 */
declare class NoopSpan implements Span {
    id: string;
    kind: "span";
    constructor();
    log(_: ExperimentLogPartialArgs): void;
    logFeedback(_event: Omit<LogFeedbackFullArgs, "id">): void;
    traced<R>(callback: (span: Span) => R, _1?: StartSpanArgs & SetCurrentArg): R;
    startSpan(_1?: StartSpanArgs): this;
    end(args?: EndSpanArgs): number;
    export(): Promise<string>;
    flush(): Promise<void>;
    close(args?: EndSpanArgs): number;
    setAttributes(_args: Omit<StartSpanArgs, "event">): void;
}
declare const NOOP_SPAN: NoopSpan;
declare global {
    var __inherited_braintrust_state: BraintrustState;
}
declare const loginSchema: z.ZodObject<{
    appUrl: z.ZodString;
    appPublicUrl: z.ZodString;
    orgName: z.ZodString;
    apiUrl: z.ZodString;
    proxyUrl: z.ZodString;
    loginToken: z.ZodString;
    orgId: z.ZodOptional<z.ZodNullable<z.ZodString>>;
    gitMetadataSettings: z.ZodOptional<z.ZodNullable<z.ZodObject<{
        collect: z.ZodEnum<["all", "none", "some"]>;
        fields: z.ZodOptional<z.ZodArray<z.ZodEnum<["dirty", "tag", "commit", "branch", "author_name", "author_email", "commit_message", "commit_time", "git_diff"]>, "many">>;
    }, "strict", z.ZodTypeAny, {
        collect: "some" | "none" | "all";
        fields?: ("dirty" | "tag" | "commit" | "branch" | "author_name" | "author_email" | "commit_message" | "commit_time" | "git_diff")[] | undefined;
    }, {
        collect: "some" | "none" | "all";
        fields?: ("dirty" | "tag" | "commit" | "branch" | "author_name" | "author_email" | "commit_message" | "commit_time" | "git_diff")[] | undefined;
    }>>>;
}, "strict", z.ZodTypeAny, {
    appUrl: string;
    appPublicUrl: string;
    orgName: string;
    apiUrl: string;
    proxyUrl: string;
    loginToken: string;
    orgId?: string | null | undefined;
    gitMetadataSettings?: {
        collect: "some" | "none" | "all";
        fields?: ("dirty" | "tag" | "commit" | "branch" | "author_name" | "author_email" | "commit_message" | "commit_time" | "git_diff")[] | undefined;
    } | null | undefined;
}, {
    appUrl: string;
    appPublicUrl: string;
    orgName: string;
    apiUrl: string;
    proxyUrl: string;
    loginToken: string;
    orgId?: string | null | undefined;
    gitMetadataSettings?: {
        collect: "some" | "none" | "all";
        fields?: ("dirty" | "tag" | "commit" | "branch" | "author_name" | "author_email" | "commit_message" | "commit_time" | "git_diff")[] | undefined;
    } | null | undefined;
}>;
type SerializedBraintrustState = z.infer<typeof loginSchema>;
declare class BraintrustState {
    private loginParams;
    id: string;
    currentExperiment: Experiment | undefined;
    currentLogger: Logger<false> | undefined;
    currentSpan: IsoAsyncLocalStorage<Span>;
    private _bgLogger;
    appUrl: string | null;
    appPublicUrl: string | null;
    loginToken: string | null;
    orgId: string | null;
    orgName: string | null;
    apiUrl: string | null;
    proxyUrl: string | null;
    loggedIn: boolean;
    gitMetadataSettings?: GitMetadataSettings;
    fetch: typeof globalThis.fetch;
    private _appConn;
    private _apiConn;
    private _proxyConn;
    constructor(loginParams: LoginOptions);
    resetLoginInfo(): void;
    copyLoginInfo(other: BraintrustState): void;
    serialize(): SerializedBraintrustState;
    static deserialize(serialized: unknown, opts?: LoginOptions): BraintrustState;
    setFetch(fetch: typeof globalThis.fetch): void;
    login(loginParams: LoginOptions & {
        forceLogin?: boolean;
    }): Promise<void>;
    appConn(): HTTPConnection;
    apiConn(): HTTPConnection;
    proxyConn(): HTTPConnection;
    bgLogger(): BackgroundLogger;
    loginReplaceApiConn(apiConn: HTTPConnection): void;
}
declare function _internalSetInitialState(): void;
declare const _internalGetGlobalState: () => BraintrustState;
declare class FailedHTTPResponse extends Error {
    status: number;
    text: string;
    data: any;
    constructor(status: number, text: string, data?: any);
}
declare class HTTPConnection {
    base_url: string;
    token: string | null;
    headers: Record<string, string>;
    fetch: typeof globalThis.fetch;
    constructor(base_url: string, fetch: typeof globalThis.fetch);
    setFetch(fetch: typeof globalThis.fetch): void;
    ping(): Promise<boolean>;
    make_long_lived(): void;
    static sanitize_token(token: string): string;
    set_token(token: string): void;
    _reset(): void;
    get(path: string, params?: Record<string, string | string[] | undefined> | undefined, config?: RequestInit): Promise<Response>;
    post(path: string, params?: Record<string, unknown> | string, config?: RequestInit): Promise<Response>;
    get_json(object_type: string, args?: Record<string, string | string[] | undefined> | undefined, retries?: number): Promise<any>;
    post_json(object_type: string, args?: Record<string, unknown> | string | undefined): Promise<any>;
}
interface ObjectMetadata {
    id: string;
    name: string;
    fullInfo: Record<string, unknown>;
}
interface ProjectExperimentMetadata {
    project: ObjectMetadata;
    experiment: ObjectMetadata;
}
interface ProjectDatasetMetadata {
    project: ObjectMetadata;
    dataset: ObjectMetadata;
}
interface OrgProjectMetadata {
    org_id: string;
    project: ObjectMetadata;
}
interface LogOptions<IsAsyncFlush> {
    asyncFlush?: IsAsyncFlush;
    computeMetadataArgs?: Record<string, any>;
}
type PromiseUnless<B, R> = B extends true ? R : Promise<Awaited<R>>;
/**
 * Update a span using the output of `span.export()`. It is important that you only resume updating
 * to a span once the original span has been fully written and flushed, since otherwise updates to
 * the span may conflict with the original span.
 *
 * @param exported The output of `span.export()`.
 * @param event The event data to update the span with. See `Experiment.log` for a full list of valid fields.
 * @param state (optional) Login state to use. If not provided, the global state will be used.
 */
declare function updateSpan({ exported, state, ...event }: {
    exported: string;
} & Omit<Partial<ExperimentEvent>, "id"> & OptionalStateArg): void;
interface ParentSpanIds {
    spanId: string;
    rootSpanId: string;
}
declare class Logger<IsAsyncFlush extends boolean> implements Exportable {
    private state;
    private lazyMetadata;
    private _asyncFlush;
    private computeMetadataArgs;
    private lastStartTime;
    private lazyId;
    private calledStartSpan;
    kind: "logger";
    constructor(state: BraintrustState, lazyMetadata: LazyValue<OrgProjectMetadata>, logOptions?: LogOptions<IsAsyncFlush>);
    get org_id(): Promise<string>;
    get project(): Promise<ObjectMetadata>;
    get id(): Promise<string>;
    private parentObjectType;
    /**
     * Log a single event. The event will be batched and uploaded behind the scenes if `logOptions.asyncFlush` is true.
     *
     * @param event The event to log.
     * @param event.input: (Optional) the arguments that uniquely define a user input (an arbitrary, JSON serializable object).
     * @param event.output: (Optional) the output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question.
     * @param event.expected: (Optional) the ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models.
     * @param event.error: (Optional) The error that occurred, if any. If you use tracing to run an experiment, errors are automatically logged when your code throws an exception.
     * @param event.scores: (Optional) a dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare logs.
     * @param event.metadata: (Optional) a dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings.
     * @param event.metrics: (Optional) a dictionary of metrics to log. The following keys are populated automatically: "start", "end".
     * @param event.id: (Optional) a unique identifier for the event. If you don't provide one, BrainTrust will generate one for you.
     * @param options Additional logging options
     * @param options.allowConcurrentWithSpans in rare cases where you need to log at the top level separately from spans on the logger elsewhere, set this to true.
     * :returns: The `id` of the logged event.
     */
    log(event: Readonly<StartSpanEventArgs>, options?: {
        allowConcurrentWithSpans?: boolean;
    }): PromiseUnless<IsAsyncFlush, string>;
    /**
     * Create a new toplevel span underneath the logger. The name defaults to "root".
     *
     * See `Span.traced` for full details.
     */
    traced<R>(callback: (span: Span) => R, args?: StartSpanArgs & SetCurrentArg): PromiseUnless<IsAsyncFlush, R>;
    /**
     * Lower-level alternative to `traced`. This allows you to start a span yourself, and can be useful in situations
     * where you cannot use callbacks. However, spans started with `startSpan` will not be marked as the "current span",
     * so `currentSpan()` and `traced()` will be no-ops. If you want to mark a span as current, use `traced` instead.
     *
     * See `traced` for full details.
     */
    startSpan(args?: StartSpanArgs): Span;
    private startSpanImpl;
    /**
     * Log feedback to an event. Feedback is used to save feedback scores, set an expected value, or add a comment.
     *
     * @param event
     * @param event.id The id of the event to log feedback for. This is the `id` returned by `log` or accessible as the `id` field of a span.
     * @param event.scores (Optional) a dictionary of numeric values (between 0 and 1) to log. These scores will be merged into the existing scores for the event.
     * @param event.expected (Optional) the ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not.
     * @param event.comment (Optional) an optional comment string to log about the event.
     * @param event.metadata (Optional) a dictionary with additional data about the feedback. If you have a `user_id`, you can log it here and access it in the Braintrust UI.
     * @param event.source (Optional) the source of the feedback. Must be one of "external" (default), "app", or "api".
     */
    logFeedback(event: LogFeedbackFullArgs): void;
    /**
     * Update a span in the experiment using its id. It is important that you only update a span once the original span has been fully written and flushed,
     * since otherwise updates to the span may conflict with the original span.
     *
     * @param event The event data to update the span with. Must include `id`. See `Experiment.log` for a full list of valid fields.
     */
    updateSpan(event: Omit<Partial<ExperimentEvent>, "id"> & Required<Pick<ExperimentEvent, "id">>): void;
    /**
     * Return a serialized representation of the logger that can be used to start subspans in other places. See `Span.start_span` for more details.
     */
    export(): Promise<string>;
    flush(): Promise<void>;
    get asyncFlush(): IsAsyncFlush | undefined;
}
interface BackgroundLoggerOpts {
    noExitFlush?: boolean;
}
declare class BackgroundLogger {
    private apiConn;
    private items;
    private activeFlush;
    private activeFlushResolved;
    private activeFlushError;
    syncFlush: boolean;
    maxRequestSize: number;
    defaultBatchSize: number;
    numTries: number;
    queueDropExceedingMaxsize: number | undefined;
    queueDropLoggingPeriod: number;
    failedPublishPayloadsDir: string | undefined;
    allPublishPayloadsDir: string | undefined;
    private queueDropLoggingState;
    constructor(apiConn: LazyValue<HTTPConnection>, opts?: BackgroundLoggerOpts);
    log(items: LazyValue<BackgroundLogEvent>[]): void;
    flush(): Promise<void>;
    private flushOnce;
    private unwrapLazyValues;
    private submitLogsRequest;
    private registerDroppedItemCount;
    private dumpDroppedEvents;
    private static writePayloadToDir;
    private triggerActiveFlush;
    private logFailedPayloadsDir;
    internalReplaceApiConn(apiConn: HTTPConnection): void;
}
type InitOpenOption<IsOpen extends boolean> = {
    open?: IsOpen;
};
type InitOptions<IsOpen extends boolean> = FullLoginOptions & {
    experiment?: string;
    description?: string;
    dataset?: AnyDataset;
    update?: boolean;
    baseExperiment?: string;
    isPublic?: boolean;
    metadata?: Record<string, unknown>;
    gitMetadataSettings?: GitMetadataSettings;
    projectId?: string;
    baseExperimentId?: string;
    repoInfo?: RepoInfo;
    setCurrent?: boolean;
    state?: BraintrustState;
} & InitOpenOption<IsOpen>;
type FullInitOptions<IsOpen extends boolean> = {
    project?: string;
} & InitOptions<IsOpen>;
type InitializedExperiment<IsOpen extends boolean | undefined> = IsOpen extends true ? ReadonlyExperiment : Experiment;
/**
 * Log in, and then initialize a new experiment in a specified project. If the project does not exist, it will be created.
 *
 * @param options Options for configuring init().
 * @param options.project The name of the project to create the experiment in. Must specify at least one of `project` or `projectId`.
 * @param options.experiment The name of the experiment to create. If not specified, a name will be generated automatically.
 * @param options.description An optional description of the experiment.
 * @param options.dataset (Optional) A dataset to associate with the experiment. You can pass in the name of the dataset (in the same project) or a dataset object (from any project).
 * @param options.update If the experiment already exists, continue logging to it. If it does not exist, creates the experiment with the specified arguments.
 * @param options.baseExperiment An optional experiment name to use as a base. If specified, the new experiment will be summarized and compared to this experiment. Otherwise, it will pick an experiment by finding the closest ancestor on the default (e.g. main) branch.
 * @param options.isPublic An optional parameter to control whether the experiment is publicly visible to anybody with the link or privately visible to only members of the organization. Defaults to private.
 * @param options.appUrl The URL of the Braintrust App. Defaults to https://www.braintrust.dev.
 * @param options.apiKey The API key to use. If the parameter is not specified, will try to use the `BRAINTRUST_API_KEY` environment variable. If no API key is specified, will prompt the user to login.
 * @param options.orgName (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.
 * @param options.metadata (Optional) A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings.
 * @param options.gitMetadataSettings (Optional) Settings for collecting git metadata. By default, will collect all git metadata fields allowed in org-level settings.
 * @param setCurrent If true (the default), set the global current-experiment to the newly-created one.
 * @param options.open If the experiment already exists, open it in read-only mode. Throws an error if the experiment does not already exist.
 * @param options.projectId The id of the project to create the experiment in. This takes precedence over `project` if specified.
 * @param options.baseExperimentId An optional experiment id to use as a base. If specified, the new experiment will be summarized and compared to this. This takes precedence over `baseExperiment` if specified.
 * @param options.repoInfo (Optional) Explicitly specify the git metadata for this experiment. This takes precedence over `gitMetadataSettings` if specified.
 * @returns The newly created Experiment.
 */
declare function init<IsOpen extends boolean = false>(options: Readonly<FullInitOptions<IsOpen>>): InitializedExperiment<IsOpen>;
/**
 * Legacy form of `init` which accepts the project name as the first parameter,
 * separately from the remaining options. See `init(options)` for full details.
 */
declare function init<IsOpen extends boolean = false>(project: string, options?: Readonly<InitOptions<IsOpen>>): InitializedExperiment<IsOpen>;
/**
 * Alias for init(options).
 */
declare function initExperiment<IsOpen extends boolean = false>(options: Readonly<InitOptions<IsOpen>>): InitializedExperiment<IsOpen>;
/**
 * Alias for init(project, options).
 */
declare function initExperiment<IsOpen extends boolean = false>(project: string, options?: Readonly<InitOptions<IsOpen>>): InitializedExperiment<IsOpen>;
/**
 * This function is deprecated. Use `init` instead.
 */
declare function withExperiment<R>(project: string, callback: (experiment: Experiment) => R, options?: Readonly<InitOptions<false> & SetCurrentArg>): R;
/**
 * This function is deprecated. Use `initLogger` instead.
 */
declare function withLogger<IsAsyncFlush extends boolean = false, R = void>(callback: (logger: Logger<IsAsyncFlush>) => R, options?: Readonly<InitLoggerOptions<IsAsyncFlush> & SetCurrentArg>): R;
type UseOutputOption<IsLegacyDataset extends boolean> = {
    useOutput?: IsLegacyDataset;
};
type InitDatasetOptions<IsLegacyDataset extends boolean> = FullLoginOptions & {
    dataset?: string;
    description?: string;
    version?: string;
    projectId?: string;
    state?: BraintrustState;
} & UseOutputOption<IsLegacyDataset>;
type FullInitDatasetOptions<IsLegacyDataset extends boolean> = {
    project?: string;
} & InitDatasetOptions<IsLegacyDataset>;
/**
 * Create a new dataset in a specified project. If the project does not exist, it will be created.
 *
 * @param options Options for configuring initDataset().
 * @param options.project The name of the project to create the dataset in. Must specify at least one of `project` or `projectId`.
 * @param options.dataset The name of the dataset to create. If not specified, a name will be generated automatically.
 * @param options.description An optional description of the dataset.
 * @param options.appUrl The URL of the Braintrust App. Defaults to https://www.braintrust.dev.
 * @param options.apiKey The API key to use. If the parameter is not specified, will try to use the `BRAINTRUST_API_KEY` environment variable. If no API key is specified, will prompt the user to login.
 * @param options.orgName (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.
 * @param options.projectId The id of the project to create the dataset in. This takes precedence over `project` if specified.
 * @param options.useOutput (Deprecated) If true, records will be fetched from this dataset in the legacy format, with the "expected" field renamed to "output". This option will be removed in a future version of Braintrust.
 * @returns The newly created Dataset.
 */
declare function initDataset<IsLegacyDataset extends boolean = typeof DEFAULT_IS_LEGACY_DATASET>(options: Readonly<FullInitDatasetOptions<IsLegacyDataset>>): Dataset<IsLegacyDataset>;
/**
 * Legacy form of `initDataset` which accepts the project name as the first
 * parameter, separately from the remaining options. See
 * `initDataset(options)` for full details.
 */
declare function initDataset<IsLegacyDataset extends boolean = typeof DEFAULT_IS_LEGACY_DATASET>(project: string, options?: Readonly<InitDatasetOptions<IsLegacyDataset>>): Dataset<IsLegacyDataset>;
/**
 * This function is deprecated. Use `initDataset` instead.
 */
declare function withDataset<R, IsLegacyDataset extends boolean = typeof DEFAULT_IS_LEGACY_DATASET>(project: string, callback: (dataset: Dataset<IsLegacyDataset>) => R, options?: Readonly<InitDatasetOptions<IsLegacyDataset>>): R;
type AsyncFlushArg<IsAsyncFlush> = {
    asyncFlush?: IsAsyncFlush;
};
type InitLoggerOptions<IsAsyncFlush> = FullLoginOptions & {
    projectName?: string;
    projectId?: string;
    setCurrent?: boolean;
    state?: BraintrustState;
} & AsyncFlushArg<IsAsyncFlush>;
/**
 * Create a new logger in a specified project. If the project does not exist, it will be created.
 *
 * @param options Additional options for configuring init().
 * @param options.projectName The name of the project to log into. If unspecified, will default to the Global project.
 * @param options.projectId The id of the project to log into. This takes precedence over projectName if specified.
 * @param options.asyncFlush If true, will log asynchronously in the background. Otherwise, will log synchronously. (false by default, to support serverless environments)
 * @param options.appUrl The URL of the Braintrust App. Defaults to https://www.braintrust.dev.
 * @param options.apiKey The API key to use. If the parameter is not specified, will try to use the `BRAINTRUST_API_KEY` environment variable. If no API
 * key is specified, will prompt the user to login.
 * @param options.orgName (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.
 * @param options.forceLogin Login again, even if you have already logged in (by default, the logger will not login if you are already logged in)
 * @param setCurrent If true (the default), set the global current-experiment to the newly-created one.
 * @returns The newly created Logger.
 */
declare function initLogger<IsAsyncFlush extends boolean = false>(options?: Readonly<InitLoggerOptions<IsAsyncFlush>>): Logger<IsAsyncFlush>;
type LoadPromptOptions = FullLoginOptions & {
    projectName?: string;
    projectId?: string;
    slug?: string;
    version?: string;
    defaults?: DefaultPromptArgs;
    noTrace?: boolean;
    state?: BraintrustState;
};
/**
 * Load a prompt from the specified project.
 *
 * @param options Options for configuring loadPrompt().
 * @param options.projectName The name of the project to load the prompt from. Must specify at least one of `projectName` or `projectId`.
 * @param options.projectId The id of the project to load the prompt from. This takes precedence over `projectName` if specified.
 * @param options.slug The slug of the prompt to load.
 * @param options.version An optional version of the prompt (to read). If not specified, the latest version will be used.
 * @param options.defaults (Optional) A dictionary of default values to use when rendering the prompt. Prompt values will override these defaults.
 * @param options.noTrace If true, do not include logging metadata for this prompt when build() is called.
 * @param options.appUrl The URL of the Braintrust App. Defaults to https://www.braintrust.dev.
 * @param options.apiKey The API key to use. If the parameter is not specified, will try to use the `BRAINTRUST_API_KEY` environment variable. If no API
 * key is specified, will prompt the user to login.
 * @param options.orgName (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.
 * @returns The prompt object.
 * @throws If the prompt is not found.
 * @throws If multiple prompts are found with the same slug in the same project (this should never happen).
 *
 * @example
 * ```javascript
 * const prompt = await loadPrompt({
 *  projectName: "My Project",
 *  slug: "my-prompt",
 * });
 * ```
 */
declare function loadPrompt({ projectName, projectId, slug, version, defaults, noTrace, appUrl, apiKey, orgName, fetch, forceLogin, state: stateArg, }: LoadPromptOptions): Promise<Prompt<true, true>>;
/**
 * Options for logging in to Braintrust.
 */
interface LoginOptions {
    /**
     * The URL of the Braintrust App. Defaults to https://www.braintrust.dev. You should not need
     * to change this unless you are doing the "Full" deployment.
     */
    appUrl?: string;
    /**
     * The API key to use. If the parameter is not specified, will try to use the `BRAINTRUST_API_KEY` environment variable.
     */
    apiKey?: string;
    /**
     * The name of a specific organization to connect to. Since API keys are scoped to organizations, this parameter is usually
     * unnecessary unless you are logging in with a JWT.
     */
    orgName?: string;
    /**
     * A custom fetch implementation to use.
     */
    fetch?: typeof globalThis.fetch;
    /**
     * By default, the SDK installs an event handler that flushes pending writes on the `beforeExit` event.
     * If true, this event handler will _not_ be installed.
     */
    noExitFlush?: boolean;
}
type FullLoginOptions = LoginOptions & {
    forceLogin?: boolean;
};
/**
 * Log into Braintrust. This will prompt you for your API token, which you can find at
 * https://www.braintrust.dev/app/token. This method is called automatically by `init()`.
 *
 * @param options Options for configuring login().
 * @param options.appUrl The URL of the Braintrust App. Defaults to https://www.braintrust.dev.
 * @param options.apiKey The API key to use. If the parameter is not specified, will try to use the `BRAINTRUST_API_KEY` environment variable. If no API
 * key is specified, will prompt the user to login.
 * @param options.orgName (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.
 * @param options.forceLogin Login again, even if you have already logged in (by default, this function will exit quickly if you have already logged in)
 */
declare function login(options?: LoginOptions & {
    forceLogin?: boolean;
}): Promise<BraintrustState>;
declare function loginToState(options?: LoginOptions): Promise<BraintrustState>;
/**
 * Log a single event to the current experiment. The event will be batched and uploaded behind the scenes.
 *
 * @param event The event to log. See `Experiment.log` for full details.
 * @returns The `id` of the logged event.
 */
declare function log(event: ExperimentLogFullArgs): string;
/**
 * Summarize the current experiment, including the scores (compared to the closest reference experiment) and metadata.
 *
 * @param options Options for summarizing the experiment.
 * @param options.summarizeScores Whether to summarize the scores. If False, only the metadata will be returned.
 * @param options.comparisonExperimentId The experiment to compare against. If None, the most recent experiment on the origin's main branch will be used.
 * @returns A summary of the experiment, including the scores (compared to the closest reference experiment) and metadata.
 */
declare function summarize(options?: {
    readonly summarizeScores?: boolean;
    readonly comparisonExperimentId?: string;
}): Promise<ExperimentSummary>;
type OptionalStateArg = {
    state?: BraintrustState;
};
/**
 * Returns the currently-active experiment (set by `braintrust.init`). Returns undefined if no current experiment has been set.
 */
declare function currentExperiment(options?: OptionalStateArg): Experiment | undefined;
/**
 * Returns the currently-active logger (set by `braintrust.initLogger`). Returns undefined if no current logger has been set.
 */
declare function currentLogger<IsAsyncFlush extends boolean>(options?: AsyncFlushArg<IsAsyncFlush> & OptionalStateArg): Logger<IsAsyncFlush> | undefined;
/**
 * Return the currently-active span for logging (set by one of the `traced` methods). If there is no active span, returns a no-op span object, which supports the same interface as spans but does no logging.
 *
 * See `Span` for full details.
 */
declare function currentSpan(options?: OptionalStateArg): Span;
/**
 * Mainly for internal use. Return the parent object for starting a span in a global context.
 */
declare function getSpanParentObject<IsAsyncFlush extends boolean>(options?: AsyncFlushArg<IsAsyncFlush> & OptionalStateArg): Span | Experiment | Logger<IsAsyncFlush>;
declare function logError(span: Span, error: unknown): void;
/**
 * Toplevel function for starting a span. It checks the following (in precedence order):
 *  * Currently-active span
 *  * Currently-active experiment
 *  * Currently-active logger
 *
 * and creates a span under the first one that is active. Alternatively, if `parent` is specified, it creates a span under the specified parent row. If none of these are active, it returns a no-op span object.
 *
 * See `Span.traced` for full details.
 */
declare function traced<IsAsyncFlush extends boolean = false, R = void>(callback: (span: Span) => R, args?: StartSpanArgs & SetCurrentArg & AsyncFlushArg<IsAsyncFlush> & OptionalStateArg): PromiseUnless<IsAsyncFlush, R>;
/**
 * Wrap a function with `traced`, using the arguments as `input` and return value as `output`.
 * Any functions wrapped this way will automatically be traced, similar to the `@traced` decorator
 * in Python. If you want to correctly propagate the function's name and define it in one go, then
 * you can do so like this:
 *
 * ```ts
 * const myFunc = wrapTraced(async function myFunc(input) {
 *  const result = await client.chat.completions.create({
 *    model: "gpt-3.5-turbo",
 *    messages: [{ role: "user", content: input }],
 *  });
 *  return result.choices[0].message.content ?? "unknown";
 * });
 * ```
 * Now, any calls to `myFunc` will be traced, and the input and output will be logged automatically.
 * If tracing is inactive, i.e. there is no active logger or experiment, it's just a no-op.
 *
 * @param fn The function to wrap.
 * @param args Span-level arguments (e.g. a custom name or type) to pass to `traced`.
 * @returns The wrapped function.
 */
declare function wrapTraced<F extends (...args: any[]) => any, IsAsyncFlush extends boolean = false>(fn: F, args?: StartSpanArgs & SetCurrentArg & AsyncFlushArg<IsAsyncFlush>): IsAsyncFlush extends false ? (...args: Parameters<F>) => Promise<Awaited<ReturnType<F>>> : F;
/**
 * A synonym for `wrapTraced`. If you're porting from systems that use `traceable`, you can use this to
 * make your codebase more consistent.
 */
declare const traceable: typeof wrapTraced;
/**
 * Lower-level alternative to `traced`. This allows you to start a span yourself, and can be useful in situations
 * where you cannot use callbacks. However, spans started with `startSpan` will not be marked as the "current span",
 * so `currentSpan()` and `traced()` will be no-ops. If you want to mark a span as current, use `traced` instead.
 *
 * See `traced` for full details.
 */
declare function startSpan<IsAsyncFlush extends boolean = false>(args?: StartSpanArgs & AsyncFlushArg<IsAsyncFlush> & OptionalStateArg): Span;
/**
 * Flush any pending rows to the server.
 */
declare function flush(options?: OptionalStateArg): Promise<void>;
/**
 * Set the fetch implementation to use for requests. You can specify it here,
 * or when you call `login`.
 *
 * @param fetch The fetch implementation to use.
 */
declare function setFetch(fetch: typeof globalThis.fetch): void;
/**
 * Runs the provided callback with the span as the current span.
 */
declare function withCurrent<R>(span: Span, callback: (span: Span) => R, state?: BraintrustState): R;
type WithTransactionId<R> = R & {
    [TRANSACTION_ID_FIELD]: TransactionId;
};
declare class ObjectFetcher<RecordType> implements AsyncIterable<WithTransactionId<RecordType>> {
    private objectType;
    private pinnedVersion;
    private mutateRecord?;
    private _fetchedData;
    constructor(objectType: "dataset" | "experiment", pinnedVersion: string | undefined, mutateRecord?: ((r: any) => RecordType) | undefined);
    get id(): Promise<string>;
    protected getState(): Promise<BraintrustState>;
    fetch(): AsyncGenerator<WithTransactionId<RecordType>>;
    [Symbol.asyncIterator](): AsyncIterator<WithTransactionId<RecordType>>;
    fetchedData(): Promise<WithTransactionId<RecordType>[]>;
    clearCache(): void;
    version(): Promise<string | undefined>;
}
type BaseMetadata = Record<string, unknown> | void;
type DefaultMetadataType = void;
type EvalCase<Input, Expected, Metadata> = {
    input: Input;
    tags?: string[];
} & (Expected extends void ? {} : {
    expected: Expected;
}) & (Metadata extends void ? {} : {
    metadata: Metadata;
});
/**
 * An experiment is a collection of logged events, such as model inputs and outputs, which represent
 * a snapshot of your application at a particular point in time. An experiment is meant to capture more
 * than just the model you use, and includes the data you use to test, pre- and post- processing code,
 * comparison metrics (scores), and any other metadata you want to include.
 *
 * Experiments are associated with a project, and two experiments are meant to be easily comparable via
 * their `inputs`. You can change the attributes of the experiments in a project (e.g. scoring functions)
 * over time, simply by changing what you log.
 *
 * You should not create `Experiment` objects directly. Instead, use the `braintrust.init()` method.
 */
declare class Experiment extends ObjectFetcher<ExperimentEvent> implements Exportable {
    private readonly lazyMetadata;
    readonly dataset?: AnyDataset;
    private lastStartTime;
    private lazyId;
    private calledStartSpan;
    private state;
    kind: "experiment";
    constructor(state: BraintrustState, lazyMetadata: LazyValue<ProjectExperimentMetadata>, dataset?: AnyDataset);
    get id(): Promise<string>;
    get name(): Promise<string>;
    get project(): Promise<ObjectMetadata>;
    private parentObjectType;
    protected getState(): Promise<BraintrustState>;
    /**
     * Log a single event to the experiment. The event will be batched and uploaded behind the scenes.
     *
     * @param event The event to log.
     * @param event.input: The arguments that uniquely define a test case (an arbitrary, JSON serializable object). Later on, Braintrust will use the `input` to know whether two test cases are the same between experiments, so they should not contain experiment-specific state. A simple rule of thumb is that if you run the same experiment twice, the `input` should be identical.
     * @param event.output: The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, the `output` should be the _result_ of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question.
     * @param event.expected: (Optional) The ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not. Braintrust currently does not compare `output` to `expected` for you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate your experiments while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models.
     * @param event.error: (Optional) The error that occurred, if any. If you use tracing to run an experiment, errors are automatically logged when your code throws an exception.
     * @param event.scores: A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare experiments.
     * @param event.metadata: (Optional) a dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any JSON-serializable type, but its keys must be strings.
     * @param event.metrics: (Optional) a dictionary of metrics to log. The following keys are populated automatically: "start", "end".
     * @param event.id: (Optional) a unique identifier for the event. If you don't provide one, BrainTrust will generate one for you.
     * @param event.dataset_record_id: (Optional) the id of the dataset record that this event is associated with. This field is required if and only if the experiment is associated with a dataset.
     * @param event.inputs: (Deprecated) the same as `input` (will be removed in a future version).
     * @param options Additional logging options
     * @param options.allowConcurrentWithSpans in rare cases where you need to log at the top level separately from spans on the experiment elsewhere, set this to true.
     * :returns: The `id` of the logged event.
     */
    log(event: Readonly<ExperimentLogFullArgs>, options?: {
        allowConcurrentWithSpans?: boolean;
    }): string;
    /**
     * Create a new toplevel span underneath the experiment. The name defaults to "root".
     *
     * See `Span.traced` for full details.
     */
    traced<R>(callback: (span: Span) => R, args?: StartSpanArgs & SetCurrentArg): R;
    /**
     * Lower-level alternative to `traced`. This allows you to start a span yourself, and can be useful in situations
     * where you cannot use callbacks. However, spans started with `startSpan` will not be marked as the "current span",
     * so `currentSpan()` and `traced()` will be no-ops. If you want to mark a span as current, use `traced` instead.
     *
     * See `traced` for full details.
     */
    startSpan(args?: StartSpanArgs): Span;
    private startSpanImpl;
    fetchBaseExperiment(): Promise<{
        id: any;
        name: any;
    } | null>;
    /**
     * Summarize the experiment, including the scores (compared to the closest reference experiment) and metadata.
     *
     * @param options Options for summarizing the experiment.
     * @param options.summarizeScores Whether to summarize the scores. If False, only the metadata will be returned.
     * @param options.comparisonExperimentId The experiment to compare against. If None, the most recent experiment on the origin's main branch will be used.
     * @returns A summary of the experiment, including the scores (compared to the closest reference experiment) and metadata.
     */
    summarize(options?: {
        readonly summarizeScores?: boolean;
        readonly comparisonExperimentId?: string;
    }): Promise<ExperimentSummary>;
    /**
     * Log feedback to an event in the experiment. Feedback is used to save feedback scores, set an expected value, or add a comment.
     *
     * @param event
     * @param event.id The id of the event to log feedback for. This is the `id` returned by `log` or accessible as the `id` field of a span.
     * @param event.scores (Optional) a dictionary of numeric values (between 0 and 1) to log. These scores will be merged into the existing scores for the event.
     * @param event.expected (Optional) the ground truth value (an arbitrary, JSON serializable object) that you'd compare to `output` to determine if your `output` value is correct or not.
     * @param event.comment (Optional) an optional comment string to log about the event.
     * @param event.metadata (Optional) a dictionary with additional data about the feedback. If you have a `user_id`, you can log it here and access it in the Braintrust UI.
     * @param event.source (Optional) the source of the feedback. Must be one of "external" (default), "app", or "api".
     */
    logFeedback(event: LogFeedbackFullArgs): void;
    /**
     * Update a span in the experiment using its id. It is important that you only update a span once the original span has been fully written and flushed,
     * since otherwise updates to the span may conflict with the original span.
     *
     * @param event The event data to update the span with. Must include `id`. See `Experiment.log` for a full list of valid fields.
     */
    updateSpan(event: Omit<Partial<ExperimentEvent>, "id"> & Required<Pick<ExperimentEvent, "id">>): void;
    /**
     * Return a serialized representation of the experiment that can be used to start subspans in other places. See `Span.start_span` for more details.
     */
    export(): Promise<string>;
    /**
     * Flush any pending rows to the server.
     */
    flush(): Promise<void>;
    /**
     * This function is deprecated. You can simply remove it from your code.
     */
    close(): Promise<string>;
}
/**
 * A read-only view of an experiment, initialized by passing `open: true` to `init()`.
 */
declare class ReadonlyExperiment extends ObjectFetcher<ExperimentEvent> {
    private state;
    private readonly lazyMetadata;
    constructor(state: BraintrustState, lazyMetadata: LazyValue<ProjectExperimentMetadata>);
    get id(): Promise<string>;
    get name(): Promise<string>;
    protected getState(): Promise<BraintrustState>;
    asDataset<Input, Expected>(): AsyncGenerator<EvalCase<Input, Expected, void>>;
}
declare function newId(): string;
/**
 * Primary implementation of the `Span` interface. See the `Span` interface for full details on each method.
 *
 * We suggest using one of the various `traced` methods, instead of creating Spans directly. See `Span.startSpan` for full details.
 */
declare class SpanImpl implements Span {
    private state;
    private isMerge;
    private loggedEndTime;
    private propagatedEvent;
    private parentObjectType;
    private parentObjectId;
    private parentComputeObjectMetadataArgs;
    private _id;
    private spanId;
    private rootSpanId;
    private spanParents;
    kind: "span";
    constructor(args: {
        state: BraintrustState;
        parentObjectType: SpanObjectTypeV3;
        parentObjectId: LazyValue<string>;
        parentComputeObjectMetadataArgs: Record<string, any> | undefined;
        parentSpanIds: ParentSpanIds | undefined;
        defaultRootType?: SpanType;
    } & Omit<StartSpanArgs, "parent">);
    get id(): string;
    setAttributes(args: Omit<StartSpanArgs, "event">): void;
    log(event: ExperimentLogPartialArgs): void;
    private logInternal;
    logFeedback(event: Omit<LogFeedbackFullArgs, "id">): void;
    traced<R>(callback: (span: Span) => R, args?: StartSpanArgs & SetCurrentArg): R;
    startSpan(args?: StartSpanArgs): Span;
    end(args?: EndSpanArgs): number;
    export(): Promise<string>;
    flush(): Promise<void>;
    close(args?: EndSpanArgs): number;
}
/**
 * A dataset is a collection of records, such as model inputs and expected outputs, which represent
 * data you can use to evaluate and fine-tune models. You can log production data to datasets,
 * curate them with interesting examples, edit/delete records, and run evaluations against them.
 *
 * You should not create `Dataset` objects directly. Instead, use the `braintrust.initDataset()` method.
 */
declare class Dataset<IsLegacyDataset extends boolean = typeof DEFAULT_IS_LEGACY_DATASET> extends ObjectFetcher<DatasetRecord<IsLegacyDataset>> {
    private state;
    private readonly lazyMetadata;
    constructor(state: BraintrustState, lazyMetadata: LazyValue<ProjectDatasetMetadata>, pinnedVersion?: string, legacy?: IsLegacyDataset);
    get id(): Promise<string>;
    get name(): Promise<string>;
    get project(): Promise<ObjectMetadata>;
    protected getState(): Promise<BraintrustState>;
    private validateEvent;
    private createArgs;
    /**
     * Insert a single record to the dataset. The record will be batched and uploaded behind the scenes. If you pass in an `id`,
     * and a record with that `id` already exists, it will be overwritten (upsert).
     *
     * @param event The event to log.
     * @param event.input The argument that uniquely define an input case (an arbitrary, JSON serializable object).
     * @param event.expected The output of your application, including post-processing (an arbitrary, JSON serializable object).
     * @param event.tags (Optional) a list of strings that you can use to filter and group records later.
     * @param event.metadata (Optional) a dictionary with additional data about the test example, model outputs, or just
     * about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log the
     * `prompt`, example's `id`, or anything else that would be useful to slice/dice later. The values in `metadata` can be any
     * JSON-serializable type, but its keys must be strings.
     * @param event.id (Optional) a unique identifier for the event. If you don't provide one, Braintrust will generate one for you.
     * @param event.output: (Deprecated) The output of your application. Use `expected` instead.
     * @returns The `id` of the logged record.
     */
    insert({ input, expected, metadata, tags, id, output, }: {
        readonly input?: unknown;
        readonly expected?: unknown;
        readonly tags?: string[];
        readonly metadata?: Record<string, unknown>;
        readonly id?: string;
        readonly output?: unknown;
    }): string;
    /**
     * Update fields of a single record in the dataset. The updated fields will be batched and uploaded behind the scenes.
     * You must pass in an `id` of the record to update. Only the fields provided will be updated; other fields will remain unchanged.
     *
     * @param event The fields to update in the record.
     * @param event.id The unique identifier of the record to update.
     * @param event.input (Optional) The new input value for the record (an arbitrary, JSON serializable object).
     * @param event.expected (Optional) The new expected output value for the record (an arbitrary, JSON serializable object).
     * @param event.tags (Optional) A list of strings to update the tags of the record.
     * @param event.metadata (Optional) A dictionary to update the metadata of the record. The values in `metadata` can be any
     * JSON-serializable type, but its keys must be strings.
     * @returns The `id` of the updated record.
     */
    update({ input, expected, metadata, tags, id, }: {
        readonly id: string;
        readonly input?: unknown;
        readonly expected?: unknown;
        readonly tags?: string[];
        readonly metadata?: Record<string, unknown>;
    }): string;
    delete(id: string): string;
    /**
     * Summarize the dataset, including high level metrics about its size and other metadata.
     * @param summarizeData Whether to summarize the data. If false, only the metadata will be returned.
     * @returns `DatasetSummary`
     * @returns A summary of the dataset.
     */
    summarize(options?: {
        readonly summarizeData?: boolean;
    }): Promise<DatasetSummary>;
    /**
     * Flush any pending rows to the server.
     */
    flush(): Promise<void>;
    /**
     * This function is deprecated. You can simply remove it from your code.
     */
    close(): Promise<string>;
}
type CompiledPromptParams = Omit<NonNullable<PromptData["options"]>["params"], "use_cache"> & {
    model: NonNullable<NonNullable<PromptData["options"]>["model"]>;
};
type ChatPrompt = {
    messages: OpenAIMessage[];
    tools?: Tools;
};
type CompletionPrompt = {
    prompt: string;
};
type CompiledPrompt<Flavor extends "chat" | "completion"> = CompiledPromptParams & {
    span_info?: {
        name?: string;
        spanAttributes?: Record<any, any>;
        metadata: {
            prompt: {
                variables: Record<string, unknown>;
                id: string;
                project_id: string;
                version: string;
            };
        };
    };
} & (Flavor extends "chat" ? ChatPrompt : Flavor extends "completion" ? CompletionPrompt : {});
type DefaultPromptArgs = Partial<CompiledPromptParams & AnyModelParam & ChatPrompt & CompletionPrompt>;
declare function renderMessage<T extends Message>(render: (template: string) => string, message: T): T;
type PromptRowWithId<HasId extends boolean = true, HasVersion extends boolean = true> = Omit<Prompt$1, "log_id" | "org_id" | "project_id" | "id" | "_xact_id"> & Partial<Pick<Prompt$1, "project_id">> & (HasId extends true ? Pick<Prompt$1, "id"> : Partial<Pick<Prompt$1, "id">>) & (HasVersion extends true ? Pick<Prompt$1, "_xact_id"> : Partial<Pick<Prompt$1, "_xact_id">>);
declare class Prompt<HasId extends boolean = true, HasVersion extends boolean = true> {
    private metadata;
    private defaults;
    private noTrace;
    private parsedPromptData;
    private hasParsedPromptData;
    constructor(metadata: PromptRowWithId<HasId, HasVersion> | PromptSessionEvent, defaults: DefaultPromptArgs, noTrace: boolean);
    get id(): HasId extends true ? string : string | undefined;
    get projectId(): string | undefined;
    get name(): string;
    get slug(): string;
    get prompt(): PromptData["prompt"];
    get version(): HasId extends true ? TransactionId : TransactionId | undefined;
    get options(): NonNullable<PromptData["options"]>;
    /**
     * Build the prompt with the given formatting options. The args you pass in will
     * be forwarded to the mustache template that defines the prompt and rendered with
     * the `mustache-js` library.
     *
     * @param buildArgs Args to forward along to the prompt template.
     */
    build<Flavor extends "chat" | "completion" = "chat">(buildArgs: unknown, options?: {
        flavor?: Flavor;
        messages?: Message[];
    }): CompiledPrompt<Flavor>;
    private runBuild;
    private getParsedPromptData;
}
type AnyDataset = Dataset<boolean>;
/**
 * Summary of a score's performance.
 * @property name Name of the score.
 * @property score Average score across all examples.
 * @property diff Difference in score between the current and reference experiment.
 * @property improvements Number of improvements in the score.
 * @property regressions Number of regressions in the score.
 */
interface ScoreSummary {
    name: string;
    score: number;
    diff?: number;
    improvements: number;
    regressions: number;
}
/**
 * Summary of a metric's performance.
 * @property name Name of the metric.
 * @property metric Average metric across all examples.
 * @property unit Unit label for the metric.
 * @property diff Difference in metric between the current and reference experiment.
 * @property improvements Number of improvements in the metric.
 * @property regressions Number of regressions in the metric.
 */
interface MetricSummary {
    name: string;
    metric: number;
    unit: string;
    diff?: number;
    improvements: number;
    regressions: number;
}
/**
 * Summary of an experiment's scores and metadata.
 * @property projectName Name of the project that the experiment belongs to.
 * @property experimentName Name of the experiment.
 * @property experimentId ID of the experiment. May be `undefined` if the eval was run locally.
 * @property projectUrl URL to the project's page in the Braintrust app.
 * @property experimentUrl URL to the experiment's page in the Braintrust app.
 * @property comparisonExperimentName The experiment scores are baselined against.
 * @property scores Summary of the experiment's scores.
 */
interface ExperimentSummary {
    projectName: string;
    experimentName: string;
    projectId?: string;
    experimentId?: string;
    projectUrl?: string;
    experimentUrl?: string;
    comparisonExperimentName?: string;
    scores: Record<string, ScoreSummary>;
    metrics?: Record<string, MetricSummary>;
}
/**
 * Summary of a dataset's data.
 *
 * @property newRecords New or updated records added in this session.
 * @property totalRecords Total records in the dataset.
 */
interface DataSummary {
    newRecords: number;
    totalRecords: number;
}
/**
 * Summary of a dataset's scores and metadata.
 *
 * @property projectName Name of the project that the dataset belongs to.
 * @property datasetName Name of the dataset.
 * @property projectUrl URL to the project's page in the Braintrust app.
 * @property datasetUrl URL to the experiment's page in the Braintrust app.
 * @property dataSummary Summary of the dataset's data.
 */
interface DatasetSummary {
    projectName: string;
    datasetName: string;
    projectUrl: string;
    datasetUrl: string;
    dataSummary: DataSummary;
}

declare const braintrustStreamChunkSchema: z.ZodUnion<[z.ZodObject<{
    type: z.ZodLiteral<"text_delta">;
    data: z.ZodString;
}, "strip", z.ZodTypeAny, {
    data: string;
    type: "text_delta";
}, {
    data: string;
    type: "text_delta";
}>, z.ZodObject<{
    type: z.ZodLiteral<"json_delta">;
    data: z.ZodString;
}, "strip", z.ZodTypeAny, {
    data: string;
    type: "json_delta";
}, {
    data: string;
    type: "json_delta";
}>, z.ZodObject<{
    type: z.ZodLiteral<"error">;
    data: z.ZodString;
}, "strip", z.ZodTypeAny, {
    data: string;
    type: "error";
}, {
    data: string;
    type: "error";
}>, z.ZodObject<{
    type: z.ZodLiteral<"progress">;
    data: z.ZodObject<{
        id: z.ZodString;
        object_type: z.ZodEnum<["prompt", "tool", "scorer", "task"]>;
        format: z.ZodEnum<["llm", "code", "global"]>;
        output_type: z.ZodEnum<["completion", "score", "any"]>;
        name: z.ZodString;
        event: z.ZodEnum<["text_delta", "json_delta", "error", "start", "done"]>;
        data: z.ZodString;
    }, "strip", z.ZodTypeAny, {
        format: "code" | "global" | "llm";
        data: string;
        name: string;
        id: string;
        object_type: "prompt" | "tool" | "scorer" | "task";
        event: "error" | "text_delta" | "json_delta" | "start" | "done";
        output_type: "completion" | "score" | "any";
    }, {
        format: "code" | "global" | "llm";
        data: string;
        name: string;
        id: string;
        object_type: "prompt" | "tool" | "scorer" | "task";
        event: "error" | "text_delta" | "json_delta" | "start" | "done";
        output_type: "completion" | "score" | "any";
    }>;
}, "strip", z.ZodTypeAny, {
    data: {
        format: "code" | "global" | "llm";
        data: string;
        name: string;
        id: string;
        object_type: "prompt" | "tool" | "scorer" | "task";
        event: "error" | "text_delta" | "json_delta" | "start" | "done";
        output_type: "completion" | "score" | "any";
    };
    type: "progress";
}, {
    data: {
        format: "code" | "global" | "llm";
        data: string;
        name: string;
        id: string;
        object_type: "prompt" | "tool" | "scorer" | "task";
        event: "error" | "text_delta" | "json_delta" | "start" | "done";
        output_type: "completion" | "score" | "any";
    };
    type: "progress";
}>, z.ZodObject<{
    type: z.ZodLiteral<"start">;
    data: z.ZodString;
}, "strip", z.ZodTypeAny, {
    data: string;
    type: "start";
}, {
    data: string;
    type: "start";
}>, z.ZodObject<{
    type: z.ZodLiteral<"done">;
    data: z.ZodString;
}, "strip", z.ZodTypeAny, {
    data: string;
    type: "done";
}, {
    data: string;
    type: "done";
}>]>;
/**
 * A chunk of data from a Braintrust stream. Each chunk type matches
 * an SSE event type.
 */
type BraintrustStreamChunk = z.infer<typeof braintrustStreamChunkSchema>;
/**
 * A Braintrust stream. This is a wrapper around a ReadableStream of `BraintrustStreamChunk`,
 * with some utility methods to make them easy to log and convert into various formats.
 */
declare class BraintrustStream {
    private stream;
    private memoizedFinalValue;
    constructor(baseStream: ReadableStream<Uint8Array>);
    constructor(stream: ReadableStream<string>);
    constructor(stream: ReadableStream<BraintrustStreamChunk>);
    /**
     * Copy the stream. This returns a new stream that shares the same underlying
     * stream (via `tee`). Since streams are consumed in Javascript, use `copy()` if you
     * need to use the stream multiple times.
     *
     * @returns A new stream that you can independently consume.
     */
    copy(): BraintrustStream;
    /**
     * Get the underlying ReadableStream.
     *
     * @returns The underlying ReadableStream<BraintrustStreamChunk>.
     */
    toReadableStream(): ReadableStream<BraintrustStreamChunk>;
    /**
     * Returns an async iterator for the BraintrustStream.
     * This allows for easy consumption of the stream using a for-await...of loop.
     *
     * @returns An async iterator that yields BraintrustStreamChunk objects.
     */
    [Symbol.asyncIterator](): AsyncIterator<BraintrustStreamChunk>;
    /**
     * Get the final value of the stream. The final value is the concatenation of all
     * the chunks in the stream, deserialized into a string or JSON object, depending on
     * the value's type.
     *
     * This function returns a promise that resolves when the stream is closed, and
     * contains the final value. Multiple calls to `finalValue()` will return the same
     * promise, so it is safe to call this multiple times.
     *
     * This function consumes the stream, so if you need to use the stream multiple
     * times, you should call `copy()` first.
     *
     * @returns A promise that resolves with the final value of the stream or `undefined` if the stream is empty.
     */
    finalValue(): Promise<unknown>;
}
/**
 * Create a stream that passes through the final value of the stream. This is
 * used to implement `BraintrustStream.finalValue()`.
 *
 * @param onFinal A function to call with the final value of the stream.
 * @returns A new stream that passes through the final value of the stream.
 */
declare function createFinalValuePassThroughStream<T extends BraintrustStreamChunk | string | Uint8Array>(onFinal: (result: unknown) => void, onError: (error: unknown) => void): TransformStream<T, BraintrustStreamChunk>;
declare function devNullWritableStream(): WritableStream;

/**
 * Arguments for the `invoke` function.
 */
interface InvokeFunctionArgs<Input, Output, Stream extends boolean = false> {
    /**
     * The ID of the function to invoke.
     */
    function_id?: string;
    /**
     * The name of the project containing the function to invoke.
     */
    projectName?: string;
    /**
     * The slug of the function to invoke.
     */
    slug?: string;
    /**
     * The name of the global function to invoke.
     */
    globalFunction?: string;
    /**
     * The ID of the prompt session to invoke the function from.
     */
    promptSessionId?: string;
    /**
     * The ID of the function in the prompt session to invoke.
     */
    promptSessionFunctionId?: string;
    /**
     * The version of the function to invoke.
     */
    version?: string;
    /**
     * The input to the function. This will be logged as the `input` field in the span.
     */
    input: Input;
    /**
     * Additional OpenAI-style messages to add to the prompt (only works for llm functions).
     */
    messages?: Message[];
    /**
     * The parent of the function. This can be an existing span, logger, or experiment, or
     * the output of `.export()` if you are distributed tracing. If unspecified, will use
     * the same semantics as `traced()` to determine the parent and no-op if not in a tracing
     * context.
     */
    parent?: Exportable | string;
    /**
     * Whether to stream the function's output. If true, the function will return a
     * `BraintrustStream`, otherwise it will return the output of the function as a JSON
     * object.
     */
    stream?: Stream;
    /**
     * The mode of the function. If "auto", will return a string if the function returns a string,
     * and a JSON object otherwise. If "parallel", will return an array of JSON objects with one
     * object per tool call.
     */
    mode?: StreamingMode;
    /**
     * A Zod schema to validate the output of the function and return a typed value. This
     * is only used if `stream` is false.
     */
    schema?: Stream extends true ? never : z.ZodSchema<Output>;
    /**
     * (Advanced) This parameter allows you to pass in a custom login state. This is useful
     * for multi-tenant environments where you are running functions from different Braintrust
     * organizations.
     */
    state?: BraintrustState;
}
/**
 * The return type of the `invoke` function. Conditionally returns a `BraintrustStream`
 * if `stream` is true, otherwise returns the output of the function using the Zod schema's
 * type if present.
 */
type InvokeReturn<Stream extends boolean, Output> = Stream extends true ? BraintrustStream : Output;
/**
 * Invoke a Braintrust function, returning a `BraintrustStream` or the value as a plain
 * Javascript object.
 *
 * @param args The arguments for the function (see {@link InvokeFunctionArgs} for more details).
 * @returns The output of the function.
 */
declare function invoke<Input, Output, Stream extends boolean = false>(args: InvokeFunctionArgs<Input, Output, Stream> & FullLoginOptions): Promise<InvokeReturn<Stream, Output>>;

type GenericFunction<Input, Output> = ((input: Input) => Output) | ((input: Input) => Promise<Output>);

type NameOrId = {
    name: string;
} | {
    id: string;
};
type CreateProjectOpts = NameOrId;
declare class ProjectBuilder {
    create(opts: CreateProjectOpts): Project;
}
declare const projects: ProjectBuilder;
declare class Project {
    readonly name?: string;
    readonly id?: string;
    tools: ToolBuilder;
    prompts: PromptBuilder;
    constructor(args: CreateProjectOpts);
}
declare class ToolBuilder {
    private readonly project;
    private taskCounter;
    constructor(project: Project);
    create<Input, Output, Fn extends GenericFunction<Input, Output>>(opts: ToolOpts<Input, Output, Fn>): CodeFunction<Input, Output, Fn>;
}
type Schema<Input, Output> = Partial<{
    parameters: z.ZodSchema<Input>;
    returns: z.ZodSchema<Output>;
}>;
interface BaseFnOpts {
    name: string;
    slug: string;
    description: string;
    ifExists: IfExists;
}
type ToolOpts<Params, Returns, Fn extends GenericFunction<Params, Returns>> = Partial<BaseFnOpts> & {
    handler: Fn;
} & Schema<Params, Returns>;
declare class CodeFunction<Input, Output, Fn extends GenericFunction<Input, Output>> {
    readonly project: Project;
    readonly handler: Fn;
    readonly name: string;
    readonly slug: string;
    readonly type: FunctionType;
    readonly description?: string;
    readonly parameters?: z.ZodSchema<Input>;
    readonly returns?: z.ZodSchema<Output>;
    readonly ifExists?: IfExists;
    constructor(project: Project, opts: Omit<ToolOpts<Input, Output, Fn>, "name" | "slug"> & {
        name: string;
        slug: string;
        type: FunctionType;
    });
    key(): string;
}
type GenericCodeFunction = CodeFunction<any, any, GenericFunction<any, any>>;
declare class CodePrompt {
    readonly project: Project;
    readonly name: string;
    readonly slug: string;
    readonly prompt: PromptData;
    readonly ifExists?: IfExists;
    readonly description?: string;
    readonly id?: string;
    readonly toolFunctions: (SavedFunctionId | GenericCodeFunction)[];
    constructor(project: Project, prompt: PromptData, toolFunctions: (SavedFunctionId | GenericCodeFunction)[], opts: Omit<PromptOpts<false, false>, "name" | "slug"> & {
        name: string;
        slug: string;
    });
}
declare const toolFunctionDefinitionSchema: z.ZodObject<{
    type: z.ZodLiteral<"function">;
    function: z.ZodObject<{
        name: z.ZodString;
        description: z.ZodOptional<z.ZodString>;
        parameters: z.ZodOptional<z.ZodRecord<z.ZodString, z.ZodUnknown>>;
        strict: z.ZodOptional<z.ZodBoolean>;
    }, "strip", z.ZodTypeAny, {
        name: string;
        description?: string | undefined;
        parameters?: Record<string, unknown> | undefined;
        strict?: boolean | undefined;
    }, {
        name: string;
        description?: string | undefined;
        parameters?: Record<string, unknown> | undefined;
        strict?: boolean | undefined;
    }>;
}, "strip", z.ZodTypeAny, {
    function: {
        name: string;
        description?: string | undefined;
        parameters?: Record<string, unknown> | undefined;
        strict?: boolean | undefined;
    };
    type: "function";
}, {
    function: {
        name: string;
        description?: string | undefined;
        parameters?: Record<string, unknown> | undefined;
        strict?: boolean | undefined;
    };
    type: "function";
}>;
type ToolFunctionDefinition = z.infer<typeof toolFunctionDefinitionSchema>;
interface PromptId {
    id: string;
}
interface PromptVersion {
    version: TransactionId;
}
type PromptContents = {
    prompt: string;
} | {
    messages: Message[];
};
type PromptOpts<HasId extends boolean, HasVersion extends boolean> = (Partial<Omit<BaseFnOpts, "name">> & {
    name: string;
}) & (HasId extends true ? PromptId : Partial<PromptId>) & (HasVersion extends true ? PromptVersion : Partial<PromptVersion>) & PromptContents & {
    model: string;
    params?: ModelParams;
    tools?: (GenericCodeFunction | SavedFunctionId | ToolFunctionDefinition)[];
    noTrace?: boolean;
};
declare class PromptBuilder {
    private readonly project;
    constructor(project: Project);
    create<HasId extends boolean = false, HasVersion extends boolean = false>(opts: PromptOpts<HasId, HasVersion>): Prompt<HasId, HasVersion>;
}

type BaseExperiment<Input, Expected, Metadata extends BaseMetadata = DefaultMetadataType> = {
    _type: "BaseExperiment";
    _phantom?: [Input, Expected, Metadata];
    name?: string;
};
/**
 * Use this to specify that the dataset should actually be the data from a previous (base) experiment.
 * If you do not specify a name, Braintrust will automatically figure out the best base experiment to
 * use based on your git history (or fall back to timestamps).
 *
 * @param options
 * @param options.name The name of the base experiment to use. If unspecified, Braintrust will automatically figure out the best base
 * using your git history (or fall back to timestamps).
 * @returns
 */
declare function BaseExperiment<Input = unknown, Expected = unknown, Metadata extends BaseMetadata = DefaultMetadataType>(options?: {
    name?: string;
}): BaseExperiment<Input, Expected, Metadata>;
type EvalData<Input, Expected, Metadata extends BaseMetadata = DefaultMetadataType> = EvalCase<Input, Expected, Metadata>[] | (() => EvalCase<Input, Expected, Metadata>[]) | Promise<EvalCase<Input, Expected, Metadata>[]> | (() => Promise<EvalCase<Input, Expected, Metadata>[]>) | AsyncGenerator<EvalCase<Input, Expected, Metadata>> | AsyncIterable<EvalCase<Input, Expected, Metadata>> | BaseExperiment<Input, Expected, Metadata> | (() => BaseExperiment<Input, Expected, Metadata>);
type EvalTask<Input, Output> = ((input: Input, hooks: EvalHooks) => Promise<Output>) | ((input: Input, hooks: EvalHooks) => Output);
interface EvalHooks {
    meta: (info: Record<string, unknown>) => void;
    span: Span;
}
type EvalScorerArgs<Input, Output, Expected, Metadata extends BaseMetadata = DefaultMetadataType> = EvalCase<Input, Expected, Metadata> & {
    output: Output;
};
type OneOrMoreScores = Score | number | null | Array<Score>;
type EvalScorer<Input, Output, Expected, Metadata extends BaseMetadata = DefaultMetadataType> = (args: EvalScorerArgs<Input, Output, Expected, Metadata>) => OneOrMoreScores | Promise<OneOrMoreScores>;
type EvalResult<Input, Output, Expected, Metadata extends BaseMetadata = DefaultMetadataType> = EvalCase<Input, Expected, Metadata> & {
    output: Output;
    scores: Record<string, number | null>;
    error: unknown;
};
interface Evaluator<Input, Output, Expected, Metadata extends BaseMetadata = DefaultMetadataType> {
    /**
     * A function that returns a list of inputs, expected outputs, and metadata.
     */
    data: EvalData<Input, Expected, Metadata>;
    /**
     * A function that takes an input and returns an output.
     */
    task: EvalTask<Input, Output>;
    /**
     * A set of functions that take an input, output, and expected value and return a score.
     */
    scores: EvalScorer<Input, Output, Expected, Metadata>[];
    /**
     * An optional name for the experiment.
     */
    experimentName?: string;
    /**
     * The number of times to run the evaluator per input. This is useful for evaluating applications that
     * have non-deterministic behavior and gives you both a stronger aggregate measure and a sense of the
     * variance in the results.
     */
    trialCount?: number;
    /**
     * Optional additional metadata for the experiment.
     */
    metadata?: Record<string, unknown>;
    /**
     * Whether the experiment should be public. Defaults to false.
     */
    isPublic?: boolean;
    /**
     * Whether to update an existing experiment with `experiment_name` if one exists. Defaults to false.
     */
    update?: boolean;
    /**
     * The duration, in milliseconds, after which to time out the evaluation.
     * Defaults to undefined, in which case there is no timeout.
     */
    timeout?: number;
    /**
     * The maximum number of tasks/scorers that will be run concurrently.
     * Defaults to undefined, in which case there is no max concurrency.
     */
    maxConcurrency?: number;
    /**
     * If specified, uses the given project ID instead of the evaluator's name to identify the project.
     */
    projectId?: string;
    /**
     * If specified, uses the logger state to initialize Braintrust objects. If unspecified, falls back
     * to the global state (initialized using your API key).
     */
    state?: BraintrustState;
    /**
     * An optional experiment name to use as a base. If specified, the new experiment will be summarized
     * and compared to this experiment.
     */
    baseExperimentName?: string;
    /**
     * An optional experiment id to use as a base. If specified, the new experiment will be summarized
     * and compared to this experiment. This takes precedence over `baseExperimentName` if specified.
     */
    baseExperimentId?: string;
}
declare class EvalResultWithSummary<Input, Output, Expected, Metadata extends BaseMetadata = DefaultMetadataType> {
    summary: ExperimentSummary;
    results: EvalResult<Input, Output, Expected, Metadata>[];
    constructor(summary: ExperimentSummary, results: EvalResult<Input, Output, Expected, Metadata>[]);
    toString(): string;
    toJSON(): {
        summary: ExperimentSummary;
        results: EvalResult<Input, Output, Expected, Metadata>[];
    };
}
interface ReporterOpts {
    verbose: boolean;
    jsonl: boolean;
}
interface ReporterBody<EvalReport> {
    /**
     * A function that takes an evaluator and its result and returns a report.
     *
     * @param evaluator
     * @param result
     * @param opts
     */
    reportEval(evaluator: EvaluatorDef<any, any, any, any>, result: EvalResultWithSummary<any, any, any, any>, opts: ReporterOpts): Promise<EvalReport> | EvalReport;
    /**
     * A function that takes all evaluator results and returns a boolean indicating
     * whether the run was successful. If you return false, the `braintrust eval`
     * command will exit with a non-zero status code.
     *
     * @param reports
     */
    reportRun(reports: EvalReport[]): boolean | Promise<boolean>;
}
type ReporterDef<EvalReport> = {
    name: string;
} & ReporterBody<EvalReport>;
type EvaluatorDef<Input, Output, Expected, Metadata extends BaseMetadata = DefaultMetadataType> = {
    projectName: string;
    evalName: string;
} & Evaluator<Input, Output, Expected, Metadata>;
type EvaluatorFile = {
    functions: CodeFunction<unknown, unknown, GenericFunction<unknown, unknown>>[];
    prompts: CodePrompt[];
    evaluators: {
        [evalName: string]: {
            evaluator: EvaluatorDef<unknown, unknown, unknown, BaseMetadata>;
            reporter?: ReporterDef<unknown> | string;
        };
    };
    reporters: {
        [reporterName: string]: ReporterDef<unknown>;
    };
};
type SpanContext = {
    currentSpan: typeof currentSpan;
    startSpan: typeof startSpan;
    withCurrent: typeof withCurrent;
    NOOP_SPAN: typeof NOOP_SPAN;
};
declare global {
    var _evals: EvaluatorFile;
    var _spanContext: SpanContext | undefined;
    var _lazy_load: boolean;
}
interface EvalOptions<EvalReport> {
    reporter?: ReporterDef<EvalReport> | string;
    onStart?: (metadata: Omit<ExperimentSummary, "scores" | "metrics">) => void;
}
declare function Eval<Input, Output, Expected = void, Metadata extends BaseMetadata = DefaultMetadataType, EvalReport = boolean>(name: string, evaluator: Evaluator<Input, Output, Expected, Metadata>, reporterOrOpts?: ReporterDef<EvalReport> | string | EvalOptions<EvalReport>): Promise<EvalResultWithSummary<Input, Output, Expected, Metadata>>;
declare function Reporter<EvalReport>(name: string, reporter: ReporterBody<EvalReport>): ReporterDef<EvalReport>;
declare function buildLocalSummary(evaluator: EvaluatorDef<any, any, any, any>, results: EvalResult<any, any, any, any>[]): ExperimentSummary;
declare function reportFailures<Input, Output, Expected, Metadata extends BaseMetadata>(evaluator: EvaluatorDef<Input, Output, Expected, Metadata>, failingResults: EvalResult<Input, Output, Expected, Metadata>[], { verbose, jsonl }: ReporterOpts): void;

interface BetaLike {
    chat: {
        completions: {
            stream: any;
        };
    };
    embeddings: any;
}
interface ChatLike {
    completions: any;
}
interface OpenAILike {
    chat: ChatLike;
    embeddings: any;
    beta?: BetaLike;
}
declare global {
    var __inherited_braintrust_wrap_openai: ((openai: any) => any) | undefined;
}
/**
 * Wrap an `OpenAI` object (created with `new OpenAI(...)`) to add tracing. If Braintrust is
 * not configured, this is a no-op
 *
 * Currently, this only supports the `v4` API.
 *
 * @param openai
 * @returns The wrapped `OpenAI` object.
 */
declare function wrapOpenAI<T extends object>(openai: T): T;
declare function wrapOpenAIv4<T extends OpenAILike>(openai: T): T;
declare const LEGACY_CACHED_HEADER = "x-cached";
declare const X_CACHED_HEADER = "x-bt-cached";
declare function parseCachedHeader(value: string | null | undefined): number | undefined;

/**
 * Wrap an ai-sdk model (created with `.chat()`, `.completion()`, etc.) to add tracing. If Braintrust is
 * not configured, this is a no-op
 *
 * @param model
 * @returns The wrapped object.
 */
declare function wrapAISDKModel<T extends object>(model: T): T;

type braintrust_AnyDataset = AnyDataset;
type braintrust_BackgroundLoggerOpts = BackgroundLoggerOpts;
declare const braintrust_BaseExperiment: typeof BaseExperiment;
type braintrust_BaseMetadata = BaseMetadata;
type braintrust_BraintrustState = BraintrustState;
declare const braintrust_BraintrustState: typeof BraintrustState;
type braintrust_BraintrustStream = BraintrustStream;
declare const braintrust_BraintrustStream: typeof BraintrustStream;
type braintrust_BraintrustStreamChunk = BraintrustStreamChunk;
type braintrust_ChatPrompt = ChatPrompt;
type braintrust_CodeFunction<Input, Output, Fn extends GenericFunction<Input, Output>> = CodeFunction<Input, Output, Fn>;
declare const braintrust_CodeFunction: typeof CodeFunction;
type braintrust_CodePrompt = CodePrompt;
declare const braintrust_CodePrompt: typeof CodePrompt;
declare const braintrust_CommentEvent: typeof CommentEvent;
type braintrust_CompiledPrompt<Flavor extends "chat" | "completion"> = CompiledPrompt<Flavor>;
type braintrust_CompiledPromptParams = CompiledPromptParams;
type braintrust_CompletionPrompt = CompletionPrompt;
type braintrust_CreateProjectOpts = CreateProjectOpts;
type braintrust_DataSummary = DataSummary;
type braintrust_Dataset<IsLegacyDataset extends boolean = typeof DEFAULT_IS_LEGACY_DATASET> = Dataset<IsLegacyDataset>;
declare const braintrust_Dataset: typeof Dataset;
declare const braintrust_DatasetRecord: typeof DatasetRecord;
type braintrust_DatasetSummary = DatasetSummary;
type braintrust_DefaultMetadataType = DefaultMetadataType;
type braintrust_DefaultPromptArgs = DefaultPromptArgs;
type braintrust_EndSpanArgs = EndSpanArgs;
declare const braintrust_Eval: typeof Eval;
type braintrust_EvalCase<Input, Expected, Metadata> = EvalCase<Input, Expected, Metadata>;
type braintrust_EvalResult<Input, Output, Expected, Metadata extends BaseMetadata = DefaultMetadataType> = EvalResult<Input, Output, Expected, Metadata>;
type braintrust_EvalScorer<Input, Output, Expected, Metadata extends BaseMetadata = DefaultMetadataType> = EvalScorer<Input, Output, Expected, Metadata>;
type braintrust_EvalScorerArgs<Input, Output, Expected, Metadata extends BaseMetadata = DefaultMetadataType> = EvalScorerArgs<Input, Output, Expected, Metadata>;
type braintrust_EvalTask<Input, Output> = EvalTask<Input, Output>;
type braintrust_Evaluator<Input, Output, Expected, Metadata extends BaseMetadata = DefaultMetadataType> = Evaluator<Input, Output, Expected, Metadata>;
type braintrust_EvaluatorDef<Input, Output, Expected, Metadata extends BaseMetadata = DefaultMetadataType> = EvaluatorDef<Input, Output, Expected, Metadata>;
type braintrust_EvaluatorFile = EvaluatorFile;
type braintrust_Experiment = Experiment;
declare const braintrust_Experiment: typeof Experiment;
declare const braintrust_ExperimentLogFullArgs: typeof ExperimentLogFullArgs;
declare const braintrust_ExperimentLogPartialArgs: typeof ExperimentLogPartialArgs;
type braintrust_ExperimentSummary = ExperimentSummary;
type braintrust_Exportable = Exportable;
type braintrust_FailedHTTPResponse = FailedHTTPResponse;
declare const braintrust_FailedHTTPResponse: typeof FailedHTTPResponse;
type braintrust_FullInitOptions<IsOpen extends boolean> = FullInitOptions<IsOpen>;
type braintrust_FullLoginOptions = FullLoginOptions;
declare const braintrust_IdField: typeof IdField;
type braintrust_InitOptions<IsOpen extends boolean> = InitOptions<IsOpen>;
declare const braintrust_InputField: typeof InputField;
declare const braintrust_InputsField: typeof InputsField;
type braintrust_InvokeFunctionArgs<Input, Output, Stream extends boolean = false> = InvokeFunctionArgs<Input, Output, Stream>;
type braintrust_InvokeReturn<Stream extends boolean, Output> = InvokeReturn<Stream, Output>;
declare const braintrust_LEGACY_CACHED_HEADER: typeof LEGACY_CACHED_HEADER;
type braintrust_LazyValue<T> = LazyValue<T>;
declare const braintrust_LazyValue: typeof LazyValue;
declare const braintrust_LogCommentFullArgs: typeof LogCommentFullArgs;
declare const braintrust_LogFeedbackFullArgs: typeof LogFeedbackFullArgs;
type braintrust_LogOptions<IsAsyncFlush> = LogOptions<IsAsyncFlush>;
type braintrust_Logger<IsAsyncFlush extends boolean> = Logger<IsAsyncFlush>;
declare const braintrust_Logger: typeof Logger;
type braintrust_LoginOptions = LoginOptions;
type braintrust_MetricSummary = MetricSummary;
declare const braintrust_NOOP_SPAN: typeof NOOP_SPAN;
type braintrust_NoopSpan = NoopSpan;
declare const braintrust_NoopSpan: typeof NoopSpan;
type braintrust_ObjectMetadata = ObjectMetadata;
declare const braintrust_OtherExperimentLogFields: typeof OtherExperimentLogFields;
declare const braintrust_ParentExperimentIds: typeof ParentExperimentIds;
declare const braintrust_ParentProjectLogIds: typeof ParentProjectLogIds;
type braintrust_Project = Project;
declare const braintrust_Project: typeof Project;
type braintrust_PromiseUnless<B, R> = PromiseUnless<B, R>;
type braintrust_Prompt<HasId extends boolean = true, HasVersion extends boolean = true> = Prompt<HasId, HasVersion>;
declare const braintrust_Prompt: typeof Prompt;
type braintrust_PromptBuilder = PromptBuilder;
declare const braintrust_PromptBuilder: typeof PromptBuilder;
type braintrust_PromptOpts<HasId extends boolean, HasVersion extends boolean> = PromptOpts<HasId, HasVersion>;
type braintrust_PromptRowWithId<HasId extends boolean = true, HasVersion extends boolean = true> = PromptRowWithId<HasId, HasVersion>;
type braintrust_ReadonlyExperiment = ReadonlyExperiment;
declare const braintrust_ReadonlyExperiment: typeof ReadonlyExperiment;
declare const braintrust_Reporter: typeof Reporter;
type braintrust_ReporterBody<EvalReport> = ReporterBody<EvalReport>;
type braintrust_ScoreSummary = ScoreSummary;
type braintrust_SerializedBraintrustState = SerializedBraintrustState;
type braintrust_SetCurrentArg = SetCurrentArg;
type braintrust_Span = Span;
type braintrust_SpanContext = SpanContext;
type braintrust_SpanImpl = SpanImpl;
declare const braintrust_SpanImpl: typeof SpanImpl;
type braintrust_StartSpanArgs = StartSpanArgs;
type braintrust_ToolBuilder = ToolBuilder;
declare const braintrust_ToolBuilder: typeof ToolBuilder;
type braintrust_ToolFunctionDefinition = ToolFunctionDefinition;
type braintrust_ToolOpts<Params, Returns, Fn extends GenericFunction<Params, Returns>> = ToolOpts<Params, Returns, Fn>;
type braintrust_WithTransactionId<R> = WithTransactionId<R>;
declare const braintrust_X_CACHED_HEADER: typeof X_CACHED_HEADER;
declare const braintrust__internalGetGlobalState: typeof _internalGetGlobalState;
declare const braintrust__internalSetInitialState: typeof _internalSetInitialState;
declare const braintrust_braintrustStreamChunkSchema: typeof braintrustStreamChunkSchema;
declare const braintrust_buildLocalSummary: typeof buildLocalSummary;
declare const braintrust_createFinalValuePassThroughStream: typeof createFinalValuePassThroughStream;
declare const braintrust_currentExperiment: typeof currentExperiment;
declare const braintrust_currentLogger: typeof currentLogger;
declare const braintrust_currentSpan: typeof currentSpan;
declare const braintrust_devNullWritableStream: typeof devNullWritableStream;
declare const braintrust_flush: typeof flush;
declare const braintrust_getSpanParentObject: typeof getSpanParentObject;
declare const braintrust_init: typeof init;
declare const braintrust_initDataset: typeof initDataset;
declare const braintrust_initExperiment: typeof initExperiment;
declare const braintrust_initLogger: typeof initLogger;
declare const braintrust_invoke: typeof invoke;
declare const braintrust_loadPrompt: typeof loadPrompt;
declare const braintrust_log: typeof log;
declare const braintrust_logError: typeof logError;
declare const braintrust_login: typeof login;
declare const braintrust_loginToState: typeof loginToState;
declare const braintrust_newId: typeof newId;
declare const braintrust_parseCachedHeader: typeof parseCachedHeader;
declare const braintrust_projects: typeof projects;
declare const braintrust_renderMessage: typeof renderMessage;
declare const braintrust_reportFailures: typeof reportFailures;
declare const braintrust_setFetch: typeof setFetch;
declare const braintrust_startSpan: typeof startSpan;
declare const braintrust_summarize: typeof summarize;
declare const braintrust_toolFunctionDefinitionSchema: typeof toolFunctionDefinitionSchema;
declare const braintrust_traceable: typeof traceable;
declare const braintrust_traced: typeof traced;
declare const braintrust_updateSpan: typeof updateSpan;
declare const braintrust_withCurrent: typeof withCurrent;
declare const braintrust_withDataset: typeof withDataset;
declare const braintrust_withExperiment: typeof withExperiment;
declare const braintrust_withLogger: typeof withLogger;
declare const braintrust_wrapAISDKModel: typeof wrapAISDKModel;
declare const braintrust_wrapOpenAI: typeof wrapOpenAI;
declare const braintrust_wrapOpenAIv4: typeof wrapOpenAIv4;
declare const braintrust_wrapTraced: typeof wrapTraced;
declare namespace braintrust {
  export { type braintrust_AnyDataset as AnyDataset, type braintrust_BackgroundLoggerOpts as BackgroundLoggerOpts, braintrust_BaseExperiment as BaseExperiment, type braintrust_BaseMetadata as BaseMetadata, braintrust_BraintrustState as BraintrustState, braintrust_BraintrustStream as BraintrustStream, type braintrust_BraintrustStreamChunk as BraintrustStreamChunk, type braintrust_ChatPrompt as ChatPrompt, braintrust_CodeFunction as CodeFunction, braintrust_CodePrompt as CodePrompt, braintrust_CommentEvent as CommentEvent, type braintrust_CompiledPrompt as CompiledPrompt, type braintrust_CompiledPromptParams as CompiledPromptParams, type braintrust_CompletionPrompt as CompletionPrompt, type braintrust_CreateProjectOpts as CreateProjectOpts, type braintrust_DataSummary as DataSummary, braintrust_Dataset as Dataset, braintrust_DatasetRecord as DatasetRecord, type braintrust_DatasetSummary as DatasetSummary, type braintrust_DefaultMetadataType as DefaultMetadataType, type braintrust_DefaultPromptArgs as DefaultPromptArgs, type braintrust_EndSpanArgs as EndSpanArgs, braintrust_Eval as Eval, type braintrust_EvalCase as EvalCase, type braintrust_EvalResult as EvalResult, type braintrust_EvalScorer as EvalScorer, type braintrust_EvalScorerArgs as EvalScorerArgs, type braintrust_EvalTask as EvalTask, type braintrust_Evaluator as Evaluator, type braintrust_EvaluatorDef as EvaluatorDef, type braintrust_EvaluatorFile as EvaluatorFile, braintrust_Experiment as Experiment, braintrust_ExperimentLogFullArgs as ExperimentLogFullArgs, braintrust_ExperimentLogPartialArgs as ExperimentLogPartialArgs, type braintrust_ExperimentSummary as ExperimentSummary, type braintrust_Exportable as Exportable, braintrust_FailedHTTPResponse as FailedHTTPResponse, type braintrust_FullInitOptions as FullInitOptions, type braintrust_FullLoginOptions as FullLoginOptions, braintrust_IdField as IdField, type braintrust_InitOptions as InitOptions, braintrust_InputField as InputField, braintrust_InputsField as InputsField, type braintrust_InvokeFunctionArgs as InvokeFunctionArgs, type braintrust_InvokeReturn as InvokeReturn, braintrust_LEGACY_CACHED_HEADER as LEGACY_CACHED_HEADER, braintrust_LazyValue as LazyValue, braintrust_LogCommentFullArgs as LogCommentFullArgs, braintrust_LogFeedbackFullArgs as LogFeedbackFullArgs, type braintrust_LogOptions as LogOptions, braintrust_Logger as Logger, type braintrust_LoginOptions as LoginOptions, type braintrust_MetricSummary as MetricSummary, braintrust_NOOP_SPAN as NOOP_SPAN, braintrust_NoopSpan as NoopSpan, type braintrust_ObjectMetadata as ObjectMetadata, braintrust_OtherExperimentLogFields as OtherExperimentLogFields, braintrust_ParentExperimentIds as ParentExperimentIds, braintrust_ParentProjectLogIds as ParentProjectLogIds, braintrust_Project as Project, type braintrust_PromiseUnless as PromiseUnless, braintrust_Prompt as Prompt, braintrust_PromptBuilder as PromptBuilder, type braintrust_PromptOpts as PromptOpts, type braintrust_PromptRowWithId as PromptRowWithId, braintrust_ReadonlyExperiment as ReadonlyExperiment, braintrust_Reporter as Reporter, type braintrust_ReporterBody as ReporterBody, type braintrust_ScoreSummary as ScoreSummary, type braintrust_SerializedBraintrustState as SerializedBraintrustState, type braintrust_SetCurrentArg as SetCurrentArg, type braintrust_Span as Span, type braintrust_SpanContext as SpanContext, braintrust_SpanImpl as SpanImpl, type braintrust_StartSpanArgs as StartSpanArgs, braintrust_ToolBuilder as ToolBuilder, type braintrust_ToolFunctionDefinition as ToolFunctionDefinition, type braintrust_ToolOpts as ToolOpts, type braintrust_WithTransactionId as WithTransactionId, braintrust_X_CACHED_HEADER as X_CACHED_HEADER, braintrust__internalGetGlobalState as _internalGetGlobalState, braintrust__internalSetInitialState as _internalSetInitialState, braintrust_braintrustStreamChunkSchema as braintrustStreamChunkSchema, braintrust_buildLocalSummary as buildLocalSummary, braintrust_createFinalValuePassThroughStream as createFinalValuePassThroughStream, braintrust_currentExperiment as currentExperiment, braintrust_currentLogger as currentLogger, braintrust_currentSpan as currentSpan, braintrust_devNullWritableStream as devNullWritableStream, braintrust_flush as flush, braintrust_getSpanParentObject as getSpanParentObject, braintrust_init as init, braintrust_initDataset as initDataset, braintrust_initExperiment as initExperiment, braintrust_initLogger as initLogger, braintrust_invoke as invoke, braintrust_loadPrompt as loadPrompt, braintrust_log as log, braintrust_logError as logError, braintrust_login as login, braintrust_loginToState as loginToState, braintrust_newId as newId, braintrust_parseCachedHeader as parseCachedHeader, braintrust_projects as projects, braintrust_renderMessage as renderMessage, braintrust_reportFailures as reportFailures, braintrust_setFetch as setFetch, braintrust_startSpan as startSpan, braintrust_summarize as summarize, braintrust_toolFunctionDefinitionSchema as toolFunctionDefinitionSchema, braintrust_traceable as traceable, braintrust_traced as traced, braintrust_updateSpan as updateSpan, braintrust_withCurrent as withCurrent, braintrust_withDataset as withDataset, braintrust_withExperiment as withExperiment, braintrust_withLogger as withLogger, braintrust_wrapAISDKModel as wrapAISDKModel, braintrust_wrapOpenAI as wrapOpenAI, braintrust_wrapOpenAIv4 as wrapOpenAIv4, braintrust_wrapTraced as wrapTraced };
}

/**
 * An isomorphic JS library for working with [Braintrust](https://braintrust.dev/). This library contains functionality
 * for running evaluations, logging completions, loading and invoking functions, and more.
 *
 * `braintrust` is distributed as a [library on NPM](https://www.npmjs.com/package/braintrust).
 * It is also open source and available on [GitHub](https://github.com/braintrustdata/braintrust-sdk/tree/main/js).
 *
 * ### Quickstart
 *
 * Install the library with npm (or yarn).
 *
 * ```bash
 * npm install braintrust
 * ```
 *
 * Then, create a file like `hello.eval.ts` with the following content:
 *
 * ```javascript
 * import { Eval } from "braintrust";
 *
 * function isEqual({ output, expected }: { output: string; expected?: string }) {
 *   return { name: "is_equal", score: output === expected ? 1 : 0 };
 * }
 *
 * Eval("Say Hi Bot", {
 *   data: () => {
 *     return [
 *       {
 *         input: "Foo",
 *         expected: "Hi Foo",
 *       },
 *       {
 *         input: "Bar",
 *         expected: "Hello Bar",
 *       },
 *     ]; // Replace with your eval dataset
 *   },
 *   task: (input: string) => {
 *     return "Hi " + input; // Replace with your LLM call
 *   },
 *   scores: [isEqual],
 * });
 * ```
 *
 * Finally, run the script with `npx braintrust eval hello.eval.ts`.
 *
 * ```bash
 * BRAINTRUST_API_KEY=<YOUR_BRAINTRUST_API_KEY> npx braintrust eval hello.eval.ts
 * ```
 *
 * @module braintrust
 */

export { type AnyDataset, type BackgroundLoggerOpts, BaseExperiment, type BaseMetadata, BraintrustState, BraintrustStream, type BraintrustStreamChunk, type ChatPrompt, CodeFunction, CodePrompt, type CompiledPrompt, type CompiledPromptParams, type CompletionPrompt, type CreateProjectOpts, type DataSummary, Dataset, type DatasetSummary, type DefaultMetadataType, type DefaultPromptArgs, type EndSpanArgs, Eval, type EvalCase, type EvalResult, type EvalScorer, type EvalScorerArgs, type EvalTask, type Evaluator, type EvaluatorDef, type EvaluatorFile, Experiment, type ExperimentSummary, type Exportable, FailedHTTPResponse, type FullInitOptions, type FullLoginOptions, type InitOptions, type InvokeFunctionArgs, type InvokeReturn, LEGACY_CACHED_HEADER, LazyValue, type LogOptions, Logger, type LoginOptions, type MetricSummary, NOOP_SPAN, NoopSpan, type ObjectMetadata, Project, type PromiseUnless, Prompt, PromptBuilder, type PromptOpts, type PromptRowWithId, ReadonlyExperiment, Reporter, type ReporterBody, type ScoreSummary, type SerializedBraintrustState, type SetCurrentArg, type Span, type SpanContext, SpanImpl, type StartSpanArgs, ToolBuilder, type ToolFunctionDefinition, type ToolOpts, type WithTransactionId, X_CACHED_HEADER, _internalGetGlobalState, _internalSetInitialState, braintrustStreamChunkSchema, buildLocalSummary, createFinalValuePassThroughStream, currentExperiment, currentLogger, currentSpan, braintrust as default, devNullWritableStream, flush, getSpanParentObject, init, initDataset, initExperiment, initLogger, invoke, loadPrompt, log, logError, login, loginToState, newId, parseCachedHeader, projects, renderMessage, reportFailures, setFetch, startSpan, summarize, toolFunctionDefinitionSchema, traceable, traced, updateSpan, withCurrent, withDataset, withExperiment, withLogger, wrapAISDKModel, wrapOpenAI, wrapOpenAIv4, wrapTraced };
