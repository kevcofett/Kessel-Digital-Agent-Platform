DOCUMENT: MPA_Expert_Lens_Audience_Strategy_v5_5.txt
CATEGORY: Expert Lens
TOPICS: audience strategy, targeting, segmentation, audience definition, customer personas

MPA EXPERT LENS: AUDIENCE STRATEGY
VERSION: 5.5
DATE: January 2026
STATUS: Production Ready
COMPLIANCE: 6-Rule Compliant
================================================================================
SECTION 1: PURPOSE AND SCOPE
================================================================================

1.1 PURPOSE

This document provides diagnostic reasoning patterns and expert-level guidance for audience strategy decisions. The agent MUST use this document to identify targeting issues, surface audience implications, and ask questions that strengthen the media plan.

1.2 SCOPE

This document covers:
- Critical audience considerations that determine success
- Common pitfalls with recommended alternatives
- Diagnostic signals indicating audience problems
- Audience overlap and segmentation guidance
- Questions worth asking that users may not consider
- Benchmark reference points for audience planning

================================================================================
SECTION 2: WHAT MATTERS MOST
================================================================================

2.1 THE FIVE CRITICAL AUDIENCE CONSIDERATIONS

CONSIDERATION 1: PRECISION VERSUS SCALE TRADEOFF
Narrow targeting increases relevance but limits reach. Broad targeting increases reach but reduces efficiency. The right balance depends on objectives and budget.

Why it matters: Too narrow wastes budget on high frequency to small audiences. Too broad wastes budget on irrelevant impressions.

CONSIDERATION 2: FIRST-PARTY DATA ADVANTAGE
First-party data enables the most precise and efficient targeting. Lookalike audiences from high-value customers typically outperform prospecting by 40-60 percent.

Why it matters: Advertisers with strong first-party data have significant competitive advantage. Those without must rely on less accurate platform signals.

CONSIDERATION 3: AUDIENCE QUALITY OVER QUANTITY
Not all impressions are equal. Reaching the right audience at lower volume beats reaching the wrong audience at high volume.

Why it matters: Poor audience targeting is the most common cause of campaign underperformance. Quality targeting dramatically improves efficiency.

CONSIDERATION 4: AUDIENCE OVERLAP ACROSS PLATFORMS
Different platforms often reach the same users. Without coordination, audiences receive excessive frequency or redundant targeting.

Why it matters: Unmanaged overlap wastes budget and can cause audience fatigue. Coordinated approach maximizes incremental reach.

CONSIDERATION 5: AUDIENCE EVOLUTION OVER TIME
Audiences are not static. Customer profiles change, new segments emerge, existing segments shrink or grow.

Why it matters: Audiences that worked historically may not work today. Regular audience refresh and testing maintains effectiveness.

================================================================================
SECTION 3: COMMON PITFALLS
================================================================================

3.1 PITFALL: DEMOGRAPHIC-ONLY TARGETING

What it looks like:
- Audience defined only by age, gender, income
- No behavioral or intent signals included
- Assumption that demographics predict behavior

Why it happens:
- Demographics are familiar and easy to define
- Legacy targeting approaches
- Limited data or platform capability

Why it matters:
- Demographics poorly predict purchase behavior
- Large portions of demographic audiences are irrelevant
- Better signals are available on most platforms

Better approach:
- Start with behavioral and intent signals
- Use demographics as secondary refinement
- Test demographic expansion when performance allows

3.2 PITFALL: OVER-RELIANCE ON LOOKALIKES

What it looks like:
- All prospecting uses lookalike audiences
- No testing of interest or behavior-based targeting
- Lookalikes not refreshed regularly

Why it happens:
- Lookalikes perform well initially
- Easy to set up and scale
- Less strategic thinking required

Why it matters:
- Lookalike quality degrades over time
- Seed audience may not represent best prospects
- Algorithm may optimize for easy conversions, not best customers

Better approach:
- Test lookalikes against interest and behavior targeting
- Refresh seed audiences regularly
- Use high-value customer seeds, not all customers
- Monitor new-to-brand and customer quality metrics

3.3 PITFALL: AUDIENCE TOO NARROW FOR BUDGET

What it looks like:
- Small audience size with large budget
- Very high frequency delivery
- Rapid creative fatigue

Why it happens:
- Desire for precision targeting
- Underestimation of audience saturation
- No frequency monitoring

Why it matters:
- Excessive frequency annoys audiences
- Creative wears out quickly
- Diminishing returns on additional spend

Better approach:
- Match audience size to budget and duration
- Set frequency caps
- Monitor creative fatigue signals
- Expand audience when frequency exceeds optimal

3.4 PITFALL: AUDIENCE TOO BROAD FOR BUDGET

What it looks like:
- Massive audience size with limited budget
- Very low frequency delivery
- No measurable impact

Why it happens:
- Fear of missing potential customers
- Lack of prioritization
- Platform recommendations to go broad

Why it matters:
- Insufficient frequency to create impact
- Cannot measure effectiveness
- Budget spread too thin for learning

Better approach:
- Narrow to highest-priority segments first
- Build frequency before expanding reach
- Test audience expansion once core is working

3.5 PITFALL: NO AUDIENCE SUPPRESSION

What it looks like:
- Acquisition campaigns reaching existing customers
- Retargeting reaching recent purchasers
- Wasted spend on wrong audience

Why it happens:
- Suppression lists not maintained
- Platform limitations on suppression
- Siloed campaign management

Why it matters:
- Budget wasted on already-converted customers
- Attribution inflated by reaching existing customers
- True acquisition cost understated

Better approach:
- Upload customer suppression lists regularly
- Use platform exclusions for recent purchasers
- Monitor new-to-brand rate as quality check

================================================================================
SECTION 4: DIAGNOSTIC SIGNALS
================================================================================

4.1 IF ACQUISITION COSTS ARE RISING

What to check:
- Is audience exhausted at current targeting
- Has competition increased
- Is creative fatigue occurring

Questions to ask:
- How has acquisition cost trended over time
- What is the frequency to this audience
- When did costs start rising and what changed

Typical implications:
- May need to expand audience
- May need creative refresh
- May indicate market saturation

4.2 IF NEW-TO-BRAND RATE IS LOW

What to check:
- Is campaign reaching existing customers
- Are suppression lists current
- Is algorithm over-optimizing for easy conversions

Questions to ask:
- What percentage of conversions are new versus returning
- How are you defining new-to-brand
- Is suppression working correctly

Typical implications:
- Campaign may be retargeting not prospecting
- True acquisition efficiency is worse than reported
- Need to adjust targeting or suppression

4.3 IF FREQUENCY IS VERY HIGH

What to check:
- Is audience too small for budget
- Are frequency caps in place
- Is same user being reached across platforms

Questions to ask:
- What is your current frequency
- What is cross-platform frequency
- At what frequency have you seen diminishing returns

Typical implications:
- Audience may be saturated
- Creative fatigue likely occurring
- Consider expanding audience or reducing budget

4.4 IF FREQUENCY IS VERY LOW

What to check:
- Is audience too large for budget
- Is targeting effective
- Are impressions going to right people

Questions to ask:
- What audience size are you targeting
- How does audience size compare to budget
- Are you seeing engagement from this audience

Typical implications:
- May not be reaching threshold for impact
- Consider narrowing audience
- May need to prioritize segments

4.5 IF CUSTOMER QUALITY IS DECLINING

What to check:
- Has targeting changed
- Has audience composition shifted
- Are algorithms optimizing for volume over value

Questions to ask:
- How are you measuring customer quality
- What is the LTV trend of new customers
- Has targeting or optimization changed recently

Typical implications:
- Targeting may need value-based adjustment
- Lookalike seeds may need updating
- Need to balance volume and quality objectives

================================================================================
SECTION 5: AUDIENCE SEGMENTATION GUIDANCE
================================================================================

5.1 PROSPECTING AUDIENCE TIERS

TIER 1: HIGH-VALUE LOOKALIKES
- Source: Top 10-20 percent customers by LTV
- Typical performance: Best efficiency, limited scale
- When to use: Initial prospecting, efficiency priority

TIER 2: BROAD LOOKALIKES
- Source: All customers or all converters
- Typical performance: Good efficiency, moderate scale
- When to use: Expanding beyond tier 1

TIER 3: INTEREST AND BEHAVIOR
- Source: Platform interest and behavior signals
- Typical performance: Moderate efficiency, good scale
- When to use: Testing beyond lookalikes, scale priority

TIER 4: BROAD TARGETING
- Source: Minimal targeting, algorithm-driven
- Typical performance: Variable efficiency, maximum scale
- When to use: Brand awareness, very large budgets

5.2 RETARGETING AUDIENCE TIERS

TIER 1: CART ABANDONERS
- Source: Users who added to cart but did not purchase
- Typical performance: Highest conversion rate
- Caution: Often would convert anyway, validate incrementality

TIER 2: PRODUCT VIEWERS
- Source: Users who viewed products
- Typical performance: Good conversion rate
- Caution: Recency matters significantly

TIER 3: SITE VISITORS
- Source: All site visitors
- Typical performance: Moderate conversion rate
- Caution: Quality varies by page visited

TIER 4: ENGAGED USERS
- Source: Email openers, video viewers, social engagers
- Typical performance: Variable conversion rate
- Caution: Engagement does not always predict purchase

5.3 AUDIENCE SIZING GUIDELINES

Minimum viable audience sizes by platform:

- Meta: 100,000 minimum for effective optimization
- Google Display: 100,000 minimum
- LinkedIn: 50,000 minimum
- TikTok: 100,000 minimum
- Programmatic: Varies by inventory, typically 100,000 plus

Budget to audience ratio guidance:

- 1M audience size typically supports 5-10K monthly spend at healthy frequency
- Below 100K audience, monthly spend should typically stay under 2-3K
- Very large audiences of 10M plus can absorb significant budget

================================================================================
SECTION 6: QUESTIONS WORTH ASKING
================================================================================

6.1 FOUNDATION QUESTIONS

These questions establish audience strategy context:

- Who is your ideal customer and why (Reveals targeting foundation)
- How do you define customer value and what makes a high-value customer (Reveals quality criteria)
- What first-party data do you have available for targeting (Reveals data assets)
- How do your current customers differ from your ideal prospects (Reveals expansion opportunity)

6.2 DATA AND CAPABILITY QUESTIONS

These questions assess targeting capability:

- Do you have CRM or customer data that can be uploaded to platforms (Reveals 1P data)
- How large is your email list and how engaged is it (Reveals retargeting potential)
- Do you have pixel or tracking data across your digital properties (Reveals behavioral data)
- What customer segmentation do you use internally (Reveals analysis sophistication)

6.3 PERFORMANCE QUESTIONS

These questions assess audience effectiveness:

- Which audiences have performed best historically (Reveals learnings)
- What is your typical new-to-brand rate (Reveals acquisition health)
- At what frequency have you seen diminishing returns (Reveals saturation thresholds)
- How do you balance reaching new customers versus engaging existing ones (Reveals philosophy)

6.4 OVERLAP AND COORDINATION QUESTIONS

These questions assess cross-platform approach:

- Do you coordinate audiences across platforms (Reveals sophistication)
- How do you handle overlap between retargeting and prospecting (Reveals segmentation)
- Do you suppress across platforms or just within each (Reveals waste management)
- How do you manage frequency across the media mix (Reveals frequency management)

================================================================================
SECTION 7: BENCHMARKS AND REFERENCE POINTS
================================================================================

7.1 AUDIENCE PERFORMANCE BENCHMARKS

Typical efficiency by audience type relative to broad prospecting:

- High-value lookalikes: 40-60 percent better CAC
- All-customer lookalikes: 20-40 percent better CAC
- Interest-based: 0-20 percent better CAC
- Retargeting: 50-70 percent better CAC (but lower incrementality)

7.2 FREQUENCY BENCHMARKS

Optimal frequency ranges by objective:

- Brand awareness: 3-5 exposures per week
- Consideration: 2-4 exposures per week
- Direct response: 3-7 exposures per week
- Retargeting: 5-10 exposures per week (with recency decay)

Signs of excessive frequency:
- Click-through rate declining
- Negative feedback increasing
- Conversion rate declining despite impressions

7.3 NEW-TO-BRAND BENCHMARKS

Healthy new-to-brand rates by campaign type:

- Pure prospecting: 80-95 percent should be new
- Mixed campaigns: 50-70 percent new
- Retargeting heavy: 20-40 percent new
- Full funnel: 40-60 percent new

Below these ranges indicates acquisition campaigns are reaching existing customers.

7.4 AUDIENCE REFRESH BENCHMARKS

Recommended refresh cadence:

- Lookalike seed audiences: Quarterly refresh
- Retargeting audiences: Continuous with 30-90 day windows
- Suppression lists: Weekly or more frequent
- Interest and behavior testing: Monthly new tests

================================================================================
SECTION 8: FALLBACK BEHAVIOR
================================================================================

8.1 IF FIRST-PARTY DATA IS UNAVAILABLE

When client has no 1P data:
- Recommend platform interest and behavior targeting
- Suggest building 1P data capture for future
- Use vertical benchmarks for efficiency expectations
- Flag reduced targeting precision in projections

8.2 IF AUDIENCE SIZE IS UNKNOWN

When audience sizing is unclear:
- Use platform audience estimation tools
- Recommend starting narrow and expanding
- Build in testing budget for audience validation
- Set frequency caps as safeguard

8.3 IF CUSTOMER VALUE DATA IS UNAVAILABLE

When LTV or value data does not exist:
- Use recency and purchase frequency as proxies
- Recommend value data capture for future
- Use all-customer lookalikes instead of high-value
- Note reduced targeting precision

8.4 IF CROSS-PLATFORM COORDINATION IS NOT POSSIBLE

When platforms cannot be coordinated:
- Acknowledge overlap and efficiency loss
- Factor redundancy into reach estimates
- Recommend platform prioritization instead of spreading
- Monitor for frequency complaints

================================================================================
SECTION 9: CROSS-REFERENCES
================================================================================

9.1 RELATED DOCUMENTS

- FIRST_PARTY_DATA_STRATEGY_v5_5.txt - Data activation guidance
- Analytics Engine v5.1 - LTV and customer value calculations
- Channel seed data - Platform-specific targeting capabilities
- MPA_Implications_Audience_Targeting_v5_5_v5_5.txt - Downstream effects
- MPA_Supporting_Instructions_v5_5.txt - Communication patterns

9.2 WEB SEARCH TRIGGERS

The agent SHOULD use web search when:
- Platform-specific targeting updates needed
- Privacy regulation impacts on targeting
- Emerging audience data sources
- Vertical-specific audience benchmarks

================================================================================
VERSION HISTORY
================================================================================

Version 1.0 - January 2026 - Initial creation
- Established critical audience considerations
- Documented common pitfalls with better approaches
- Added diagnostic signals for audience issues
- Included segmentation guidance
- Added questions worth asking

================================================================================
END OF DOCUMENT
================================================================================
