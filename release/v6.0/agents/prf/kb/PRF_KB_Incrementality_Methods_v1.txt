PRF KNOWLEDGE BASE - INCREMENTALITY TESTING METHODS

OVERVIEW

This document provides comprehensive methodology for measuring incremental lift from marketing activities. Incrementality testing isolates the causal effect of marketing by comparing outcomes between exposed and unexposed groups. Unlike correlation-based attribution, incrementality answers the fundamental question of what would have happened without the marketing intervention.

Incrementality measurement is essential for accurate marketing investment decisions. Without causal evidence, marketers risk optimizing toward channels that capture existing demand rather than creating new demand. True incrementality separates persuasion from selection effects and organic behavior.

INCREMENTALITY FUNDAMENTALS

Incremental lift measures the additional conversions or revenue generated by marketing that would not have occurred otherwise. The formula is simple but the measurement is complex. Incremental lift equals conversions with marketing minus conversions without marketing.

The challenge is that we cannot observe the same user in both states simultaneously. We must use statistical techniques to estimate the counterfactual outcome. This requires careful experimental design or sophisticated quasi-experimental methods.

Selection bias is the primary threat to incrementality measurement. Users who see ads differ systematically from users who do not. They may be more likely to convert regardless of ad exposure. Without proper controls, we attribute organic conversions to marketing.

True incrementality accounts for:
- Organic baseline behavior
- Selection effects in ad delivery
- Competitive switching versus market expansion
- Cannibalization across channels
- Time-shifted conversions

GEO HOLDOUT METHODOLOGY

THEORETICAL FOUNDATION

Geo holdout experiments randomly assign geographic regions to treatment and control conditions. Treatment regions receive normal marketing while control regions receive no marketing or reduced marketing. The difference in outcomes measures incremental lift.

Geographic randomization eliminates selection bias at the user level. Ad platforms cannot selectively target users within a region. All users in treatment regions have equal opportunity for exposure. This preserves the integrity of the experimental comparison.

Geo experiments work because geographic regions are independent experimental units. What happens in one region does not directly affect outcomes in another region. This assumption holds when regions are sufficiently separated and when there is no significant cross-regional commerce.

REGION SELECTION CRITERIA

Selecting appropriate regions is critical for valid geo experiments. Regions must be:
- Large enough for statistical power
- Similar enough for valid comparison
- Independent of each other
- Representative of the target market

Minimum population thresholds depend on baseline conversion rates. For conversion rates around 1 percent, regions need at least 10000 potential customers per week. For rates around 0.1 percent, minimum increases to 100000. Insufficient population reduces statistical power.

Use historical data to assess region comparability. Compare metrics like baseline conversion rates, average order values, demographic profiles, and seasonal patterns. Matched pairs or stratified randomization improves balance.

Avoid regions with significant cross-border commerce. If customers in a control region frequently purchase from treatment region stores, contamination occurs. Use natural boundaries like mountains, rivers, or sparse population areas.

EXPERIMENT DESIGN PARAMETERS

Duration must be sufficient to observe the full customer journey. For short consideration cycles, two to four weeks may suffice. For longer cycles, extend to eight to twelve weeks. Include at least one complete business cycle to capture weekly patterns.

Holdout percentage balances power against business cost. Common splits are 90/10 or 80/20 for treatment/control. Larger holdout improves precision but sacrifices more revenue. Never drop below 10 percent control for adequate power.

Pre-experiment calibration period establishes baseline equivalence. Run two to four weeks without any treatment differences. Confirm that future treatment and control regions have similar metrics. If imbalanced, re-randomize or use difference-in-differences.

Power analysis determines required sample size:
- Specify minimum detectable effect (typically 5 to 10 percent lift)
- Set significance level (typically 0.05)
- Set power level (typically 0.80)
- Calculate required region-weeks of data

ANALYSIS APPROACH

Calculate lift as the percentage difference in conversion rates between treatment and control regions. Use weighted averages if regions differ in size. Apply appropriate statistical tests for significance.

Standard error calculation must account for regional clustering. Users within regions are not independent observations. Use cluster-robust standard errors or hierarchical models. Ignoring clustering understates uncertainty.

Lift calculation formula:
- Treatment conversion rate = Treatment conversions / Treatment population
- Control conversion rate = Control conversions / Control population
- Lift = (Treatment rate - Control rate) / Control rate

Confidence intervals use the delta method or bootstrap resampling. Report point estimate and 95 percent interval. Intervals crossing zero indicate non-significant results.

HANDLING SEASONALITY AND TRENDS

Seasonal patterns affect both treatment and control equally if randomization is proper. However, seasonal trends can increase variance and reduce precision. Plan experiments during relatively stable periods when possible.

If experiments must span seasonal transitions, use difference-in-differences. Compare pre-to-post changes in treatment versus control. This removes common trends and isolates the treatment effect.

Time series analysis can improve precision by modeling expected outcomes. Fit models to pre-experiment data and project forward. Measure lift relative to projected counterfactual rather than concurrent control.

DIFFERENCE IN DIFFERENCES

THEORETICAL FOUNDATION

Difference-in-differences compares changes over time between treatment and control groups. It removes time-invariant confounders and common trends. The key assumption is parallel trends: absent treatment, both groups would have followed the same trajectory.

The DID estimator is:
- First difference: Post minus pre within each group
- Second difference: Treatment change minus control change

This double differencing eliminates:
- Level differences between groups (selection bias)
- Time trends common to both groups
- Seasonal effects shared by both groups

DID is quasi-experimental because treatment assignment may not be random. Validity depends on the parallel trends assumption. Use pre-treatment data to assess whether groups followed similar trajectories.

PARALLEL TRENDS VALIDATION

Plot pre-treatment outcomes for both groups over time. Visually assess whether trends are parallel. Formal tests compare slopes or test for pre-treatment differences in trends.

If parallel trends are violated, DID estimates are biased. Consider:
- Matching methods to improve comparability
- Synthetic control for better counterfactual
- Instrumental variables if available
- Alternative identification strategies

Warning signs of parallel trends violations:
- Diverging pre-treatment trajectories
- Differential seasonality
- Different responses to common shocks
- Selection on anticipated treatment effects

DATA REQUIREMENTS

DID requires panel data with multiple time periods for each unit. Minimum two periods (pre and post) but more periods strengthen parallel trends testing. Weekly or daily data enables trend assessment.

Required data structure:
- Unit identifier (region, customer, etc)
- Time period indicator
- Treatment indicator
- Outcome variable
- Covariates for robustness checks

Treatment timing can be uniform or staggered. Uniform timing means all treated units receive treatment simultaneously. Staggered timing means different units treated at different times. Staggered designs require specialized estimators.

ESTIMATION METHODS

Basic DID regression includes unit fixed effects, time fixed effects, and treatment indicator:
- Outcome = Unit fixed effect + Time fixed effect + Treatment indicator + Error
- Treatment coefficient is the DID estimate

Cluster standard errors at the treatment assignment level. If treatment is at the region level, cluster by region. Clustering accounts for serial correlation within units.

Add covariates to improve precision:
- Pre-treatment characteristics interacted with time
- Time-varying controls
- Trends specific to unit characteristics

Robust DID methods address recent methodological advances:
- Callaway-SantAnna for staggered adoption
- Sun-Abraham for heterogeneous effects
- De Chaisemartin-DHaultfoeuille for varying treatment timing

INTERPRETATION GUIDELINES

DID estimates average treatment effect on the treated. This may differ from average treatment effect for the population. Extrapolation requires assumption that effects are similar for untreated units.

Decompose total effect into intensive and extensive margins. Intensive margin is effect on existing customers. Extensive margin is new customer acquisition. Each may have different strategic implications.

Report effect sizes in meaningful units:
- Absolute lift in conversions or revenue
- Percentage lift relative to control
- Return on incremental spend
- Incremental cost per acquisition

PROPENSITY SCORE MATCHING

THEORETICAL FOUNDATION

Propensity score matching creates comparable treatment and control groups based on observable characteristics. The propensity score is the probability of receiving treatment given covariates. Matching on propensity score balances all observed covariates simultaneously.

The key assumption is selection on observables (ignorability). Conditional on observed covariates, treatment assignment is as good as random. Unobserved confounders violate this assumption and bias results.

PSM is useful when:
- Randomized experiment is not possible
- Treatment and control groups differ substantially
- Rich covariate data is available
- Unobserved confounding is unlikely

PROPENSITY SCORE ESTIMATION

Estimate propensity scores using logistic regression or machine learning:
- Outcome: Treatment indicator (0/1)
- Predictors: All pre-treatment covariates
- Model: Logistic regression, random forest, gradient boosting

Include all variables that predict both treatment and outcome. Missing relevant predictors causes omitted variable bias. Include interaction terms and transformations if relationships are nonlinear.

Check propensity score overlap. Treatment and control groups must have overlapping propensity score distributions. No overlap in some regions means those units cannot be matched. Trim or weight to address limited overlap.

Common support requirement:
- Propensity scores must range between 0 and 1
- Both groups must have observations across the score range
- Exclude units with extreme scores (below 0.05 or above 0.95)

MATCHING ALGORITHMS

Nearest neighbor matching finds the closest control unit for each treatment unit based on propensity score. Options include one-to-one or one-to-many matching. Caliper restricts matches to within specified distance.

Radius matching uses all control units within specified radius. Provides more matches but may include poor matches. Balance precision against match quality.

Kernel matching weights all control units by distance. Closer controls receive higher weight. Bandwidth parameter controls weight decay. Data-driven bandwidth selection is recommended.

Full matching optimally partitions sample into matched sets. Each set contains at least one treatment and one control. Minimizes overall distance while using all observations.

Matching recommendations:
- Start with nearest neighbor with caliper
- Check balance diagnostics
- Try alternative methods as sensitivity check
- Use weights from matching in outcome analysis

BALANCE DIAGNOSTICS

Matching must achieve covariate balance between groups. Compare means and distributions of all covariates after matching. Standardized mean difference should be below 0.1 for good balance.

Balance metrics to assess:
- Standardized mean difference for each covariate
- Variance ratios
- Distribution overlap statistics
- Joint balance tests

If balance is inadequate:
- Re-specify propensity score model
- Include additional covariates or transformations
- Try alternative matching algorithms
- Consider different estimators like weighting

OUTCOME ANALYSIS

After matching, estimate treatment effect by comparing outcomes:
- Simple comparison of means
- Regression adjustment for remaining imbalance
- Weighted regression using matching weights

Standard errors must account for matching uncertainty. Bootstrap or analytical corrections are required. Naive standard errors understate uncertainty.

Sensitivity analysis assesses robustness to unobserved confounding. How much unobserved confounding would be required to nullify results? Use Rosenbaum bounds or simulation-based methods.

SYNTHETIC CONTROL METHOD

THEORETICAL FOUNDATION

Synthetic control creates a weighted combination of untreated units that matches the treated unit pre-treatment trajectory. This synthetic unit provides the counterfactual for what would have happened without treatment.

The method is designed for case studies with a single treated unit and multiple potential control units. Traditional methods struggle with single-unit studies. Synthetic control leverages all available comparisons optimally.

Key requirements:
- Multiple untreated units available as donors
- Good pre-treatment fit achievable
- Treatment effect large enough to detect
- No interference between units

DONOR POOL CONSTRUCTION

Select donor units that could plausibly serve as counterfactual. Donors should be similar to the treated unit and unaffected by treatment. Exclude units that received similar treatment or have anticipatory effects.

Too few donors limits flexibility. Too many can lead to overfitting. Typical pools have 10 to 50 donors. Exclude clear outliers and irrelevant units.

For geographic studies, donor pool might include:
- Other regions in same country
- Regions with similar demographics
- Regions with similar economic conditions
- Regions with similar historical patterns

WEIGHT OPTIMIZATION

Synthetic control finds weights that minimize pre-treatment discrepancy:
- Weights are non-negative and sum to one
- Objective: minimize difference between treated and synthetic pre-treatment outcomes
- Constraints: weights within valid range

Optimization typically uses constrained quadratic programming. Predictor variables should include:
- Pre-treatment outcome levels
- Pre-treatment outcome trends
- Relevant covariates

Multiple starting points help avoid local optima. Cross-validation can help select predictors and tuning parameters.

FIT QUALITY ASSESSMENT

Good pre-treatment fit is essential for valid inference. Compare treated and synthetic series visually. Calculate root mean squared prediction error for pre-treatment period.

Poor fit indicates:
- Synthetic control cannot approximate treated unit
- Inferential basis is weak
- Results should be interpreted cautiously

Good fit means synthetic control successfully predicts pre-treatment outcomes. This builds confidence that post-treatment divergence reflects treatment effect rather than poor matching.

INFERENCE AND PLACEBO TESTS

Standard errors are not available in closed form. Use placebo tests for inference:
- Apply synthetic control to each donor unit
- Pretend each donor was treated at the same time
- Compare actual treatment effect to placebo effects

If treatment effect is large relative to placebo effects, it is unlikely to be chance. Report ratio of treatment effect to placebo standard deviation. Ratios above 2 suggest statistical significance.

Permutation-based p-values count fraction of placebos with effects as large as actual treatment. P-value of 0.05 means only 5 percent of placebos had larger effects.

POST-TREATMENT ANALYSIS

Calculate treatment effect as difference between actual and synthetic outcomes. Track effect over time to assess persistence and dynamics. Cumulative effects integrate instantaneous effects.

Decompose sources of effect if possible:
- Direct response to marketing
- Word of mouth and spillovers
- Long-term brand building
- Competitive effects

Report effect sizes with uncertainty ranges. Acknowledge limitations of single-unit inference. Triangulate with other evidence when possible.

PRACTICAL IMPLEMENTATION GUIDANCE

SELECTING THE RIGHT METHOD

Choose incrementality method based on context:
- Geo holdout for campaigns that can be withheld from regions
- DID for natural experiments and policy changes
- PSM for observational data with rich covariates
- Synthetic control for single-unit case studies

Method comparison matrix:
- Geo holdout: highest validity, requires regional flexibility
- DID: moderate validity, requires parallel trends
- PSM: lower validity, requires selection on observables
- Synthetic: good for single units, requires donor pool

SAMPLE SIZE PLANNING

Power calculations determine required sample size. Underpowered studies produce noisy results. Overpowered studies waste resources. Target 80 percent power for meaningful effect sizes.

Key parameters for power analysis:
- Baseline conversion rate
- Minimum detectable lift
- Significance level (usually 0.05)
- Variance of outcome variable
- Correlation within clusters

For geo holdouts, power depends on number of regions and conversions per region. Few large regions provide less power than many small regions. Trade off power against implementation complexity.

DURATION GUIDELINES

Minimum experiment duration depends on:
- Customer decision cycle length
- Weekly and monthly business patterns
- Lag between exposure and conversion
- Time for effects to stabilize

General recommendations:
- Fast consumer purchases: 2-4 weeks
- Considered consumer purchases: 4-8 weeks
- B2B purchases: 8-12 weeks
- Brand building campaigns: 12+ weeks

Allow for learning period at start. Initial period effects may not represent steady state. Analyze time patterns to assess stabilization.

CONTAMINATION AND SPILLOVER

Contamination occurs when control group is affected by treatment. Sources include:
- Cross-regional shopping
- Word of mouth
- Competitive response
- Media spillover

Minimize contamination through:
- Buffer zones between regions
- Monitoring for cross-group effects
- Selecting independent units
- Adjusting for spillover in analysis

COMBINING MULTIPLE METHODS

No single method is definitive. Triangulation across methods increases confidence. Consistent results from different approaches strengthen conclusions.

Recommended approach:
- Run randomized geo experiment when possible
- Validate with DID or synthetic control
- Check against observational PSM estimates
- Reconcile differences thoughtfully

Differences across methods often reveal:
- Violation of assumptions in some methods
- Heterogeneous effects across populations
- Time-varying effects
- Implementation differences

INTERPRETING INCREMENTALITY RESULTS

TRANSLATING TO BUSINESS METRICS

Convert lift estimates to actionable metrics:
- Incremental conversions = Lift percentage times baseline conversions
- Incremental revenue = Incremental conversions times average order value
- Incremental cost per acquisition = Spend divided by incremental conversions
- Incremental ROAS = Incremental revenue divided by spend

Compare incremental metrics to standard metrics:
- Incremental CPA often 2-5x higher than attributed CPA
- Incremental ROAS often 50-70 percent of attributed ROAS
- Gap indicates proportion of non-incremental activity

BUILDING INCREMENTALITY INTO PLANNING

Use incremental metrics for budget allocation. Invest in channels with lowest incremental CPA or highest incremental ROAS. Reduce investment in channels capturing mostly organic demand.

Update incrementality estimates regularly:
- Quarterly experiments for major channels
- Annual experiments for smaller channels
- Continuous monitoring for anomalies
- Recalibrate when strategy changes

COMMON PITFALLS

Avoid these incrementality measurement errors:
- Underpowered experiments producing noise
- Contaminated control groups
- Parallel trends violations in DID
- Hidden selection in PSM
- Overfitting in synthetic control
- Misinterpreting confidence intervals

Warning signs of invalid results:
- Implausible effect sizes
- Sensitivity to minor specification changes
- Poor pre-treatment fit or balance
- Systematic differences in diagnostic checks
- Results inconsistent with other evidence

ERROR HANDLING AND VALIDATION

Quality checks for incrementality analysis:
- Verify randomization or matching quality
- Test parallel trends or balance
- Check for contamination
- Assess sensitivity to assumptions
- Validate against alternative methods

When results are uncertain:
- Report confidence intervals honestly
- Acknowledge limitations
- Recommend additional testing
- Use conservative estimates for planning
