PRF KNOWLEDGE BASE - LEARNING EXTRACTION v1
VERSION: 1.0
STATUS: Production Ready
COMPLIANCE: 6-Rule Compliant

================================================================================
SECTION 1 - LEARNING EXTRACTION FRAMEWORK
================================================================================

PURPOSE OF LEARNING EXTRACTION

Learning extraction transforms campaign data into organizational intelligence. The goal is to identify patterns, insights, and best practices that improve future campaign performance.

LEARNING LIFECYCLE

STAGE 1 - CAPTURE
Document observations during and after campaigns.
Raw data, context, and preliminary conclusions.

STAGE 2 - VALIDATE
Test observations against additional data.
Confirm patterns are real, not coincidental.

STAGE 3 - GENERALIZE
Determine if learning applies broadly or only to specific contexts.
Define conditions where learning is relevant.

STAGE 4 - CODIFY
Document learning in standardized format.
Make accessible for future planning.

STAGE 5 - APPLY
Integrate learning into planning processes.
Update benchmarks and recommendations.

STAGE 6 - VERIFY
Confirm learning holds true in new campaigns.
Refine or retire based on ongoing results.

LEARNING CATEGORIES

BENCHMARK LEARNINGS
Updates to expected performance metrics based on actual results.
Example: Actual CPM for retail display was 15 percent higher than benchmarks.

AUDIENCE LEARNINGS
Insights about target segment behavior and response.
Example: High-income segment converted at 2x rate of broad audience.

CHANNEL LEARNINGS
Insights about platform or channel performance.
Example: TikTok outperformed Instagram for Gen Z awareness.

CREATIVE LEARNINGS
Insights about message and creative effectiveness.
Example: Video under 15 seconds had 40 percent higher completion rate.

TIMING LEARNINGS
Insights about when to reach audiences.
Example: B2B conversions peaked Tuesday-Thursday, 10AM-2PM.

TACTICAL LEARNINGS
Insights about campaign setup and optimization.
Example: Lookalike expansion beyond 3 percent degraded performance.

================================================================================
SECTION 2 - POST-MORTEM METHODOLOGY
================================================================================

POST-MORTEM TIMING

IMMEDIATE POST-MORTEM (Within 1 Week)
- Initial performance summary
- Obvious wins and misses
- Preliminary root causes
- Urgent learnings for active campaigns

FULL POST-MORTEM (Within 2-4 Weeks)
- Complete data available
- Attribution windows closed
- Comprehensive analysis
- Validated conclusions

POST-MORTEM STRUCTURE

SECTION 1 - EXECUTIVE SUMMARY
- Campaign objectives recap
- Overall success assessment
- Key metrics vs targets
- Top 3 learnings headline

SECTION 2 - OBJECTIVES ASSESSMENT
For each objective:
- Target metric
- Actual result
- Achievement status (Yes, No, Partial)
- Brief explanation of variance

SECTION 3 - PERFORMANCE ANALYSIS

OVERALL EFFICIENCY
- Total spend vs budget
- Aggregate CPA or ROAS
- Efficiency vs benchmark
- Trend over campaign duration

CHANNEL BREAKDOWN
For each channel:
- Spend allocation actual vs plan
- Primary KPIs achieved
- Efficiency metrics
- Contribution to overall results

AUDIENCE ANALYSIS
- Segment performance comparison
- New vs returning user performance
- Audience size vs reach achieved
- Engagement by segment

CREATIVE ANALYSIS
- Message and creative performance ranking
- Format effectiveness comparison
- Creative fatigue observations
- A/B test results summary

SECTION 4 - WHAT WORKED

Identify top performing elements:
- Channels that exceeded targets
- Audiences that over-indexed
- Creatives with strong engagement
- Tactics that drove efficiency

For each, document:
- Specific results achieved
- Why it worked (hypothesis)
- Conditions for success
- Recommendation for future

SECTION 5 - WHAT DID NOT WORK

Identify underperforming elements:
- Channels that missed targets
- Audiences that underperformed
- Creatives with poor engagement
- Tactics that failed

For each, document:
- Specific shortfall
- Root cause analysis
- What was tried to fix
- Recommendation to avoid or retry

SECTION 6 - ROOT CAUSE ANALYSIS

For significant variances (greater than 20 percent):
- Was the plan realistic?
- Was execution aligned with plan?
- Were there external factors?
- Was measurement accurate?

Use 5 Whys methodology:
1. Why did X happen?
2. Why did that cause occur?
3. Why was that condition present?
4. Why was that not anticipated?
5. Why was that not mitigated?

SECTION 7 - LEARNINGS AND RECOMMENDATIONS

Document actionable insights:
- Specific, not vague
- Supported by data
- Applicable to future campaigns
- Prioritized by impact

SECTION 8 - KNOWLEDGE BASE UPDATES

Propose updates to:
- Benchmark tables
- Best practice documentation
- Planning templates
- Evaluation criteria

================================================================================
SECTION 3 - LEARNING VALIDATION CRITERIA
================================================================================

VALIDATION REQUIREMENTS

Before promoting a learning to knowledge base status, it must meet validation criteria.

CRITERION 1 - STATISTICAL SIGNIFICANCE
- Result is unlikely due to chance
- P-value less than 0.05 for key findings
- Sample size sufficient for conclusion
- Confidence interval is meaningful

CRITERION 2 - REPLICATION
- Learning observed in 2 plus campaigns
- Holds across different time periods
- Consistent across similar contexts
- Not dependent on unique circumstances

CRITERION 3 - MAGNITUDE
- Impact is meaningful to business
- Efficiency improvement greater than 10 percent
- Performance difference is actionable
- Worth the effort to implement

CRITERION 4 - GENERALIZABILITY
- Applicable beyond original campaign
- Conditions for applicability defined
- Limitations clearly stated
- Not overfitted to specific case

VALIDATION PROCESS

STEP 1 - DOCUMENT HYPOTHESIS
State the learning as a testable hypothesis.
Example: Video creative under 15 seconds achieves higher VCR than longer formats.

STEP 2 - GATHER SUPPORTING DATA
Collect data from multiple campaigns.
Document sample sizes and conditions.

STEP 3 - STATISTICAL TEST
Apply appropriate statistical test.
Calculate significance and effect size.

STEP 4 - PEER REVIEW
Have another analyst review methodology.
Challenge assumptions and conclusions.

STEP 5 - PILOT APPLICATION
Test learning in next relevant campaign.
Monitor for confirmation or contradiction.

STEP 6 - PROMOTE OR RETIRE
If confirmed: Promote to knowledge base.
If contradicted: Refine or retire hypothesis.

================================================================================
SECTION 4 - LEARNING DOCUMENTATION FORMAT
================================================================================

STANDARD LEARNING FORMAT

Each learning should be documented consistently.

LEARNING ID
Unique identifier for tracking.
Format: LRN-[Category]-[Year]-[Sequence]
Example: LRN-AUD-2026-042

LEARNING TITLE
Clear, descriptive headline.
Example: High-income lookalikes outperform broad targeting for luxury retail.

LEARNING STATEMENT
One paragraph summary of the insight.
What was observed, under what conditions, with what result.

SUPPORTING DATA
Quantitative evidence:
- Campaigns included
- Sample sizes
- Key metrics
- Statistical significance

CONDITIONS
When does this learning apply?
- Verticals
- Objectives
- Channels
- Audience types
- Budget ranges

LIMITATIONS
When does this learning NOT apply?
- Exceptions noted
- Contexts where opposite may be true
- Additional factors to consider

RECOMMENDATION
Specific action to take based on learning.
How to apply in future planning.

VALIDATION STATUS
- Hypothesis: Initial observation, not validated
- Validated: Confirmed across multiple campaigns
- Retired: Contradicted or no longer applicable

LAST UPDATED
Date of most recent review or update.

================================================================================
SECTION 5 - BENCHMARK UPDATE PROCESS
================================================================================

WHEN TO UPDATE BENCHMARKS

UPDATE TRIGGERS
- Actual performance differs from benchmark by greater than 20 percent consistently
- 3 plus campaigns show consistent pattern
- Industry reports indicate benchmark shift
- Platform changes affect expected performance

BENCHMARK CATEGORIES

EFFICIENCY BENCHMARKS
- CPM by channel and format
- CPC by channel and objective
- CPA by vertical and channel
- ROAS by vertical and objective

ENGAGEMENT BENCHMARKS
- CTR by format and placement
- CVR by vertical and channel
- VCR by format and length
- Engagement rate by platform

DELIVERY BENCHMARKS
- Reach curves by channel
- Frequency optima by objective
- Viewability by placement
- Completion rates by format

BENCHMARK UPDATE METHODOLOGY

STEP 1 - GATHER DATA
Compile actual results from recent campaigns.
Minimum 5 campaigns or 100K spend for category.

STEP 2 - CALCULATE NEW VALUES
Mean and median of recent actuals.
Standard deviation for range setting.
Compare to current benchmark.

STEP 3 - ASSESS CHANGE
Is the difference meaningful?
Greater than 10 percent change warrants update.
Less than 10 percent may be normal variance.

STEP 4 - DOCUMENT RATIONALE
Why is benchmark changing?
Market conditions, platform changes, methodology.
Date of change and prior value.

STEP 5 - IMPLEMENT UPDATE
Update benchmark tables.
Notify planning teams.
Note in knowledge base.

================================================================================
SECTION 6 - LEARNING PROMOTION WORKFLOW
================================================================================

PROMOTION CRITERIA

PROMOTE TO KNOWLEDGE BASE WHEN
- Learning validated across 2 plus campaigns
- Statistical significance confirmed
- Impact greater than 10 percent on relevant metric
- Generalizable beyond specific context
- Clear action recommendation

DO NOT PROMOTE WHEN
- Single campaign observation only
- Statistical significance not established
- Impact is marginal (less than 5 percent)
- Highly specific to unique circumstances
- Action is unclear or impractical

PROMOTION PROCESS

STEP 1 - CANDIDATE IDENTIFICATION
During post-mortem, flag potential learnings.
Note initial supporting evidence.

STEP 2 - VALIDATION QUEUE
Add to validation tracking.
Assign owner for validation.

STEP 3 - CROSS-CAMPAIGN CHECK
As new campaigns complete, check for confirmation.
Document supporting or contradicting evidence.

STEP 4 - VALIDATION COMPLETE
When criteria met, prepare documentation.
Format per standard learning template.

STEP 5 - REVIEW AND APPROVAL
Submit for peer review.
Obtain approval from knowledge owner.

STEP 6 - KNOWLEDGE BASE ENTRY
Add to appropriate knowledge base section.
Tag with relevant categories.
Link to supporting campaigns.

STEP 7 - COMMUNICATION
Notify relevant teams of new learning.
Include in planning team updates.

================================================================================
SECTION 7 - CONTINUOUS LEARNING SYSTEM
================================================================================

LEARNING REPOSITORY STRUCTURE

ACTIVE LEARNINGS
Validated insights currently in use.
Regularly applied in planning.

HYPOTHESIS QUEUE
Observations awaiting validation.
Prioritized by potential impact.

RETIRED LEARNINGS
Previously valid insights no longer applicable.
Documented for historical reference.

LEARNING MAINTENANCE

QUARTERLY REVIEW
Review all active learnings.
Confirm still valid or flag for revalidation.
Update with recent data if needed.

ANNUAL AUDIT
Comprehensive review of learning repository.
Retire outdated learnings.
Consolidate overlapping learnings.
Identify gaps in coverage.

TRIGGERED REVIEW
When market conditions change significantly.
When platform makes major updates.
When new channels or formats emerge.

LEARNING METRICS

Track learning system effectiveness:
- Number of active learnings
- Learnings applied per plan
- Impact of applied learnings
- Learning promotion rate
- Learning retirement rate

================================================================================
SECTION 8 - LEARNING FORMULAS
================================================================================

SIGNIFICANCE TESTING

Z_Score = (Sample_Mean - Population_Mean) / (StdDev / sqrt(N))
P_Value = Probability of Z_Score under null hypothesis
Significant if P_Value less than 0.05

EFFECT SIZE

Cohens_D = (Mean_A - Mean_B) / Pooled_StdDev
Small effect: d = 0.2
Medium effect: d = 0.5
Large effect: d = 0.8

CONFIDENCE INTERVAL

CI_95 = Mean +/- (1.96 x StdDev / sqrt(N))

BENCHMARK CALCULATIONS

Updated_Benchmark = (0.7 x New_Data_Mean) + (0.3 x Previous_Benchmark)
Benchmark_Range_Low = Updated_Benchmark - (1.5 x StdDev)
Benchmark_Range_High = Updated_Benchmark + (1.5 x StdDev)

================================================================================
END OF DOCUMENT
================================================================================
