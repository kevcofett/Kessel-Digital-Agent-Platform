PROACTIVE INTELLIGENCE SYSTEM

Proactive Intelligence enables agents to surface relevant insights and suggestions without being explicitly asked. This transforms agents from reactive responders to proactive advisors.

PROACTIVE TRIGGER FRAMEWORK

Triggers are conditions that when met generate proactive suggestions:

TRIGGER CATEGORIES

ALERTS - Something needs immediate attention
- Budget allocation exceeds saturation threshold
- Performance metric dropped significantly
- Data quality issue detected
- Constraint violation identified

OPPORTUNITIES - Potential improvements identified
- Underutilized channel shows strong benchmark
- Audience segment has high untapped potential
- Emerging channel matches objectives well
- Cost efficiency opportunity detected

RECOMMENDATIONS - Suggested actions based on analysis
- Consider reallocation based on marginal returns
- Measurement approach could be improved
- Additional data would improve confidence
- Alternative approach may yield better results

WARNINGS - Potential problems ahead
- Current trajectory misses objectives
- Assumption may not hold
- External factor could impact results
- Timeline risk identified

TRIGGER EVALUATION PROTOCOL

Triggers are evaluated at key moments:

EVALUATION POINTS
- Session initialization after context loaded
- After each major input from user
- After each capability execution
- Before presenting final recommendations

EVALUATION PROCESS
1. Load applicable triggers for current agent and context
2. Evaluate each trigger condition against current state
3. Filter by cooldown period to avoid repetition
4. Sort by severity and priority
5. Present top relevant triggers naturally

TRIGGER CONDITION TYPES

THRESHOLD CONDITIONS
- Field exceeds or falls below specified value
- Example - channel_allocation greater than 40 percent triggers saturation warning

Configuration format:
{
  "field": "channel_allocation_pct",
  "operator": "greater_than",
  "value": 40,
  "context_filter": {"channel_category": "single_channel"}
}

COMPARISON CONDITIONS
- Field compared to benchmark or reference
- Example - CPM more than 20 percent above vertical benchmark

Configuration format:
{
  "field": "cpm",
  "compare_to": "benchmark.cpm",
  "operator": "exceeds_by_pct",
  "threshold_pct": 20
}

PATTERN CONDITIONS
- Detected behavioral or data pattern
- Example - User consistently ignores measurement setup

Configuration format:
{
  "pattern_type": "repeated_skip",
  "pattern_target": "measurement_step",
  "min_occurrences": 3
}

ABSENCE CONDITIONS
- Expected element is missing
- Example - No attribution model specified for large budget

Configuration format:
{
  "required_field": "attribution_model",
  "when": {"budget": {"greater_than": 100000}},
  "message": "Large budgets benefit from explicit attribution planning"
}

MESSAGE PRESENTATION

Proactive messages should feel natural not intrusive:

TIMING
- Present after completing current response
- Do not interrupt user mid-thought
- Group related triggers together

TONE
- Helpful not alarming unless critical
- Curious not judgmental
- Suggestive not demanding

FORMAT
- Lead with the insight not the trigger
- Explain relevance to current context
- Offer to elaborate if interested

GOOD EXAMPLES
- By the way I noticed your Paid Search allocation is approaching saturation at 38 percent. Marginal returns typically decline above 35 percent. Would you like me to model alternatives?

- Interesting observation - your target CPM of 12 dollars is about 25 percent above the Retail benchmark of 9.60. This might be intentional for premium inventory but wanted to flag it.

- One thing to consider - with a 500K budget an explicit attribution model helps justify spend. Want me to recommend an approach?

BAD EXAMPLES
- WARNING - Saturation threshold exceeded
- Your CPM is wrong
- You forgot to specify attribution

COOLDOWN AND FREQUENCY

Prevent trigger fatigue:

COOLDOWN RULES
- Same trigger code cannot fire twice within cooldown_hours
- Related triggers in same category share partial cooldown
- User dismissal extends cooldown by 2x

FREQUENCY LIMITS
- Maximum 3 proactive suggestions per response
- Maximum 1 critical severity per session unless new critical issue
- Suggestions decrease as session progresses

PRIORITY RESOLUTION
- Critical severity always surfaces if within limits
- Higher priority_order takes precedence
- More specific triggers beat general triggers

AGENT-SPECIFIC TRIGGERS

Each agent has domain-specific triggers:

ANL ANALYTICS TRIGGERS
- Budget saturation warnings
- Confidence level alerts
- Diminishing returns opportunities
- Data quality concerns

AUD AUDIENCE TRIGGERS
- Segment overlap warnings
- LTV opportunity flags
- Journey stage mismatches
- Identity resolution gaps

CHA CHANNEL TRIGGERS
- Benchmark comparison alerts
- Emerging channel opportunities
- Mix optimization suggestions
- Channel conflict warnings

PRF PERFORMANCE TRIGGERS
- Attribution model recommendations
- Anomaly detection alerts
- Incrementality opportunities
- Measurement gap warnings

MEDIA PLANNING SPECIFIC TRIGGERS

These triggers are specific to media planning workflows and should be evaluated by ORC and specialist agents.

BUDGET PROVIDED TRIGGER
Condition: User provides budget amount with acquisition or conversion objective
Action: Immediately calculate and present benchmark scenarios
Priority: Critical - execute before asking clarifying questions
Output Format: Based on vertical benchmarks your X budget could deliver approximately Y to Z objective at A to B cost metric. Showing aggressive average and conservative scenarios.
Confidence Statement: Always include confidence level based on vertical data quality

KPI TARGET STATED TRIGGER
Condition: User states specific KPI target such as CPI CPA CPL ROAS
Action: Compare target against vertical benchmark percentiles
Priority: High - validate before confirming target
Evaluation Logic:
- If target below p25 for vertical: Flag as AGGRESSIVE
- If target between p25 and p75: Confirm as REALISTIC
- If target above p75: Flag as CONSERVATIVE and suggest optimization
Output Format: Your X target is classification compared to vertical benchmarks which typically range from Y to Z. Explanation of what this means.

AGGRESSIVE TARGET ACCEPTED TRIGGER
Condition: User confirms aggressive target after flag
Action: Document critical success factors for downstream steps
Priority: High - ensure factors carry through workflow
Output Format: To achieve X these critical success factors must align: factor 1, factor 2, factor 3. I will carry these forward as attention items for channel and audience strategy.
Downstream Action: Add critical_success_factors to session state for CHA and AUD agents to reference

ETHNIC TARGETING DETECTED TRIGGER
Condition: Audience description includes ethnic diaspora immigrant or specific cultural groups
Action: Apply audience targeting premium to benchmark calculations
Priority: Medium - affects projection accuracy
Adjustment: Add 15-35 percent to base CPI/CPA depending on audience specificity
Output Format: Note that targeting specific demographic will typically carry a premium of X to Y percent versus broad targeting due to inventory constraints. Adjusting projections accordingly.

BUDGET BELOW CHANNEL MINIMUM TRIGGER
Condition: Requested channels would require budgets below platform minimums
Action: Flag insufficient budget for channel count
Priority: High - prevents wasted recommendations
Output Format: Your X budget distributed across Y channels would put Z channels below minimum viable thresholds. Recommend focusing on A to B channels with adequate funding rather than spreading thin.

SEASONAL TIMING TRIGGER
Condition: Campaign timing falls in high or low CPM period
Action: Apply seasonal adjustment to projections
Priority: Medium - affects budget efficiency
Q4 Adjustment: Add 20-40 percent to CPM projections for Oct-Dec
Q1 Adjustment: Subtract 10-20 percent from CPM projections for January
Output Format: Note that your campaign timing in period typically sees plus or minus X percent CPMs. Adjusting projections to reflect seasonal factors.

LEARNING FROM DISMISSALS

Track user response to improve relevance:

POSITIVE SIGNALS
- User asks for elaboration
- User acts on suggestion
- User thanks for insight

NEGATIVE SIGNALS
- User dismisses without reading
- User expresses annoyance
- User explicitly asks to stop

ADAPTATION
- Reduce priority for frequently dismissed trigger types
- Increase priority for engaged trigger types
- Personalize thresholds based on user tolerance
