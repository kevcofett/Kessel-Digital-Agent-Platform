PRF KNOWLEDGE BASE - LEARNING EXTRACTION AND INSIGHT SYNTHESIS

VERSION INFORMATION

Document version 1.0 released January 2026. This knowledge base provides comprehensive methodology for extracting actionable learnings from campaign performance data, synthesizing insights across campaigns, detecting recurring patterns, and generating optimization playbooks.

OVERVIEW

Learning extraction transforms raw performance data into actionable intelligence. Rather than simply reporting metrics, this methodology identifies causal relationships, isolates contributing factors, and codifies findings into reusable knowledge. The goal is organizational learning that compounds over time, enabling each campaign to benefit from all prior campaigns.

Effective learning extraction requires distinguishing between observations and learnings. An observation notes what happened. A learning explains why it happened and predicts when it will happen again. Observations have limited shelf life while learnings create lasting competitive advantage.

LEARNING EXTRACTION METHODOLOGY

WHAT CONSTITUTES A LEARNING

A learning is a finding that meets all of the following criteria:
- Causal relationship identified between action and outcome
- Statistical significance exceeds confidence threshold
- Finding is generalizable beyond the specific campaign context
- Actionable recommendations can be derived from the finding
- Finding can be validated against future campaigns

Learnings must always include:
- Clear statement of the finding
- Evidence supporting the causal claim
- Confidence score based on statistical rigor
- Applicable contexts where learning applies
- Boundary conditions where learning may not apply

LEARNING CATEGORIES

Strategic learnings inform high-level planning decisions:
- Optimal budget thresholds by channel and objective
- Audience-message fit patterns
- Seasonal timing advantages
- Competitive response dynamics
- Creative fatigue thresholds by audience

Tactical learnings inform execution decisions:
- Bidding strategy effectiveness by context
- Daypart and day-of-week performance patterns
- Creative element performance by placement
- Targeting refinement opportunities
- Pacing adjustment triggers

Operational learnings inform process improvements:
- Data quality issues and remediation patterns
- Platform-specific optimization sequences
- Reporting cadence effectiveness
- Stakeholder communication patterns
- Quality assurance checkpoint effectiveness

CONFIDENCE THRESHOLDS

High confidence learnings meet these criteria:
- Statistical significance at p less than 0.05
- Effect size exceeds minimum detectable effect
- Sample size provides 80 percent or greater power
- Finding replicated across multiple time periods
- No confounding variables identified

Medium confidence learnings meet these criteria:
- Statistical significance at p less than 0.10
- Effect size approaches minimum detectable effect
- Sample size provides 60 to 80 percent power
- Finding observed in single time period
- Potential confounding variables noted

Low confidence learnings serve as hypotheses:
- Statistical significance at p less than 0.20
- Directional effect observed
- Sample size insufficient for definitive conclusion
- Requires validation in future campaigns
- Documented for tracking and future testing

LEARNING ATTRIBUTION

Learnings must be attributed to specific variables:
- Independent variable that was changed or varied
- Dependent variable that was measured
- Control variables that were held constant
- Contextual factors that may moderate the effect
- Time period when observation occurred

Multi-factor learnings require careful decomposition:
- Interaction effects between variables
- Confounded effects that cannot be separated
- Sequential effects where order matters
- Threshold effects where relationship is nonlinear
- Saturation effects where returns diminish

TEMPORAL VALIDITY

Learnings have different shelf lives based on stability:
- Platform algorithm learnings expire within 30 to 90 days
- Consumer behavior learnings remain valid 6 to 12 months
- Competitive dynamics learnings valid 3 to 6 months
- Seasonal patterns valid year over year with adjustment
- Strategic learnings may persist multiple years

Mark learnings with expiration guidance:
- Volatile learnings require revalidation each quarter
- Stable learnings require annual revalidation
- Foundational learnings require validation when major market shifts occur

CROSS-CAMPAIGN INSIGHT AGGREGATION

NORMALIZATION REQUIREMENTS

Before aggregating learnings across campaigns, normalize for:
- Budget scale differences using index or percentage
- Time period duration using daily or weekly averages
- Audience size differences using reach-adjusted metrics
- Objective differences using objective-specific KPIs
- Platform differences using platform-neutral definitions

Normalization methods by metric type:
- Conversion metrics normalize by impression or click volume
- Engagement metrics normalize by reach or frequency
- Efficiency metrics normalize by spend level
- Awareness metrics normalize by target audience size
- Brand metrics normalize by baseline measurement

STATISTICAL SIGNIFICANCE FOR PATTERNS

When claiming a pattern exists across campaigns:
- Minimum of 3 supporting campaigns required
- Chi-square test for categorical patterns
- ANOVA for continuous variable patterns
- Effect size must be consistent in direction
- Heterogeneity assessment using I-squared statistic

When pattern confidence is reported:
- Report number of supporting vs contradicting campaigns
- Report weighted average effect size
- Report confidence interval for aggregate effect
- Flag outlier campaigns that deviate significantly
- Note sample size contribution from each campaign

WEIGHTING BY RECENCY AND SAMPLE SIZE

Recent campaigns receive higher weight:
- Campaigns within 90 days weighted at 1.0
- Campaigns 90 to 180 days weighted at 0.7
- Campaigns 180 to 365 days weighted at 0.4
- Campaigns over 365 days weighted at 0.2
- Adjust weights for major market or platform changes

Sample size weighting follows inverse variance:
- Weight proportional to 1 divided by variance
- Larger samples contribute more to aggregate
- Small samples contribute proportionally less
- Prevents single large campaign from dominating
- Ensures stable estimates from combined data

CONFLICTING INSIGHT RESOLUTION

When learnings conflict across campaigns:
- First verify data quality in each campaign
- Check for contextual differences explaining conflict
- Test for moderating variables
- Consider temporal changes between campaigns
- Document conflict with both findings preserved

Resolution approaches by conflict type:
- Apparent conflict due to context difference - document boundary conditions
- True conflict due to market change - update learning with date boundary
- Conflict due to measurement error - flag data quality and exclude
- Conflict due to statistical noise - combine with uncertainty noted

META-ANALYSIS APPROACH

Formal meta-analysis for high-value learnings:
- Fixed effects model when campaigns are homogeneous
- Random effects model when campaigns differ systematically
- Forest plot visualization of individual and combined effects
- Funnel plot assessment for publication bias
- Sensitivity analysis excluding outlier campaigns

PATTERN DETECTION ALGORITHMS

SUCCESS PATTERN IDENTIFICATION

Success patterns are configurations that reliably produce positive outcomes. Identification requires:
- Define success criteria for the pattern domain
- Identify candidate configuration variables
- Measure success rate for each configuration
- Test statistical significance of success rate differences
- Validate pattern on holdout campaigns

Success pattern criteria:
- Success rate exceeds baseline by statistically significant margin
- Pattern observed in minimum 5 campaign instances
- Pattern persists across different time periods
- Pattern holds across different audience segments
- Clear mechanism explains why pattern produces success

Configuration variables for pattern detection:
- Budget level relative to category and competitor
- Audience targeting breadth vs depth
- Creative message type and format
- Channel mix composition
- Flighting pattern and pacing

FAILURE PATTERN WARNING THRESHOLDS

Failure patterns are configurations that reliably produce negative outcomes. Detection requires:
- Define failure criteria and threshold severity
- Track configuration variables in underperforming campaigns
- Calculate failure rate by configuration
- Set warning thresholds based on risk tolerance
- Create early warning indicators

Failure pattern thresholds:
- Critical failure patterns trigger immediate review
- Warning failure patterns flag for monitoring
- Caution failure patterns note for consideration
- Failure rate above 50 percent for configuration is critical
- Failure rate 30 to 50 percent is warning level

Early warning indicators:
- Leading indicators that precede failure
- Time lag between indicator and failure
- Indicator reliability in predicting failure
- Recommended monitoring frequency
- Escalation protocol when indicator triggers

RECURRING VS ONE-TIME PATTERN CLASSIFICATION

Patterns classified by persistence:
- Recurring patterns appear in 70 percent or more of applicable campaigns
- Frequent patterns appear in 40 to 70 percent of applicable campaigns
- Occasional patterns appear in 20 to 40 percent of applicable campaigns
- Rare patterns appear in less than 20 percent of applicable campaigns

One-time occurrences are not patterns:
- Single campaign observation is hypothesis only
- Document for tracking but do not codify as pattern
- Require minimum 3 occurrences to consider pattern
- Require statistical test to confirm pattern vs chance

Pattern persistence scoring:
- Calculate occurrence rate across applicable campaigns
- Weight by recency using decay function
- Adjust for campaign similarity and independence
- Score ranges from 0 to 100 for persistence strength
- Patterns scoring below 40 require validation

PATTERN PERSISTENCE SCORING

Persistence score calculation:
- Count occurrences in most recent 12 months
- Weight each occurrence by recency
- Divide by number of applicable campaign opportunities
- Multiply by 100 for percentage score
- Adjust for seasonal or cyclical applicability

Interpreting persistence scores:
- Score 80 to 100 indicates highly reliable pattern
- Score 60 to 80 indicates reliable pattern with exceptions
- Score 40 to 60 indicates moderate pattern requiring context
- Score 20 to 40 indicates weak pattern requiring validation
- Score below 20 indicates unreliable hypothesis

PLAYBOOK GENERATION FRAMEWORK

PLAYBOOK STRUCTURE

Standard playbook sections:
- Playbook title and version
- Applicable scope and context
- Condition-action rules
- Decision logic and sequencing
- Exception handling
- Review and update cadence

Each playbook rule contains:
- Rule identifier for reference
- Condition statement in clear language
- Action statement with specific steps
- Confidence score based on supporting evidence
- Exceptions where rule may not apply
- Source learnings that informed the rule

CONDITION-ACTION RULE EXTRACTION

Rules extracted from learnings follow pattern:
- WHEN condition is observed
- AND context factors apply
- THEN take specified action
- EXPECT specified outcome
- UNLESS exception conditions present

Condition specification requirements:
- Observable and measurable trigger
- Threshold or range clearly defined
- Data source identified for monitoring
- Timing of observation specified
- False positive rate estimated

Action specification requirements:
- Specific steps to execute
- Sequence if multiple steps required
- Timing and urgency of action
- Owner responsible for action
- Verification that action was taken

CONFIDENCE SCORING FOR RECOMMENDATIONS

Rule confidence based on evidence strength:
- High confidence of 80 to 100 from randomized tests
- Medium-high confidence of 60 to 80 from quasi-experiments
- Medium confidence of 40 to 60 from observational patterns
- Low confidence of 20 to 40 from limited observations
- Hypothesis of below 20 requiring testing

Confidence adjustment factors:
- Increase confidence if replicated across contexts
- Decrease confidence if conflicting evidence exists
- Increase confidence if mechanism is well understood
- Decrease confidence if based on small samples
- Adjust for time elapsed since evidence gathered

VERSION CONTROL FOR PLAYBOOK UPDATES

Playbook versioning protocol:
- Major version increment for structural changes
- Minor version increment for rule additions or modifications
- Patch version increment for clarifications only
- Change log documents all modifications
- Previous versions archived for reference

Update triggers:
- New learning invalidates existing rule
- Pattern no longer observed at required frequency
- Context has changed making rule obsolete
- Feedback indicates rule is not effective
- Scheduled quarterly review identifies updates needed

INTEGRATION WITH PERFORMANCE DATA

REQUIRED DATA INPUTS

Campaign-level data requirements:
- Campaign identifier and metadata
- Objective classification and KPIs
- Budget and spend by channel and time period
- Audience targeting parameters
- Creative assets and messaging
- Flight dates and pacing

Performance metrics required:
- Primary KPI actuals vs targets
- Secondary KPIs for context
- Efficiency metrics including CPM CPC CPA
- Engagement metrics including CTR VCR
- Conversion metrics at each funnel stage

Contextual data for analysis:
- Competitive activity during campaign
- Market conditions and external factors
- Platform changes during campaign
- Audience composition changes
- Creative rotation and testing data

INPUT SCHEMA SPECIFICATION

Campaign data JSON structure:
- campaign_id as string identifier
- campaign_name as descriptive string
- objective as enumerated type
- start_date and end_date as ISO dates
- total_budget as number
- channels as array of channel objects
- audiences as array of audience objects
- creatives as array of creative objects

Performance data JSON structure:
- campaign_id as string identifier
- date as ISO date for daily granularity
- channel as string identifier
- impressions as integer count
- clicks as integer count
- conversions as integer count
- spend as decimal currency
- custom_metrics as object with KPI values

MINIMUM DATA REQUIREMENTS

Minimum for learning extraction:
- 14 days of performance data
- At least 1000 impressions total
- At least 10 conversion events
- Complete spend data by day
- Accurate tracking implementation

Minimum for pattern detection:
- 5 or more comparable campaigns
- Consistent measurement across campaigns
- Common variables tracked
- Sufficient variation in configurations
- Adequate sample in each configuration

DATA QUALITY VALIDATION

Pre-extraction data quality checks:
- Completeness check for missing dates or channels
- Consistency check for metric relationships
- Outlier detection for anomalous values
- Tracking validation for conversion accuracy
- Attribution check for proper credit assignment

Quality score calculation:
- Deduct points for each quality issue found
- Weight deductions by issue severity
- Report overall quality score 0 to 100
- Flag campaigns below quality threshold
- Document known quality limitations

HISTORICAL LOOKBACK WINDOWS

Standard lookback periods:
- Short-term learning extraction uses 30 day window
- Medium-term pattern detection uses 90 day window
- Long-term strategic analysis uses 365 day window
- Trend analysis uses 24 month window minimum

Lookback selection guidance:
- Match lookback to learning type
- Shorter windows for tactical learnings
- Longer windows for strategic learnings
- Adjust for seasonality in applicable domains
- Consider market stability when selecting window

OUTPUT SPECIFICATIONS

LEARNING EXTRACTION OUTPUT

Learning object structure:
- learning_id as unique identifier
- category as strategic or tactical or operational
- finding as clear statement of learning
- evidence as supporting data summary
- confidence as score 0 to 100
- actionability as immediate or near-term or long-term
- applicable_contexts as array of contexts
- boundary_conditions as array of limitations
- expiration_date as ISO date
- source_campaigns as array of campaign identifiers

INSIGHT AGGREGATION OUTPUT

Cross-campaign insight structure:
- insight_id as unique identifier
- insight as synthesized finding statement
- supporting_campaigns as array of campaign identifiers
- confidence as weighted score 0 to 100
- conflicts as array of conflicting findings if any
- strategic_implication as business impact statement
- meta_patterns as array of higher-order patterns

PATTERN DETECTION OUTPUT

Pattern object structure:
- pattern_id as unique identifier
- pattern_type as success or failure
- pattern_description as clear statement
- occurrence_count as integer
- persistence_score as 0 to 100
- conditions as array of trigger conditions
- expected_outcome as predicted result
- warning_indicators as array for failure patterns
- mitigation_actions as array for failure patterns

PLAYBOOK OUTPUT

Playbook object structure:
- playbook_name as descriptive title
- playbook_version as semantic version
- applicable_scope as context description
- rules as array of rule objects
- decision_tree as logic flow description
- review_cadence as frequency string
- last_updated as ISO date
- source_learnings as array of learning identifiers

BEST PRACTICES

LEARNING EXTRACTION BEST PRACTICES

Always validate causation before claiming a learning:
- Correlation does not imply causation
- Look for confounding variables
- Consider reverse causality
- Test mechanism plausibility
- Seek experimental confirmation when possible

Document uncertainty explicitly:
- State confidence level clearly
- Note sample size limitations
- Flag potential confounds
- Indicate validation status
- Set expiration for volatile learnings

Prioritize actionable learnings:
- Focus on variables within control
- Emphasize learnings with clear actions
- Deprioritize interesting but unactionable findings
- Connect learnings to specific decisions
- Validate that actions are feasible

INSIGHT SYNTHESIS BEST PRACTICES

Aggregate with appropriate skepticism:
- More data does not always mean better insights
- Quality of underlying learnings matters
- Watch for Simpson paradox in aggregation
- Segment analysis may reveal hidden patterns
- Heterogeneity suggests need for conditional insights

Communicate uncertainty appropriately:
- Avoid false precision in aggregates
- Report ranges not point estimates
- Distinguish strong from weak patterns
- Flag where more data needed
- Update stakeholders when insights change

PATTERN DETECTION BEST PRACTICES

Avoid overfitting to historical data:
- Patterns must be generalizable
- Validate on holdout campaigns
- Simple patterns preferred to complex
- Watch for spurious correlations
- Require mechanism for pattern validity

Balance sensitivity and specificity:
- Too sensitive creates false alarms
- Too specific misses real patterns
- Tune thresholds based on cost of errors
- Monitor false positive and negative rates
- Adjust thresholds based on feedback

PLAYBOOK BEST PRACTICES

Keep rules simple and executable:
- Clear conditions that can be monitored
- Specific actions that can be taken
- Avoid ambiguous language
- Test rules for interpretability
- Train users on rule application

Maintain playbooks actively:
- Regular review cadence required
- Remove outdated rules promptly
- Add new rules as learnings emerge
- Track rule effectiveness
- Gather user feedback on usability

ERROR HANDLING

When data is insufficient:
- Return partial results with quality warnings
- Indicate which analyses could not be performed
- Recommend data collection improvements
- Provide minimum requirements for future analysis

When learnings conflict irreconcilably:
- Document both findings with full context
- Do not force false consensus
- Recommend additional testing to resolve
- Flag conflict for human review

When patterns are unstable:
- Reduce confidence scores appropriately
- Mark patterns as requiring validation
- Increase monitoring frequency
- Set shorter expiration dates
