AUD_KB_Identity_Graph_Algorithms_v1.0.txt
VERSION: 1.0
STATUS: Production Ready
COMPLIANCE: 6-Rule Compliant
LAST UPDATED: 2026-01-18
CHARACTER COUNT: 30290

================================================================================
SECTION 1 - IDENTITY GRAPH FUNDAMENTALS
================================================================================

OVERVIEW

Identity graph algorithms form the computational foundation for resolving fragmented consumer identities across devices, channels, and touchpoints into unified customer profiles. These algorithms enable marketers to understand the complete customer journey rather than viewing disconnected interactions as separate individuals. The strategic importance of identity resolution continues to grow as privacy regulations limit third-party tracking and consumers interact across an increasing number of digital and physical touchpoints.

Modern identity graphs represent consumers and their identifiers as nodes in a connected network, with edges representing relationships between identifiers that belong to the same individual. The fundamental challenge lies in determining which connections represent true identity matches versus coincidental associations. This document provides comprehensive technical guidance on the algorithmic approaches used to build, maintain, and query identity graphs at enterprise scale.

IDENTITY GRAPH ARCHITECTURE

The architecture of an identity graph consists of several interconnected components that work together to resolve identities accurately and efficiently.

Node Types in Identity Graphs
- Person nodes represent resolved individual identities as the ultimate output of resolution
- Identifier nodes represent specific identity signals such as email addresses or device identifiers
- Household nodes represent groupings of individuals sharing a physical address
- Account nodes represent business relationships between individuals and organizations
- Device nodes represent specific hardware devices associated with individuals

Edge Types and Relationships
- Deterministic edges connect identifiers with proven relationships through authentication events
- Probabilistic edges connect identifiers with statistical likelihood of belonging to same individual
- Temporal edges capture time-bounded relationships that may change over time
- Hierarchical edges connect individuals to households or accounts
- Cross-device edges link different device types to the same individual

Graph Data Models
- Property graphs store attributes on both nodes and edges for rich metadata
- Labeled property graphs add semantic labels to enable typed queries
- Hypergraphs allow edges to connect more than two nodes for complex relationships
- Temporal graphs maintain historical states for point-in-time resolution

IDENTITY SIGNAL TAXONOMY

Understanding the types and quality of identity signals is essential for effective resolution algorithms.

First-Party Deterministic Signals
- Authenticated email addresses from login events provide highest confidence
- Phone numbers collected through verified account creation
- Customer account identifiers from loyalty programs
- Physical addresses from shipping and billing records
- Government identifiers where legally permitted and consented

Device and Technical Signals
- Mobile advertising identifiers including IDFA and GAID
- Browser cookies both first-party and third-party where available
- IP addresses with varying persistence and precision
- Device fingerprints combining browser and hardware attributes
- Connected TV identifiers from streaming platforms

Behavioral Signals
- Login patterns across devices and locations
- Purchase behaviors indicating shared household
- Content consumption patterns suggesting individual preferences
- Location data from mobile devices
- App usage patterns across device portfolio


================================================================================
SECTION 2 - DETERMINISTIC MATCHING ALGORITHMS
================================================================================

OVERVIEW

Deterministic matching algorithms establish identity links based on exact or near-exact matches of persistent identifiers. These algorithms provide the highest confidence connections in an identity graph and form the backbone upon which probabilistic methods build. The key challenge in deterministic matching lies in handling identifier variations while maintaining precision.

EXACT MATCH ALGORITHMS

Direct Identifier Matching
- Email address matching requires normalization for case and whitespace
- Phone number matching requires standardization to E.164 format
- Postal address matching requires USPS or similar authority standardization
- Account number matching provides highest confidence when available
- Government identifier matching where legally permitted

Email Normalization Process
- Convert all characters to lowercase for case-insensitive comparison
- Remove periods from Gmail addresses per Gmail equivalence rules
- Remove plus-sign suffixes used for email aliasing
- Trim leading and trailing whitespace
- Validate format compliance before comparison

Phone Normalization Process
- Strip all formatting characters including parentheses dashes and spaces
- Prepend country code if missing using default from context
- Validate against E.164 specification for length and format
- Handle extension information separately from main number
- Consider carrier portability when using as long-term identifier

FUZZY MATCH ALGORITHMS

String Similarity Metrics
- Levenshtein distance measures minimum single-character edits required
- Jaro-Winkler similarity weights prefix matches more heavily for names
- Soundex and Metaphone encode phonetic similarity for name matching
- N-gram similarity compares character subsequence overlap
- Cosine similarity on character vectors for longer strings

Name Matching Techniques
- First name and last name should be matched separately then combined
- Nickname dictionaries map common variations to canonical forms
- Cultural naming conventions require locale-specific handling
- Hyphenated names require special tokenization logic
- Suffix and prefix handling for generational and professional titles

Address Matching Techniques
- Component parsing separates street number name unit and postal code
- Abbreviation expansion standardizes St to Street and similar
- Apartment and unit normalization handles varied formats
- ZIP plus four provides additional precision for matching
- USPS Address Matching System provides authoritative standardization

MATCH CONFIDENCE SCORING

Confidence Score Components
- Identifier type reliability weight based on persistence and uniqueness
- Match method precision weight based on algorithmic accuracy
- Recency weight based on time since identifier was observed
- Frequency weight based on number of confirming observations
- Source quality weight based on data provider reliability

Score Calibration Methods
- Holdout validation using labeled match pairs
- Precision-recall curve analysis at various thresholds
- Human review sampling for ground truth generation
- Cross-validation across different time periods
- A/B testing of threshold changes on downstream metrics


================================================================================
SECTION 3 - PROBABILISTIC MATCHING ALGORITHMS
================================================================================

OVERVIEW

Probabilistic matching algorithms compute the statistical likelihood that two identity records belong to the same individual when deterministic identifiers are unavailable or insufficient. These methods are essential for extending identity graphs beyond authenticated users and for cross-device resolution where no shared login exists.

FELLEGI-SUNTER FRAMEWORK

The Fellegi-Sunter model provides the theoretical foundation for probabilistic record linkage and remains influential in modern implementations.

Framework Components
- Agreement weight represents log likelihood ratio for matching field values
- Disagreement weight represents log likelihood ratio for non-matching values
- M-probability estimates likelihood of agreement given true match
- U-probability estimates likelihood of agreement given non-match
- Composite weight sums individual field weights for overall score

Weight Calculation
- M-probability estimation requires labeled training data or strong assumptions
- U-probability can be estimated from field value frequency distributions
- Frequency-based weights assign higher value to rare matching values
- Missing value handling requires explicit probability assignments
- Field interaction weights capture dependent fields

Decision Rules
- Upper threshold above which pairs are automatically matched
- Lower threshold below which pairs are automatically non-matched
- Gray zone between thresholds requires manual review or additional data
- Threshold optimization balances precision and recall requirements
- Adaptive thresholds may vary by identifier type or data source

BAYESIAN MATCHING METHODS

Bayesian approaches provide a principled framework for combining prior knowledge with observed evidence.

Prior Probability Estimation
- Base match rate in population provides starting point
- Prior can incorporate domain knowledge about match likelihood
- Hierarchical priors share strength across similar comparison types
- Informative priors from historical labeled data
- Non-informative priors when domain knowledge is limited

Likelihood Models
- Field-specific likelihood functions for different data types
- Mixture models capture heterogeneous data quality
- Copula models handle dependencies between fields
- Hierarchical likelihood structures for nested comparisons
- Missing data patterns inform likelihood calculations

Posterior Inference
- Exact calculation feasible for small candidate sets
- MCMC methods for complex hierarchical models
- Variational inference for faster approximate solutions
- Sequential updating as new evidence arrives
- Posterior predictive checks for model validation

MACHINE LEARNING MATCHING

Modern machine learning approaches can capture complex patterns that rule-based systems miss.

Supervised Learning Approaches
- Random forests handle mixed feature types and capture interactions
- Gradient boosting provides high accuracy with proper tuning
- Support vector machines effective for high-dimensional comparisons
- Neural networks can learn feature representations automatically
- Ensemble methods combine multiple algorithms for robustness

Feature Engineering for Matching
- String similarity scores at multiple granularities
- Phonetic encodings for name comparison
- Geographic distance for address comparison
- Temporal proximity for event-based signals
- Behavioral similarity scores from activity patterns

Active Learning for Label Efficiency
- Uncertainty sampling selects ambiguous pairs for labeling
- Query-by-committee identifies pairs where models disagree
- Expected model change selects most informative examples
- Diversity sampling ensures coverage of pair types
- Human-in-the-loop workflows for continuous improvement


================================================================================
SECTION 4 - BLOCKING STRATEGIES
================================================================================

OVERVIEW

Blocking strategies reduce the computational complexity of identity resolution by limiting pairwise comparisons to records that share certain characteristics. Without blocking, comparing all pairs in a dataset of N records requires N-squared-over-two comparisons, which becomes infeasible at scale. Effective blocking maintains high recall while dramatically reducing comparison count.

STANDARD BLOCKING METHODS

Key-Based Blocking
- Single-field blocking groups records sharing exact field value
- Concatenated key blocking combines multiple fields for finer blocks
- Phonetic blocking uses Soundex or Metaphone codes for names
- Geographic blocking groups by postal code or region
- Temporal blocking groups by date range for time-sensitive identifiers

Sorted Neighborhood Blocking
- Records sorted by blocking key are compared within sliding window
- Window size controls tradeoff between recall and comparison count
- Multi-pass approach uses different sort keys in each pass
- Adaptive window size varies based on key density
- Efficient for datasets with natural ordering

ADVANCED BLOCKING TECHNIQUES

Canopy Clustering
- Loose clusters formed using computationally cheap distance metric
- Tight threshold defines cluster membership
- Loose threshold allows overlap between clusters
- Records compared only within same canopy
- Reduces comparisons while maintaining high pair completeness

Locality-Sensitive Hashing
- Hash functions designed so similar records likely share hash values
- MinHash effective for set similarity in text-based identifiers
- SimHash effective for cosine similarity in feature vectors
- Multiple hash tables increase recall at cost of more comparisons
- Band and row configuration controls precision-recall tradeoff

Suffix Array Blocking
- Records indexed by all suffixes of blocking key
- Enables partial match blocking without knowing match position
- Memory-efficient implementations using suffix arrays
- Effective for variable-length identifiers with common suffixes
- Supports wildcard and substring queries

BLOCKING QUALITY METRICS

Pair Completeness
- Measures fraction of true matches captured by blocking scheme
- Critical metric as missed pairs cannot be recovered in matching phase
- Target ninety-five percent or higher for production systems
- Estimated using labeled sample of true matches
- May vary significantly across different match types

Reduction Ratio
- Measures fraction of comparisons eliminated by blocking
- Higher ratio means more efficient resolution process
- Target ninety-nine percent or higher for large-scale systems
- Tradeoff with pair completeness requires careful optimization
- Reported separately for different blocking passes

HYBRID BLOCKING ARCHITECTURES

Multi-Pass Blocking
- Different blocking keys used in sequential passes
- Union of candidate pairs from all passes compared
- Covers different failure modes of individual blocking schemes
- Diminishing returns after three to four passes typically
- Pass ordering can prioritize precision or recall

Hierarchical Blocking
- Coarse blocking creates initial candidate groups
- Fine blocking refines within coarse groups
- Reduces memory requirements for large datasets
- Enables parallel processing of independent groups
- Natural fit for geographic or temporal hierarchies


================================================================================
SECTION 5 - MACHINE LEARNING MATCHING
================================================================================

OVERVIEW

Machine learning approaches to identity matching have become increasingly sophisticated, moving beyond simple supervised classification to deep learning architectures that can learn feature representations directly from raw identifier data. These methods excel at capturing complex patterns and generalizing across diverse identifier types.

DEEP LEARNING FOR MATCHING

Siamese Networks
- Twin neural networks share weights and learn distance metric
- Contrastive loss trains network to minimize distance for matches
- Triplet loss uses anchor-positive-negative triplets for training
- Effective for learning representations of variable-length strings
- Pre-trained embeddings can transfer across domains

Transformer-Based Matching
- BERT and similar models provide contextual embeddings for text
- Fine-tuning on matching task adapts pre-trained representations
- Cross-encoder architecture compares records jointly
- Bi-encoder architecture enables efficient indexing and retrieval
- Attention mechanisms highlight relevant features for matching

Graph Neural Networks
- Learn node representations from graph structure
- Message passing aggregates information from neighbors
- Node classification predicts match probability for candidate pairs
- Link prediction directly models edge existence probability
- Effective for leveraging transitive relationships

EMBEDDING-BASED MATCHING

Embedding Generation
- Character-level embeddings capture subword patterns
- Word-level embeddings for tokenized identifiers
- Concatenation of field-specific embeddings
- Dimensionality reduction for efficient comparison
- Normalization for consistent similarity scoring

Approximate Nearest Neighbor Search
- FAISS library enables billion-scale similarity search
- HNSW index provides good recall-speed tradeoff
- IVF index partitions space for faster search
- Product quantization reduces memory requirements
- Hybrid approaches combine multiple index types

CROSS-DEVICE IDENTITY MODELS

Device Graph Construction
- Observed co-occurrence forms initial edge candidates
- Temporal patterns strengthen or weaken connections
- Location overlap provides supporting evidence
- Login events provide deterministic anchors
- Third-party graph integration supplements first-party data

Cross-Device Probability Models
- Co-login probability from authentication events
- Co-location probability from IP and GPS data
- Co-visitation probability from browsing patterns
- Temporal co-occurrence probability from timestamps
- Combined probability using independence or copula models

Identity Persistence Modeling
- Device identifier reset patterns affect linkage persistence
- Browser cookie deletion rates by browser type
- Mobile advertising ID opt-out rates
- IP address change frequency by connection type
- Model decay factors for probability degradation over time

TRAINING DATA GENERATION

Positive Example Generation
- Login events across devices provide ground truth matches
- CRM data with multiple contact methods for same customer
- Transaction linkages from shared payment methods
- Household file overlays for address-based matching
- Panel data with declared device ownership

Negative Example Generation
- Random sampling provides easy negatives
- Hard negative mining selects similar non-matches
- Same-household non-matches capture common confusions
- Temporal negatives from identifier reassignment
- Stratified sampling ensures coverage of edge cases

Data Augmentation
- Synthetic typos simulate data entry errors
- Address variation generation from templates
- Name permutation for different name orders
- Missing field simulation for partial records
- Noise injection for robustness training


================================================================================
SECTION 6 - GRAPH ALGORITHMS FOR RESOLUTION
================================================================================

OVERVIEW

Once pairwise match decisions are made, graph algorithms determine how to cluster identifiers into resolved person entities. This clustering step is critical because pairwise decisions may be inconsistent and the transitive closure of matches may create overly large clusters. Graph algorithms provide principled approaches to balance precision and recall in final entity resolution.

CONNECTED COMPONENTS

Basic Connected Components
- Transitive closure treats any path as confirming same entity
- Union-Find algorithm provides efficient implementation
- Simple but tends toward over-clustering with noisy edges
- Appropriate when match precision is very high
- Baseline for comparison with more sophisticated methods

Weighted Connected Components
- Edge weights represent match confidence scores
- Minimum spanning tree identifies strongest connections
- Weak edges can be pruned before clustering
- Weight threshold controls cluster granularity
- Multiple thresholds enable hierarchical clustering

GRAPH CLUSTERING ALGORITHMS

Correlation Clustering
- Objective function minimizes disagreements with pairwise decisions
- NP-hard but good approximation algorithms exist
- Naturally handles both positive and negative edges
- Does not require specifying number of clusters
- Well-suited for entity resolution with both match and non-match signals

Markov Clustering
- Simulates random walks to find natural clusters
- Expansion and inflation operations alternate
- Inflation parameter controls cluster granularity
- Robust to noise in edge weights
- Scales to large graphs with sparse matrix operations

Louvain Community Detection
- Hierarchical algorithm optimizes modularity
- Fast convergence enables large-scale application
- Resolution parameter controls community size
- Deterministic variant available for reproducibility
- Widely used in production identity systems

Spectral Clustering
- Uses eigenvectors of graph Laplacian for clustering
- Captures global graph structure
- Normalized cuts variant handles varying cluster sizes
- Computational cost limits applicability to smaller graphs
- Useful for high-value entity resolution with quality emphasis

GRAPH-BASED CONFLICT RESOLUTION

Negative Edge Handling
- Explicit non-match edges prevent incorrect clustering
- Must-not-link constraints in constrained clustering
- Chromatic number bounds minimum clusters for conflicting nodes
- Soft constraints allow probabilistic conflict resolution
- Human review queue for unresolvable conflicts

Cluster Splitting Heuristics
- Over-clustered entities detected by size anomalies
- Betweenness centrality identifies natural split points
- Weakest link removal iteratively breaks large clusters
- Feature-based splitting uses attribute similarity
- Temporal analysis identifies identity theft or shared accounts

Cluster Merging Strategies
- Under-clustered entities detected by strong cross-cluster edges
- Merge candidates scored by inter-cluster edge strength
- Conservative merging requires multiple supporting edges
- Confidence propagation through merged clusters
- Audit trail for merge decisions supports rollback

INCREMENTAL GRAPH UPDATES

Online Clustering Algorithms
- New identifiers assigned to existing clusters or create new
- Edge additions may trigger cluster merges
- Edge removals may trigger cluster splits
- Amortized update cost enables real-time resolution
- Consistency maintenance across distributed systems

Change Detection and Propagation
- New observations trigger local graph updates
- Changed match scores propagate to affected clusters
- Cascade limits prevent runaway updates
- Batch processing for efficiency with delayed consistency
- Event-driven architecture for low-latency updates


================================================================================
SECTION 7 - SCALING CONSIDERATIONS
================================================================================

OVERVIEW

Enterprise identity graphs may contain billions of nodes and hundreds of billions of edges, requiring careful attention to computational and storage efficiency. Scaling identity resolution requires distributed algorithms, efficient data structures, and architectural patterns that maintain accuracy while meeting latency and throughput requirements.

DISTRIBUTED RESOLUTION ARCHITECTURES

Partition Strategies
- Hash-based partitioning distributes identifiers across nodes
- Range-based partitioning groups related identifiers together
- Graph-aware partitioning minimizes cross-partition edges
- Hybrid approaches combine multiple partition schemes
- Rebalancing procedures handle data growth and skew

Cross-Partition Resolution
- Blocking keys may span multiple partitions
- Distributed blocking requires coordination layer
- Cross-partition matches require merge protocols
- Eventual consistency models for high availability
- Strong consistency for financial or compliance applications

Distributed Graph Databases
- Neo4j Cluster provides ACID transactions with sharding
- Amazon Neptune offers managed graph database service
- JanusGraph enables horizontal scaling on commodity hardware
- TigerGraph optimizes for deep link analytics
- Selection criteria include query patterns and scale requirements

PERFORMANCE OPTIMIZATION

Caching Strategies
- Frequently queried entities cached in memory
- Cache invalidation on entity updates
- Distributed cache for consistent cross-node access
- Tiered caching with hot and warm layers
- Cache hit rate monitoring for optimization

Index Optimization
- B-tree indexes for exact match queries
- Full-text indexes for fuzzy string search
- Spatial indexes for geographic queries
- Composite indexes for multi-field lookups
- Index maintenance overhead versus query performance

Query Optimization
- Query planning for efficient traversal patterns
- Predicate pushdown to reduce intermediate results
- Parallel query execution across partitions
- Query result pagination for large result sets
- Query timeout and resource limits

BATCH VERSUS REAL-TIME RESOLUTION

Batch Resolution Architecture
- Full graph rebuild on regular schedule
- Efficient bulk operations for large datasets
- Simpler consistency model with snapshot isolation
- Resource scheduling during low-traffic periods
- Suitable for analytical use cases with delayed freshness

Real-Time Resolution Architecture
- Incremental updates as new data arrives
- Sub-second latency for identity lookup
- Stream processing for continuous resolution
- Event sourcing for audit and replay capability
- Suitable for operational use cases requiring current state

Lambda Architecture
- Batch layer provides comprehensive but delayed view
- Speed layer provides approximate but current view
- Serving layer merges batch and speed results
- Complexity of maintaining dual processing paths
- Kappa architecture alternative uses streaming only

DATA QUALITY AT SCALE

Quality Monitoring
- Match rate trends indicate data or algorithm drift
- Cluster size distribution detects over or under clustering
- Cross-validation on holdout data measures accuracy
- Human review sampling provides ground truth
- Alerting on quality metric degradation

Error Handling
- Graceful degradation when components unavailable
- Retry logic with exponential backoff for transient failures
- Dead letter queues for problematic records
- Circuit breakers prevent cascade failures
- Detailed error logging for debugging

Recovery Procedures
- Point-in-time recovery from backups
- Incremental rebuild from source data
- Rollback capability for algorithm changes
- Data reconciliation after recovery
- Disaster recovery across regions


================================================================================
SECTION 8 - AGENT APPLICATION GUIDANCE
================================================================================

WHEN TO USE THIS KNOWLEDGE

This knowledge base document should be referenced when the AUD Agent encounters the following scenarios and queries.

Identity Resolution Queries
- Questions about matching algorithms for customer data
- Requests to evaluate identity resolution vendor capabilities
- Design discussions for customer data platform architecture
- Privacy-compliant approaches to cross-device tracking
- Accuracy improvement for existing identity systems

Technical Implementation Guidance
- Algorithm selection for specific identifier types
- Blocking strategy design for large-scale resolution
- Quality metric definition and monitoring setup
- Distributed architecture patterns for identity graphs
- Real-time versus batch resolution tradeoffs

Vendor Evaluation Support
- Capability assessment for identity resolution providers
- Technical due diligence questions for vendor selection
- Integration architecture for third-party identity services
- Benchmarking methodology for resolution accuracy
- Total cost of ownership analysis including scale factors

INTEGRATION WITH OTHER AGENTS

AUD Agent Internal Usage
- Apply identity graph algorithms to audience building workflows
- Use blocking strategies when processing large customer files
- Leverage graph clustering for household composition
- Implement cross-device resolution for unified audience profiles
- Monitor identity quality metrics for audience accuracy

Cross-Agent Data Flows
- NDS Agent receives resolved identity counts for reach estimation
- CSO Agent uses identity persistence for journey continuity
- PRF Agent requires identity accuracy for measurement validity
- ANL Agent leverages identity links for attribution modeling
- DOC Agent references resolution methodology in documentation

PRACTICAL IMPLEMENTATION RECOMMENDATIONS

Starting Point Selection
- Begin with deterministic matching on authenticated identifiers
- Add probabilistic methods incrementally as data permits
- Implement blocking before attempting large-scale resolution
- Start with connected components before advanced clustering
- Establish quality baselines before optimization

Quality Assurance Protocols
- Labeled test set for accuracy measurement
- Human review workflow for edge cases
- Automated monitoring with alerting thresholds
- Regular model retraining on fresh labeled data
- A/B testing for algorithm improvements

Privacy and Compliance Integration
- Consent tracking at identifier level
- Purpose limitation for identity linkages
- Data minimization in stored attributes
- Retention policies for identity relationships
- Audit logging for compliance demonstration

COMMON PITFALLS AND MITIGATIONS

Over-Clustering Prevention
- Set conservative match thresholds initially
- Implement explicit non-match constraints
- Monitor cluster size distribution for anomalies
- Use correlation clustering with negative edges
- Regular human review of largest clusters

Under-Clustering Detection
- Track cross-cluster strong edge frequency
- Monitor duplicate entity indicators
- Compare resolved counts to expected population
- Use multiple blocking passes for coverage
- Review singleton clusters for missed matches

Scalability Planning
- Design for ten times current data volume
- Implement horizontal scaling from the start
- Use batch processing for non-critical updates
- Cache frequently accessed entities
- Plan distributed architecture before reaching limits

================================================================================
END OF DOCUMENT
================================================================================
