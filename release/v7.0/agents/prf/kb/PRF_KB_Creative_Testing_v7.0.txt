PRF KNOWLEDGE BASE - CREATIVE TESTING v1
VERSION: 1.0
STATUS: Production Ready
COMPLIANCE: 6-Rule Compliant

================================================================================
SECTION 1 - CREATIVE TESTING FUNDAMENTALS
================================================================================

CREATIVE TESTING PURPOSE

Creative testing systematically evaluates advertising creative variations to identify top performers and optimize campaign effectiveness.

TESTING BENEFITS
- Identify highest-performing creative
- Reduce wasted spend on poor performers
- Generate learnings for future creative
- Improve overall campaign efficiency

TESTING CHALLENGES
- Statistical significance requirements
- Platform algorithm interference
- Multiple variables complexity
- Time and resource constraints

TESTING HIERARCHY

CONCEPT TESTING
Compare fundamentally different creative approaches.
Example: Emotional appeal vs rational appeal.

MESSAGE TESTING
Compare different value propositions or claims.
Example: Price focus vs quality focus.

EXECUTION TESTING
Compare variations within same concept.
Example: Different headlines for same visual.

OPTIMIZATION TESTING
Fine-tune elements of proven creative.
Example: CTA button color or copy.

================================================================================
SECTION 2 - STATISTICAL FRAMEWORK
================================================================================

SAMPLE SIZE CALCULATION

MINIMUM SAMPLE SIZE FORMULA
N = (Z^2 x p x (1-p)) / E^2

Where:
- Z = Z-score for confidence level (1.96 for 95%)
- p = Baseline conversion rate
- E = Margin of error (minimum detectable effect)

SIMPLIFIED CALCULATION
For 95% confidence, 80% power:
N_per_variant = 16 x Variance / Effect_Size^2

PRACTICAL CALCULATION
For conversion rate testing:
N_per_variant = (16 x p x (1-p)) / MDE^2

EXAMPLE
- Baseline CTR: 1% (0.01)
- Minimum detectable effect: 0.2% (20% relative lift)
- Absolute MDE: 0.002
- N = (16 x 0.01 x 0.99) / 0.002^2 = 39,600 per variant

STATISTICAL SIGNIFICANCE

P-VALUE INTERPRETATION
- p less than 0.05: Statistically significant at 95% confidence
- p less than 0.01: Highly significant at 99% confidence
- p greater than 0.10: Not statistically significant

CONFIDENCE INTERVALS
Report results with confidence intervals:
Result = Point_Estimate +/- Margin_of_Error

PRACTICAL VS STATISTICAL SIGNIFICANCE
- Statistically significant but small effect: May not be actionable
- Large effect but not significant: Need more data
- Both significant and meaningful: Take action

COMMON ERRORS

TYPE I ERROR (FALSE POSITIVE)
Concluding difference exists when it does not.
Controlled by significance level (alpha).

TYPE II ERROR (FALSE NEGATIVE)
Failing to detect real difference.
Controlled by statistical power.

MULTIPLE COMPARISON PROBLEM
Testing many variants inflates false positive rate.
Bonferroni correction: alpha/number of tests
Better: Sequential testing or Bayesian approaches

================================================================================
SECTION 3 - TESTING METHODOLOGIES
================================================================================

A/B TESTING

DEFINITION
Compare two variants with random assignment.

SETUP
- Control: Current or baseline creative
- Treatment: New creative variation
- Random split: 50/50 typical
- Measure: Key performance metric

ADVANTAGES
- Simple to implement
- Clear interpretation
- Statistical rigor

LIMITATIONS
- Tests only two variants
- Requires significant traffic
- One variable at a time ideal

MULTIVARIATE TESTING (MVT)

DEFINITION
Test multiple variables simultaneously to find optimal combination.

SETUP
- Define variables (headline, image, CTA)
- Define levels for each variable
- Full factorial: All combinations tested
- Fractional factorial: Subset of combinations

ADVANTAGES
- Test interactions between elements
- Efficient for multiple variables
- Comprehensive insights

LIMITATIONS
- Requires very high traffic
- Complex analysis
- Longer test duration

TRAFFIC REQUIREMENTS
Full factorial with 3 variables x 3 levels = 27 combinations
Each needing significant sample size.

SEQUENTIAL TESTING

DEFINITION
Analyze results as data accumulates, stopping when conclusion reached.

ADVANTAGES
- Faster decisions possible
- More efficient use of traffic
- Can stop early with clear winner

IMPLEMENTATION
- Set stopping rules in advance
- Use sequential analysis methods
- Maintain statistical validity

MULTI-ARMED BANDIT

DEFINITION
Algorithm that balances exploration (testing) with exploitation (using winners).

HOW IT WORKS
- Start with equal allocation
- Shift traffic toward better performers
- Continue learning while optimizing

ADVANTAGES
- Minimizes opportunity cost
- Adapts in real-time
- Good for ongoing optimization

LIMITATIONS
- Less rigorous statistical conclusions
- May not find true winner
- Platform-specific implementation

================================================================================
SECTION 4 - PLATFORM TESTING FEATURES
================================================================================

META CREATIVE TESTING

A/B TESTING
- Compare up to 5 test cells
- Control and variations
- Statistical significance reported
- Test specific variables

DYNAMIC CREATIVE
- Multiple assets in one ad
- Algorithm finds combinations
- Less control over testing
- Good for optimization

CREATIVE REPORTING
- Asset-level performance
- Ranking by metric
- Not full A/B test rigor

GOOGLE ADS TESTING

AD VARIATIONS
- Test specific changes across campaigns
- Headlines, descriptions, URLs
- Automated significance calculation

RESPONSIVE SEARCH ADS
- Multiple headlines and descriptions
- Algorithm optimizes combinations
- Asset performance ratings
- Limited isolation of variables

EXPERIMENTS
- Campaign-level experiments
- Control vs experiment split
- Full statistical reporting

PROGRAMMATIC TESTING

DSP CREATIVE ROTATION
- Even rotation for testing
- Performance-based optimization
- Manual analysis often needed

THIRD-PARTY TESTING
- Creative testing platforms
- Cross-platform consistency
- Unified reporting

================================================================================
SECTION 5 - CREATIVE TESTING BEST PRACTICES
================================================================================

TEST DESIGN PRINCIPLES

TEST ONE VARIABLE
When possible, isolate single variable.
Confounding variables obscure insights.

MEANINGFUL DIFFERENCES
Test concepts that are meaningfully different.
Minor tweaks yield minor (undetectable) differences.

CLEAR HYPOTHESIS
State expected outcome before testing.
Hypothesis: Variant B will increase CTR by 15% because of emotional appeal.

DEFINE SUCCESS METRIC
Choose primary metric before test starts.
Secondary metrics for additional insight.

TEST STRUCTURE

MINIMUM VIABLE TEST
- 2 variants (A/B)
- Clear primary metric
- Adequate sample size
- Defined test duration

STANDARD TEST
- 3-5 variants
- Primary and secondary metrics
- Segmented analysis
- Learning documentation

COMPREHENSIVE TEST
- Multiple concepts
- Full measurement suite
- Segment and audience analysis
- Creative library building

TIMING CONSIDERATIONS

TEST DURATION
- Minimum 7 days (capture day-of-week variation)
- Typically 2-4 weeks
- Until statistical significance reached
- Cap at reasonable duration

SEASONALITY
- Avoid testing during anomalous periods
- Results may not generalize
- Note seasonal context

TRAFFIC REQUIREMENTS
- Estimate required sample size
- Verify traffic can support test
- Consider traffic allocation percentage

================================================================================
SECTION 6 - CREATIVE FATIGUE DETECTION
================================================================================

FATIGUE DEFINITION

Creative fatigue occurs when audience over-exposure to creative reduces effectiveness.

FATIGUE INDICATORS
- CTR declining over time
- Frequency increasing
- Conversion rate declining
- CPM or CPC increasing

FATIGUE DETECTION

MONITORING METRICS
Track week-over-week changes in:
- Click-through rate
- Conversion rate
- Cost per result
- Engagement rate

FATIGUE THRESHOLDS
- CTR decline: 20%+ from peak
- Frequency: Above 4-5 per week (prospecting)
- CPM increase: 20%+ without competitive cause
- Conversion decline: 15%+ from baseline

TIME-BASED ANALYSIS
Plot performance over time.
Identify inflection points.
Correlate with frequency.

FATIGUE MITIGATION

CREATIVE REFRESH SCHEDULE
- Awareness campaigns: Every 4-6 weeks
- Consideration: Every 3-4 weeks
- Conversion: Every 2-3 weeks
- Retargeting: Every 2 weeks

ROTATION STRATEGIES
- Always have new creative in pipeline
- Rotate rather than replace entirely
- Test new concepts continuously
- Archive and potentially revive later

FREQUENCY MANAGEMENT
- Set appropriate frequency caps
- Exclude recently converted
- Segment by exposure level
- Consider channel-level frequency

================================================================================
SECTION 7 - LEARNING DOCUMENTATION
================================================================================

TEST DOCUMENTATION

PRE-TEST DOCUMENTATION
- Hypothesis and rationale
- Test setup details
- Success metrics defined
- Expected impact

POST-TEST DOCUMENTATION
- Results summary
- Statistical significance
- Key learnings
- Recommended actions

CREATIVE LEARNING LIBRARY

LIBRARY COMPONENTS
- Test results archive
- Winning elements database
- Losing elements database
- Audience-specific insights

LEARNING CATEGORIES
- Visual elements (colors, imagery, layout)
- Copy elements (headlines, CTAs, tone)
- Format performance
- Audience response patterns

KNOWLEDGE TRANSFER

SHARING LEARNINGS
- Structured learning reports
- Cross-team communication
- Creative brief integration
- Benchmark updates

APPLYING LEARNINGS
- Inform new creative development
- Update creative guidelines
- Refine testing hypotheses
- Build institutional knowledge

================================================================================
SECTION 8 - AGENT APPLICATION GUIDANCE
================================================================================

TEST RECOMMENDATION FRAMEWORK

WHEN TO TEST
- Launching new campaign
- Performance declining
- New creative concept
- Audience expansion

TEST PRIORITIZATION
1. High-impact concepts (different approaches)
2. Message variations (value propositions)
3. Visual variations (imagery, video)
4. Execution details (CTAs, colors)

SAMPLE SIZE GUIDANCE

QUICK CALCULATION
For 95% confidence, 80% power, and 20% relative MDE:
- 1% baseline conversion: ~40,000 per variant
- 2% baseline conversion: ~20,000 per variant
- 5% baseline conversion: ~8,000 per variant

TRAFFIC ALLOCATION
- Test traffic: 20-50% of campaign typically
- Balance learning vs performance
- Consider opportunity cost

REPORTING GUIDANCE

TEST RESULT COMMUNICATION
- Lead with recommendation
- State statistical confidence
- Show magnitude of difference
- Note caveats and limitations

AVOID OVER-INTERPRETATION
- Do not generalize from single test
- Consider context and timing
- Replicate important findings
- Be humble about certainty

COMMON PITFALLS

ENDING TESTS EARLY
- Wait for statistical significance
- Full week minimum
- Avoid peeking and stopping

TESTING TOO MANY VARIANTS
- Traffic spread too thin
- None reach significance
- Focus on fewer, bigger differences

IGNORING SEGMENTS
- Overall result may hide segment differences
- Analyze by audience, device, placement
- Different creative for different segments

================================================================================
END OF DOCUMENT
================================================================================
