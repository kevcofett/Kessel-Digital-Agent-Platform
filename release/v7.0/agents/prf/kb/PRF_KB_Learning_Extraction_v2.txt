PRF KNOWLEDGE BASE - LEARNING EXTRACTION AND INSIGHT SYNTHESIS

OVERVIEW

This document provides comprehensive methodology for extracting actionable learnings from campaign performance data, synthesizing cross-campaign insights, detecting recurring patterns, and generating optimization playbooks. Learning extraction transforms raw performance data into institutional knowledge that improves future campaign planning and execution.

Learning extraction is the systematic process of identifying what worked, what did not work, and why. This process converts campaign outcomes into reusable knowledge assets. Effective learning extraction accelerates organizational capability development and prevents repeated mistakes across campaigns.

The learning extraction system operates across four interconnected capabilities. Learning Extractor identifies specific insights from individual campaigns. Cross-Campaign Insights aggregates patterns across multiple campaigns. Pattern Detection identifies recurring success and failure modes. Playbook Generator creates actionable guidance documents from accumulated learnings.

LEARNING EXTRACTION METHODOLOGY

EXTRACTION FRAMEWORK

Learning extraction follows a structured five-step process that ensures comprehensive coverage and actionable outputs.

Step one is data collection. Gather all relevant performance metrics, creative assets, audience definitions, channel allocations, and timing parameters from the campaign. Include both quantitative metrics and qualitative observations from campaign managers.

Step two is performance classification. Categorize each metric and dimension as exceeding expectations, meeting expectations, or underperforming expectations. Use pre-defined benchmarks and campaign-specific targets as reference points.

Step three is root cause analysis. For each significant deviation from expectations, identify the likely contributing factors. Consider external factors like seasonality, competitive activity, and market conditions alongside execution factors.

Step four is learning formulation. Translate root cause findings into specific, actionable learning statements. Each learning must specify the context, the observation, and the implication for future campaigns.

Step five is confidence scoring. Assign confidence levels to each learning based on data quality, sample size, and consistency with other evidence. High confidence learnings have strong statistical support and clear causal mechanisms.

LEARNING CATEGORIES

Learnings are classified into four primary categories that determine storage, retrieval, and application rules.

Success learnings document what worked well and why. These learnings identify tactics, strategies, or executions that exceeded expectations. Success learnings must specify the conditions under which the success occurred to enable appropriate replication.

Failure learnings document what did not work and why. These learnings identify approaches that underperformed expectations. Failure learnings must distinguish between execution failures and strategy failures to guide appropriate corrective action.

Insight learnings document unexpected observations or relationships discovered during analysis. These learnings often reveal new opportunities or risks not previously considered. Insight learnings require validation before incorporation into standard practices.

Pattern learnings document recurring themes observed across multiple campaigns or time periods. These learnings have the highest confidence levels due to repeated observation. Pattern learnings form the foundation for optimization playbooks.

LEARNING STATEMENT STRUCTURE

Every learning statement must follow a standardized structure that ensures clarity and actionability.

Context statement describes the specific situation where the learning applies. Include relevant parameters like channel, audience type, objective, budget level, and timing. Context enables appropriate matching of learnings to future situations.

Observation statement describes what was observed with specific metrics and timeframes. Use quantitative data wherever possible. Avoid vague characterizations like good or bad performance.

Implication statement describes what should be done differently in future campaigns. Provide specific, actionable guidance rather than general principles. Include any conditions or caveats that affect application.

Confidence indicator rates the reliability of the learning on a scale from zero to one. Confidence above 0.8 indicates high reliability suitable for standard practice. Confidence between 0.5 and 0.8 indicates moderate reliability requiring situational judgment. Confidence below 0.5 indicates preliminary findings requiring additional validation.

Evidence reference points to the source data and analysis supporting the learning. Include campaign identifiers, date ranges, and specific metrics referenced. Evidence enables verification and refinement over time.

CONFIDENCE SCORING METHODOLOGY

Confidence scores reflect the reliability and generalizability of each learning. Multiple factors contribute to the final confidence score.

Statistical significance contributes up to 0.3 points. Learnings supported by statistically significant differences at the 95 percent level receive full points. Marginal significance at 90 percent receives 0.2 points. Non-significant differences receive zero points.

Sample size contributes up to 0.2 points. Campaigns with large budgets and extended durations provide more reliable data. Minimum thresholds apply for each channel and metric type.

Consistency contributes up to 0.2 points. Learnings consistent with findings from other campaigns receive full points. Contradictory evidence reduces consistency score proportionally.

Causal clarity contributes up to 0.2 points. Learnings with clear causal mechanisms receive full points. Correlational findings without causal explanation receive partial points.

Recency contributes up to 0.1 points. Recent learnings receive full points. Learnings decay over time as market conditions change. Half-life varies by learning type with tactical learnings decaying faster than strategic learnings.

CROSS-CAMPAIGN INSIGHT AGGREGATION

AGGREGATION RULES

Cross-campaign insights combine learnings from multiple campaigns to identify broader patterns and principles. Aggregation follows specific rules to ensure valid synthesis.

Minimum campaign threshold requires at least three campaigns with similar characteristics before aggregation. Two-campaign comparisons may identify preliminary patterns but cannot establish cross-campaign insights.

Similarity criteria define which campaigns can be aggregated. Campaigns must share at least two of the following characteristics: same vertical industry, same primary objective, same primary channel, same audience type, or same budget tier.

Contradiction resolution addresses conflicting learnings from different campaigns. When learnings contradict, examine contextual differences that may explain the discrepancy. If no explanation emerges, note the contradiction and reduce confidence scores for both learnings.

Temporal weighting gives more influence to recent campaigns. Apply exponential decay with half-life of 180 days for tactical learnings and 365 days for strategic learnings. Very old learnings may be archived rather than aggregated.

INSIGHT SYNTHESIS PROCESS

Synthesis transforms individual campaign learnings into generalized cross-campaign insights through systematic analysis.

Clustering groups similar learnings based on topic, channel, tactic, or outcome. Use semantic similarity to identify learnings addressing the same underlying phenomenon even when stated differently.

Abstraction identifies the common principle underlying clustered learnings. Remove campaign-specific details to formulate generalizable statements. Preserve essential context required for appropriate application.

Validation tests synthesized insights against holdout campaigns not included in the original analysis. Insights that accurately predict holdout campaign outcomes receive higher confidence. Insights that fail validation require refinement or rejection.

Documentation captures the full provenance of synthesized insights. Record all source campaigns, individual learnings, synthesis methodology, and validation results. Provenance enables future refinement and challenge.

INSIGHT CATEGORIES

Cross-campaign insights are categorized by scope and application.

Tactical insights address specific execution decisions like creative elements, targeting parameters, or bidding strategies. These insights have narrow application but high specificity.

Strategic insights address broader planning decisions like channel mix, audience prioritization, or budget allocation. These insights have wide application but require situational adaptation.

Contextual insights describe how effectiveness varies by situation. These insights specify when different approaches work best rather than identifying universally superior approaches.

Cautionary insights identify common mistakes or pitfalls observed across campaigns. These insights help planners avoid known failure modes.

PATTERN DETECTION ALGORITHMS

PATTERN TYPES

The pattern detection system identifies several distinct pattern types that recur across campaigns and time periods.

Performance patterns describe recurring relationships between inputs and outputs. Examples include response curves showing diminishing returns at high spend levels or audience fatigue patterns showing declining engagement over time.

Seasonal patterns describe recurring variations tied to calendar periods. Examples include holiday shopping peaks, back-to-school timing, or quarterly business cycles. Seasonal patterns enable proactive planning adjustments.

Competitive patterns describe recurring competitive dynamics. Examples include share of voice correlations with market share or competitive response patterns to promotional activity.

Creative patterns describe recurring creative performance characteristics. Examples include video length optimal ranges or messaging themes that consistently resonate with specific audiences.

Anomaly patterns describe unusual events that recur under specific conditions. Examples include algorithm changes causing temporary performance disruptions or external events causing attention spikes.

DETECTION METHODOLOGY

Pattern detection employs multiple analytical techniques depending on pattern type and data characteristics.

Time series analysis identifies seasonal and trend patterns in longitudinal data. Apply decomposition to separate trend, seasonal, and residual components. Use statistical tests to confirm significance of detected patterns.

Correlation analysis identifies relationships between variables across campaigns. Calculate correlation coefficients and test for statistical significance. Distinguish correlation from causation through additional analysis.

Clustering analysis groups campaigns with similar performance profiles. Use k-means or hierarchical clustering to identify natural groupings. Examine cluster characteristics to identify distinguishing patterns.

Anomaly detection identifies unusual observations that may signal emerging patterns or one-time events. Use statistical control limits or machine learning models to flag anomalies. Investigate flagged items to determine pattern potential.

Sequence analysis identifies patterns in the order of events or touchpoints. Apply sequential pattern mining to customer journey data. Identify common pathways associated with conversion or abandonment.

PATTERN VALIDATION

Detected patterns require validation before incorporation into playbooks or recommendations.

Statistical validation confirms that patterns are unlikely to have occurred by chance. Apply appropriate statistical tests based on pattern type. Require significance at 95 percent confidence for pattern confirmation.

Logical validation confirms that patterns have plausible explanations. Patterns without logical explanation may be spurious correlations. Subject unexplained patterns to additional scrutiny.

Temporal validation confirms that patterns persist over time. Apply out-of-time validation using recent data not included in detection. Patterns that fail temporal validation may reflect historical conditions no longer applicable.

Cross-validation confirms that patterns generalize across different campaign subsets. Apply k-fold validation to test pattern stability. Patterns that vary substantially across folds have limited generalizability.

PLAYBOOK GENERATION

PLAYBOOK STRUCTURE

Optimization playbooks translate accumulated learnings and patterns into actionable guidance documents. Every playbook follows a standardized structure.

Executive summary provides a brief overview of the playbook scope, key recommendations, and expected impact. Limit to one page for quick reference.

Situation analysis describes the context where the playbook applies. Specify the objective, channel, audience, and other relevant parameters. Clearly state boundary conditions where the playbook should not be applied.

Key learnings summarizes the most important findings supporting playbook recommendations. Include confidence scores and evidence references. Organize by theme or decision area.

Recommendations provides specific, actionable guidance for campaign planning and execution. Structure recommendations as clear directives with supporting rationale. Include both must-do and must-avoid items.

Implementation checklist provides a step-by-step guide for applying playbook recommendations. Enable practitioners to verify compliance with guidance. Include timing recommendations for each step.

Measurement framework defines how to assess playbook effectiveness. Specify metrics to track and targets to achieve. Enable continuous improvement through performance feedback.

Appendix provides detailed supporting analysis and data. Include for reference without cluttering main document. Enable deep dives for those requiring additional detail.

PLAYBOOK TRIGGERS

Playbooks are generated when specific conditions are met indicating sufficient knowledge accumulation.

Learning threshold triggers playbook generation when the number of validated learnings in a specific area exceeds the minimum threshold. Default threshold is ten learnings but varies by playbook type.

Pattern confirmation triggers playbook generation when a significant pattern is validated across multiple campaigns. Pattern-based playbooks have higher confidence than learning-only playbooks.

Request trigger enables manual playbook generation for specific needs even when automatic thresholds are not met. Manual playbooks are flagged as preliminary pending additional validation.

Update trigger regenerates existing playbooks when significant new learnings or patterns are identified. Track playbook version and change log. Notify stakeholders of material updates.

PLAYBOOK TYPES

Different playbook types serve different planning and execution needs.

Channel playbooks provide guidance specific to individual media channels. Cover channel selection criteria, optimization tactics, and common pitfalls. One playbook per major channel.

Objective playbooks provide guidance organized by campaign objective. Cover awareness building, consideration driving, conversion optimization, and retention. Objective playbooks span channels.

Audience playbooks provide guidance specific to audience segments. Cover targeting approaches, messaging themes, and channel preferences. Useful for account-based or segment-specific planning.

Tactical playbooks provide guidance on specific tactics like retargeting, sequential messaging, or promotional timing. Highly specific and actionable for execution teams.

Troubleshooting playbooks provide guidance for diagnosing and resolving common performance problems. Organized by symptom with diagnostic steps and remediation options.

TEMPORAL RELEVANCE MANAGEMENT

LEARNING DECAY

Learnings lose relevance over time as market conditions, consumer behavior, and platform capabilities evolve. The system manages temporal relevance through decay modeling.

Decay half-life varies by learning type. Tactical learnings about specific platform features have short half-lives of 90 to 180 days. Strategic learnings about audience behavior have longer half-lives of 365 to 730 days. Fundamental learnings about human psychology have very long half-lives exceeding 1000 days.

Decay triggers accelerate relevance loss when specific events occur. Platform algorithm changes trigger accelerated decay for affected platform learnings. Major market disruptions trigger accelerated decay for affected market learnings. Competitive entry or exit triggers accelerated decay for competitive learnings.

Freshness scoring combines recency with decay modeling to calculate current relevance. Freshness scores range from zero to one with scores below 0.3 triggering archival consideration. Display freshness scores alongside learnings to guide appropriate application.

ARCHIVAL RULES

Learnings below relevance thresholds are archived rather than deleted. Archival preserves institutional memory while preventing application of outdated guidance.

Archive threshold is 0.2 freshness score sustained for 30 days. Learnings meeting this threshold move to archive status. Archived learnings are excluded from aggregation and playbook generation.

Retrieval from archive is possible for historical analysis or validation purposes. Archived learnings are flagged as historical when retrieved. Users must acknowledge archival status before application.

Permanent deletion occurs only for learnings explicitly invalidated by subsequent evidence. Deletion requires documentation of invalidation rationale. Deleted learnings are logged for audit purposes.

CONTRADICTION RESOLUTION

DETECTION OF CONTRADICTIONS

Contradictory learnings occur when different campaigns produce opposing conclusions about the same phenomenon. The system detects and manages contradictions systematically.

Semantic matching identifies learnings addressing the same topic or decision. Apply natural language processing to identify semantic similarity. Flag learning pairs with high similarity but opposing recommendations.

Logical analysis identifies learnings that cannot both be true. Apply formal logic to identify mutual exclusivity. Consider conditional statements that may resolve apparent contradictions.

RESOLUTION APPROACHES

Multiple approaches exist for resolving contradictory learnings depending on the nature of the contradiction.

Contextual resolution identifies situational factors that explain the contradiction. Different contexts may legitimately produce different outcomes. Refine learnings to specify applicable contexts.

Confidence resolution favors higher confidence learnings when contexts are similar. Archive or downweight lower confidence contradicting learnings. Document resolution rationale.

Synthesis resolution combines contradicting learnings into conditional statements. Specify conditions where each learning applies. Create more nuanced guidance than either learning alone.

Escalation resolution flags unresolvable contradictions for human review. Some contradictions reflect genuine uncertainty requiring judgment. Maintain both learnings with contradiction notation until resolved.

INTEGRATION WITH PRF CAPABILITIES

ATTRIBUTION INTEGRATION

Learning extraction integrates with attribution analysis to identify channel-specific learnings. Attribution data reveals which channels drove outcomes enabling channel-specific learning extraction. Multi-touch attribution provides richer learning context than last-touch models.

ANOMALY INTEGRATION

Learning extraction integrates with anomaly detection to identify unexpected outcomes requiring investigation. Anomalies often signal important learnings not visible in aggregate performance. Automated anomaly flagging accelerates learning identification.

FORECASTING INTEGRATION

Learning extraction feeds forecasting models to improve prediction accuracy. Historical learnings inform assumption setting for forecasts. Forecast errors generate learnings for model refinement.

INCREMENTALITY INTEGRATION

Learning extraction integrates with incrementality testing to identify causal learnings. Incrementality results provide highest confidence learnings due to experimental design. Prioritize incrementality-derived learnings in playbooks.

IMPLEMENTATION REQUIREMENTS

DATA REQUIREMENTS

Effective learning extraction requires comprehensive data capture throughout campaign lifecycle. Minimum required data elements include campaign objectives, target audience definitions, channel allocations, creative specifications, timing parameters, and performance metrics across all standard KPIs.

PROCESS REQUIREMENTS

Learning extraction requires dedicated analytical resources and defined processes. Assign responsibility for extraction activities. Establish regular extraction cadence aligned with campaign completion. Create feedback loops between extraction and planning teams.

QUALITY REQUIREMENTS

Learning quality depends on analytical rigor and documentation discipline. Apply statistical methods appropriately for each analysis type. Document all assumptions and limitations. Review learnings for clarity and actionability before storage.

GOVERNANCE REQUIREMENTS

Learning governance ensures appropriate access, usage, and maintenance. Define roles for learning creation, review, and approval. Establish retention policies aligned with relevance decay. Audit learning application for compliance with guidance.
