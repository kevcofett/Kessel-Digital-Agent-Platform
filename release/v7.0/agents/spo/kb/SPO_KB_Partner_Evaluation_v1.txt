SPO KNOWLEDGE BASE - PARTNER EVALUATION v1
VERSION: 1.0
STATUS: Production Ready
COMPLIANCE: 6-Rule Compliant

================================================================================
SECTION 1 - DSP EVALUATION FRAMEWORK
================================================================================

DSP SELECTION CRITERIA

Selecting the right Demand Side Platform requires evaluation across multiple dimensions. Each criterion should be weighted based on advertiser priorities and campaign objectives.

CORE EVALUATION DIMENSIONS

FEE TRANSPARENCY (Weight: 20 percent)
- Full disclosure of platform fees
- Clear breakdown of all costs
- No hidden markups or arbitrage
- Log-level data availability

SUPPLY PATH VISIBILITY (Weight: 15 percent)
- Ability to see full bid path
- Seller identification
- Reseller flagging capability
- Supply path optimization tools

INVENTORY QUALITY (Weight: 20 percent)
- Premium publisher relationships
- MFA filtering capability
- Domain transparency
- Quality score availability

BRAND SAFETY (Weight: 15 percent)
- Pre-bid blocking options
- Keyword exclusion depth
- Third-party integration support
- Custom category creation

FRAUD PREVENTION (Weight: 15 percent)
- Pre-bid IVT filtering
- Post-bid fraud detection
- Bot traffic identification
- Click fraud prevention

REPORTING AND ANALYTICS (Weight: 15 percent)
- Granular reporting options
- Real-time data availability
- Custom dimension support
- API access for automation

DSP EVALUATION SCORECARD

SCORE 90-100: EXCELLENT
- Full transparency across all dimensions
- Industry-leading capabilities
- Strong partnership potential
- Recommended for primary DSP role

SCORE 75-89: GOOD
- Solid performance most dimensions
- Minor gaps in one or two areas
- Suitable for secondary DSP role
- Monitor improvement trajectory

SCORE 60-74: ADEQUATE
- Meets basic requirements
- Notable gaps in key areas
- Consider for specific use cases only
- Require improvement plan

SCORE BELOW 60: INSUFFICIENT
- Significant deficiencies
- Not recommended for use
- Review only if major improvements
- Seek alternatives

================================================================================
SECTION 2 - DSP EVALUATION BY TYPE
================================================================================

MAJOR INDEPENDENT DSPs

THE TRADE DESK
Strengths:
- Excellent fee transparency
- Strong supply path tools
- Premium inventory access
- Unified ID leadership

Considerations:
- Higher minimum spend
- Steep learning curve
- Managed service premium

Best For: Mid-to-large advertisers seeking transparency

GOOGLE DV360
Strengths:
- Google inventory integration
- YouTube access
- Strong audience targeting
- Broad reach

Considerations:
- Opaque pricing structure
- Walled garden limitations
- Attribution complexity

Best For: Advertisers prioritizing Google ecosystem

AMAZON DSP
Strengths:
- Amazon shopper data
- Retail media integration
- Purchase attribution
- Growing CTV inventory

Considerations:
- Limited non-Amazon visibility
- Minimum spend requirements
- Complex reporting

Best For: Retail and e-commerce advertisers

SPECIALTY DSPs

CONNECTED TV DSPs
- Innovid: Creative and measurement focus
- Tremor: Premium CTV inventory
- Viant: Household-level targeting
- Evaluation focus: CTV-specific capabilities

MOBILE DSPs
- InMobi: Global mobile reach
- Liftoff: App install focus
- ironSource: Gaming expertise
- Evaluation focus: Mobile-specific metrics

PROGRAMMATIC AUDIO DSPs
- AdsWizz: Podcast and streaming
- Triton: Radio and digital audio
- Evaluation focus: Audio inventory quality

================================================================================
SECTION 3 - SSP EVALUATION FRAMEWORK
================================================================================

SSP SELECTION CRITERIA

Supply Side Platform evaluation focuses on inventory quality, publisher relationships, and fee transparency.

CORE EVALUATION DIMENSIONS

PUBLISHER QUALITY (Weight: 25 percent)
- Direct publisher relationships
- Premium content publishers
- No MFA inventory
- Brand-safe environments

TRANSPARENCY (Weight: 20 percent)
- Take rate disclosure
- Bid floor clarity
- Auction type clarity
- Seller ID availability

TECHNICAL CAPABILITIES (Weight: 20 percent)
- Header bidding support
- Unified auction participation
- Bid deduplication
- Low latency

INVENTORY DIVERSITY (Weight: 15 percent)
- Format coverage (display, video, native)
- Device coverage (desktop, mobile, CTV)
- Geographic coverage
- Audience diversity

FEE STRUCTURE (Weight: 20 percent)
- Competitive take rates
- Volume discount availability
- Clear billing practices
- No hidden fees

SSP QUALITY TIERS

TIER 1 - PREMIUM SSPs
- Direct relationships with top publishers
- Full transparency
- Take rates: 15-20 percent
- Examples: OpenX, PubMatic, Magnite

TIER 2 - QUALITY SSPs
- Mix of direct and indirect inventory
- Good transparency
- Take rates: 18-25 percent
- Suitable for scale needs

TIER 3 - VOLUME SSPs
- Primarily indirect inventory
- Variable transparency
- Take rates: 20-30 percent
- Use with caution, monitor quality

TIER 4 - AVOID
- Unknown inventory sources
- Poor transparency
- High or undisclosed fees
- Not recommended

================================================================================
SECTION 4 - PARTNER EVALUATION PROCESS
================================================================================

EVALUATION METHODOLOGY

PHASE 1 - REQUIREMENTS GATHERING
Duration: 1-2 weeks

Activities:
- Document campaign objectives
- Define must-have capabilities
- Identify nice-to-have features
- Set budget parameters
- Establish timeline

Deliverable: Partner requirements document

PHASE 2 - MARKET SCAN
Duration: 1-2 weeks

Activities:
- Identify candidate partners
- Review public information
- Check industry references
- Assess market position
- Create shortlist (3-5 candidates)

Deliverable: Shortlist with initial assessment

PHASE 3 - DETAILED EVALUATION
Duration: 2-4 weeks

Activities:
- Request proposals or demos
- Conduct reference calls
- Review case studies
- Evaluate technical capabilities
- Score against criteria

Deliverable: Evaluation scorecard per candidate

PHASE 4 - COMMERCIAL NEGOTIATION
Duration: 1-2 weeks

Activities:
- Request pricing proposals
- Negotiate terms
- Review contracts
- Finalize SLAs
- Secure approvals

Deliverable: Final partner selection and contracts

EVALUATION QUESTIONNAIRE TEMPLATE

FEE TRANSPARENCY QUESTIONS
- What is your platform fee structure?
- Are there any additional fees beyond platform fee?
- Do you provide log-level data showing all costs?
- How are data fees passed through?
- What is included in managed service pricing?

SUPPLY PATH QUESTIONS
- How do you handle duplicate bid requests?
- What supply path optimization tools do you offer?
- Can you identify and exclude resellers?
- What percentage of inventory is direct vs indirect?
- How do you validate seller authenticity?

INVENTORY QUALITY QUESTIONS
- What is your MFA detection methodology?
- What percentage of inventory is premium publisher?
- How do you define and maintain inclusion lists?
- What viewability rates do you achieve?
- How do you handle fraud detection?

BRAND SAFETY QUESTIONS
- What pre-bid blocking options are available?
- Which third-party safety vendors integrate?
- Can you create custom category exclusions?
- How do you handle sensitive content?
- What is your response time for safety incidents?

================================================================================
SECTION 5 - VERIFICATION VENDOR EVALUATION
================================================================================

VERIFICATION VENDOR CATEGORIES

VIEWABILITY SPECIALISTS
- DoubleVerify: Industry standard MRC accredited
- IAS: Strong methodology and coverage
- Moat: Analytics-focused approach

BRAND SAFETY SPECIALISTS
- DoubleVerify: Comprehensive category coverage
- IAS: Strong contextual analysis
- Zefr: YouTube and social focus

FRAUD DETECTION SPECIALISTS
- Human (White Ops): Bot detection leader
- Pixalate: Cross-platform fraud
- DoubleVerify: Integrated fraud solution

VERIFICATION EVALUATION CRITERIA

ACCREDITATION (Weight: 25 percent)
- MRC accreditation status
- IAB certification
- Third-party audits
- Methodology transparency

COVERAGE (Weight: 20 percent)
- Platform coverage (DSPs, networks)
- Format coverage (display, video, CTV)
- Geographic coverage
- Mobile coverage

ACCURACY (Weight: 25 percent)
- False positive rate
- Detection methodology
- Benchmark performance
- Client references

INTEGRATION (Weight: 15 percent)
- Pre-bid integration depth
- Post-bid reporting
- API availability
- Custom configuration

SUPPORT (Weight: 15 percent)
- Account management
- Technical support
- Training resources
- Issue resolution time

================================================================================
SECTION 6 - DATA PROVIDER EVALUATION
================================================================================

DATA PROVIDER CATEGORIES

FIRST-PARTY DATA ONBOARDERS
- LiveRamp: Industry leader
- Experian: Credit bureau heritage
- Acxiom: Large consumer database
- Evaluation focus: Match rates, identity resolution

THIRD-PARTY DATA PROVIDERS
- Oracle Data Cloud: Broad segments
- Nielsen: Media measurement heritage
- TransUnion: Credit-informed segments
- Evaluation focus: Accuracy, freshness, scale

CONTEXTUAL DATA PROVIDERS
- Oracle Contextual: Page-level analysis
- GumGum: Visual context
- Peer39: Category classification
- Evaluation focus: Granularity, accuracy

LOCATION DATA PROVIDERS
- Foursquare: Visitation data
- PlaceIQ: Movement patterns
- Gravy Analytics: Location intelligence
- Evaluation focus: Precision, recency

DATA PROVIDER EVALUATION CRITERIA

ACCURACY (Weight: 30 percent)
- Validation methodology
- Third-party audits
- Sample testing results
- Benchmark comparisons

SCALE (Weight: 20 percent)
- Addressable audience size
- Geographic coverage
- Segment depth
- Match rates

FRESHNESS (Weight: 20 percent)
- Data update frequency
- Decay handling
- Real-time capability
- Historical depth

PRIVACY COMPLIANCE (Weight: 20 percent)
- Consent management
- GDPR compliance
- CCPA compliance
- Data sourcing transparency

COST EFFICIENCY (Weight: 10 percent)
- CPM pricing
- Volume discounts
- Minimum commitments
- ROI track record

================================================================================
SECTION 7 - PARTNER SCORING METHODOLOGY
================================================================================

WEIGHTED SCORING MODEL

STEP 1 - DEFINE WEIGHTS
Assign weights to each criterion based on advertiser priorities. Weights must sum to 100 percent.

STEP 2 - SCORE EACH CRITERION
Rate each partner on each criterion using scale 1-10:
- 1-2: Poor, does not meet requirements
- 3-4: Below average, significant gaps
- 5-6: Average, meets basic requirements
- 7-8: Good, exceeds requirements
- 9-10: Excellent, industry leading

STEP 3 - CALCULATE WEIGHTED SCORE
Weighted_Score = Sum of (Criterion_Score x Criterion_Weight)

STEP 4 - RANK AND COMPARE
Order partners by weighted score. Analyze score distribution across criteria.

EXAMPLE CALCULATION

Partner A Evaluation:
- Fee Transparency (20 percent): Score 8 = 1.6
- Supply Quality (25 percent): Score 7 = 1.75
- Brand Safety (20 percent): Score 9 = 1.8
- Fraud Prevention (20 percent): Score 8 = 1.6
- Support (15 percent): Score 6 = 0.9
- Total Weighted Score: 7.65

Partner B Evaluation:
- Fee Transparency (20 percent): Score 6 = 1.2
- Supply Quality (25 percent): Score 9 = 2.25
- Brand Safety (20 percent): Score 7 = 1.4
- Fraud Prevention (20 percent): Score 7 = 1.4
- Support (15 percent): Score 8 = 1.2
- Total Weighted Score: 7.45

Result: Partner A scores higher (7.65 vs 7.45)

================================================================================
SECTION 8 - PARTNER EVALUATION FORMULAS
================================================================================

SCORING FORMULAS

Weighted_Score = Sum of (Score_i x Weight_i) for all criteria
Normalized_Score = (Raw_Score - Min_Score) / (Max_Score - Min_Score) x 100
Relative_Value = Partner_Score / Best_Partner_Score x 100

COST EFFICIENCY FORMULAS

Effective_Rate = Total_Fees / Total_Spend x 100
Rate_Premium = Partner_Rate / Benchmark_Rate x 100
Cost_Per_Quality_Point = Total_Fees / Quality_Score

PARTNER COMPARISON FORMULAS

Quality_Cost_Ratio = Quality_Score / Effective_Rate
Net_Value_Score = Quality_Score - (Rate_Premium x Weight)
Recommendation_Index = Weighted_Score x (1 - Risk_Factor)

================================================================================
END OF DOCUMENT
================================================================================
