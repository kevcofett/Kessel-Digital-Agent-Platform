PRF KNOWLEDGE BASE - A/B TESTING METHODS

OVERVIEW

This document provides comprehensive methodology for designing, executing, and analyzing A/B tests in marketing contexts. A/B testing enables rigorous comparison of treatment variations through randomized controlled experiments. Proper statistical methods ensure valid conclusions and optimal resource allocation.

A/B testing is the gold standard for causal inference in digital marketing. Unlike observational methods, randomized experiments eliminate selection bias by design. Every visitor has equal probability of assignment to each variation, ensuring treatment and control groups are comparable.

SAMPLE SIZE CALCULATION

STATISTICAL POWER FUNDAMENTALS

Sample size determines the ability to detect true effects. Underpowered tests produce inconclusive results. Overpowered tests waste traffic that could inform future experiments. Proper sample size calculation balances these tradeoffs.

Power is the probability of detecting a true effect when it exists. Standard power levels are 80 percent for exploratory tests and 90 percent for confirmatory tests. Higher power requires larger samples but reduces false negative risk.

The minimum detectable effect is the smallest difference considered practically meaningful. Set this based on business context, not statistical convenience. A 0.1 percent lift may be statistically significant with large samples but meaningless for business decisions.

Key parameters for sample size calculation:
- Baseline conversion rate
- Minimum detectable effect (relative or absolute)
- Significance level (typically 0.05)
- Statistical power (typically 0.80)
- Number of variations
- One-tailed versus two-tailed test

CALCULATION FORMULAS

For binary outcomes like conversion rate:
- Sample size per variation = 2 times (z-alpha-half + z-beta) squared times p times (1-p) / delta squared
- Where p is baseline rate, delta is minimum detectable effect
- z-alpha-half is critical value for significance (1.96 for alpha=0.05)
- z-beta is critical value for power (0.84 for power=0.80)

For continuous outcomes like revenue per visitor:
- Sample size per variation = 2 times (z-alpha-half + z-beta) squared times sigma squared / delta squared
- Where sigma is standard deviation, delta is minimum detectable effect

Multiply by number of variations minus one for multiple treatment arms. Apply correction factors for unequal allocation or stratification.

PRACTICAL ADJUSTMENTS

Inflation factors account for real-world complications:
- Multiple comparisons: multiply by number of pairwise tests
- Stratification: typically reduces required sample by 5 to 15 percent
- Clustering: multiply by design effect (1 + rho times cluster size minus 1)
- Non-compliance: inflate for expected crossover rate

Rule of thumb minimums:
- At least 100 conversions per variation for stable estimates
- At least 1000 visitors per variation for reasonable precision
- At least 7 days to capture weekly patterns
- Full business cycle for seasonal businesses

TRAFFIC ESTIMATION

Estimate required test duration from sample size and traffic:
- Duration = Required sample size / Daily traffic per variation
- Round up to full weeks for weekly pattern capture
- Add buffer for early stopping or technical issues

If duration exceeds acceptable window, consider:
- Relaxing minimum detectable effect
- Reducing number of variations
- Using sequential testing to allow earlier decisions
- Focusing on higher-traffic segments

STATISTICAL SIGNIFICANCE TESTING

FREQUENTIST HYPOTHESIS TESTING

The null hypothesis is that there is no difference between variations. The alternative hypothesis is that a difference exists. Testing assesses whether observed data is inconsistent with the null hypothesis.

The p-value is the probability of observing data as extreme as measured if the null hypothesis were true. Small p-values indicate evidence against the null. P-values below the significance threshold (typically 0.05) lead to rejecting the null.

Type I error (false positive) occurs when rejecting a true null hypothesis. Significance level alpha controls Type I error rate. Setting alpha = 0.05 means accepting up to 5 percent false positive rate.

Type II error (false negative) occurs when failing to reject a false null hypothesis. Power equals 1 minus Type II error rate. Setting power = 0.80 means accepting up to 20 percent false negative rate.

TEST STATISTICS

For conversion rate comparison, use z-test or chi-squared test:
- z-statistic = (p1 - p2) / sqrt(p-pooled times (1-p-pooled) times (1/n1 + 1/n2))
- p-pooled = (x1 + x2) / (n1 + n2)
- Compare z-statistic to critical values (1.96 for two-tailed alpha=0.05)

For continuous outcomes, use t-test:
- t-statistic = (mean1 - mean2) / sqrt(s1-squared/n1 + s2-squared/n2)
- Use Welch's t-test for unequal variances
- Compare to t-distribution critical values

For multiple variations, use ANOVA or Kruskal-Wallis:
- Test overall difference first
- Apply post-hoc tests for pairwise comparisons
- Adjust for multiple comparisons

CONFIDENCE INTERVALS

Confidence intervals provide more information than binary significance decisions. A 95 percent confidence interval means that if we repeated the experiment many times, 95 percent of intervals would contain the true effect.

For conversion rate difference:
- Point estimate: p1 - p2
- Standard error: sqrt(p1(1-p1)/n1 + p2(1-p2)/n2)
- 95 percent CI: point estimate plus/minus 1.96 times standard error

For relative lift:
- Point estimate: (p1 - p2) / p2
- Use delta method for standard error
- CI may be asymmetric for large lifts

Interpret intervals substantively:
- If interval excludes zero, effect is significant
- If interval excludes minimum meaningful effect, effect may be too small to matter
- Interval width indicates precision of estimate

MULTIPLE COMPARISON CORRECTIONS

Testing multiple variations or metrics inflates false positive rate. With 20 independent tests at alpha=0.05, expect 1 false positive on average.

Bonferroni correction divides alpha by number of tests:
- Adjusted alpha = 0.05 / number of tests
- Conservative, may miss true effects
- Use for small number of critical tests

Holm-Bonferroni is less conservative:
- Rank p-values from smallest to largest
- Compare each to alpha / (n - rank + 1)
- Stop when first non-significant result found

False Discovery Rate (FDR) control is appropriate for exploratory analysis:
- Benjamini-Hochberg procedure controls expected proportion of false discoveries
- Set FDR at 5-10 percent for exploration
- More powerful than family-wise error rate control

SEQUENTIAL TESTING

MOTIVATION AND BENEFITS

Sequential testing allows examining results before the experiment completes. This enables stopping early when effects are large or clearly null. Efficiency gains range from 20 to 50 percent compared to fixed-horizon tests.

Fixed-horizon tests require waiting until predetermined sample size. Peeking at results and making decisions inflates false positive rate. A test analyzed 5 times has roughly 15 percent false positive rate instead of 5 percent.

Sequential methods formally account for multiple analyses. They maintain valid error rates while enabling flexibility. This matches practical needs of A/B testing programs.

GROUP SEQUENTIAL DESIGNS

Group sequential designs pre-specify analysis times. At each interim analysis, compare test statistic to adjusted boundaries. Stop if boundaries are crossed; otherwise continue.

Popular boundary types:
- OBrien-Fleming: conservative early, spending alpha at end. Good for confirming expected effects.
- Pocock: constant boundaries across analyses. More likely to stop early.
- Haybittle-Peto: very conservative interim (alpha=0.001), full alpha at final.

Choose number of looks based on practical constraints. More looks enable faster stopping but reduce power slightly. Five looks is a common choice balancing flexibility and efficiency.

Alpha spending functions generalize to continuous monitoring:
- Define cumulative alpha spent as function of information fraction
- OBrien-Fleming spending: alpha times (2 - 2 times Phi(z-alpha-half / sqrt(t)))
- Pocock spending: alpha times log(1 + (e-1) times t)

ALWAYS VALID INFERENCE

Modern sequential methods enable continuous monitoring without pre-specifying analysis times. These provide valid inference regardless of when experimenter decides to look.

Mixture sequential probability ratio tests maintain valid confidence sequences. At any stopping time, conclusions are valid. This addresses practical challenges of pre-specifying interim analyses.

Implementation approaches:
- Confidence sequences based on betting interpretations
- Anytime-valid p-values from e-values
- Always-valid confidence intervals from conjugate priors

Trade-off is slightly wider confidence intervals compared to fixed-horizon tests. Practical efficiency gains typically outweigh this cost.

BAYESIAN A/B TESTING

BAYESIAN FRAMEWORK

Bayesian testing treats unknown parameters as random variables with probability distributions. Prior distributions encode beliefs before data. Posterior distributions update beliefs given data. Decisions use full posterior, not just point estimates.

Key advantages of Bayesian approach:
- Intuitive probability statements about which variation is best
- Natural incorporation of prior knowledge
- No multiple comparison problem from continuous monitoring
- Direct probability of practical significance

Prior specification influences results, especially with small samples. Use weakly informative priors that encode reasonable ranges without dominating data. Beta(1,1) is uniform for conversion rates. Beta(2,2) is mildly informative toward 50 percent.

POSTERIOR CALCULATION

For binary outcomes with conjugate priors:
- Prior: Beta(alpha-prior, beta-prior)
- Likelihood: Binomial
- Posterior: Beta(alpha-prior + conversions, beta-prior + non-conversions)

Calculate probability that treatment beats control:
- Simulate from both posteriors
- Count proportion where treatment sample exceeds control sample
- This probability directly answers the business question

For continuous outcomes with normal priors:
- Prior: Normal-Gamma for mean and precision
- Posterior: Normal-Gamma with updated parameters
- Calculate probability of meaningful difference

DECISION RULES

Bayesian decision rules directly optimize business outcomes:
- Expected loss minimization: choose action minimizing expected cost of being wrong
- Probability threshold: implement variation if probability of being best exceeds threshold
- Value of information: continue testing if expected information value exceeds cost

Expected loss calculation:
- For each variation, calculate expected value if it is not actually best
- Weight by probability it is not best
- Choose variation minimizing this expected loss

Typical thresholds:
- 95 percent probability of being best for low-risk decisions
- 99 percent for high-stakes or hard-to-reverse changes
- Consider asymmetric thresholds if upside and downside costs differ

SAMPLE RATIO MISMATCH DETECTION

WHAT IS SRM

Sample ratio mismatch occurs when actual allocation differs from intended allocation. If 50/50 split yields 52/48, something is wrong. SRM indicates implementation bugs, bot traffic, or biased dropoff that invalidates results.

SRM can bias results even if final analysis is statistically significant. The mechanism causing unequal allocation may also affect outcomes. Never trust results from experiments with significant SRM.

Common SRM causes:
- Bucketing algorithm bugs
- Session versus user randomization mismatches
- Bot traffic affecting one variation differently
- Redirect delays causing differential dropoff
- Ad blocker interference
- Browser caching issues

SRM DETECTION METHODS

Chi-squared test compares observed to expected counts:
- Expected count = total times allocation probability
- Test statistic = sum of (observed - expected) squared / expected
- Compare to chi-squared distribution with k-1 degrees of freedom

For 50/50 split:
- Calculate chi-squared = (n1 - n2) squared / (n1 + n2)
- P-value below 0.001 indicates SRM
- Use conservative threshold due to serious implications

Sequential SRM monitoring during experiment:
- Calculate daily chi-squared statistics
- Alert if cumulative evidence exceeds threshold
- Investigate before concluding experiment

SRM INVESTIGATION

When SRM is detected:
- Pause experiment immediately
- Investigate randomization implementation
- Check for differential bot traffic
- Examine dropoff rates by variation
- Review technical implementation details

Never adjust for SRM statistically. The bias mechanism is unknown, so correction is impossible. Fix the root cause and restart or discard the experiment.

Documentation requirements:
- Record SRM magnitude and timing
- Document investigation findings
- Note decision (restart, discard, or fix and continue)
- Update implementation to prevent recurrence

ADVANCED TOPICS

STRATIFIED RANDOMIZATION

Stratification ensures balance on important covariates. Random variation may cause imbalance in small samples. Stratified assignment eliminates this source of noise.

Common stratification variables:
- New versus returning visitors
- Device type
- Geographic region
- Traffic source
- User segment

Implementation approaches:
- Within-stratum randomization: separate random assignment per stratum
- Rerandomization: reject assignments with imbalance
- Covariate-adaptive randomization: dynamic balancing

Analysis must account for stratification. Stratified analysis pools within-stratum estimates. Efficiency gains depend on covariate predictiveness. Strong predictors can reduce variance 10 to 30 percent.

HETEROGENEOUS TREATMENT EFFECTS

Treatment effects often vary across user segments. Average effects may hide important patterns. Segment analysis reveals where treatments work best.

Pre-specified subgroup analysis:
- Define segments before experiment starts
- Separate power calculations per segment
- Interpret with appropriate multiple comparison adjustment

Exploratory subgroup analysis:
- Use machine learning to discover heterogeneity
- Causal forests, honest inference trees
- Interpret as hypothesis generating, not confirmatory

REGRESSION ADJUSTMENT

Regression adjustment uses pre-treatment covariates to reduce variance. CUPED (Controlled-experiment Using Pre-Experiment Data) is common in tech.

Basic CUPED:
- Outcome-adjusted = Outcome - theta times (Covariate - mean-covariate)
- theta = covariance(Outcome, Covariate) / variance(Covariate)
- Variance reduction proportional to R-squared

Advanced methods:
- Multiple covariates via regression
- Machine learning for covariate adjustment
- Doubly robust estimators combining regression and weighting

Covariate selection:
- Use pre-treatment variables only
- Highly predictive of outcome
- Not affected by treatment assignment
- Historical outcomes are often most predictive

EXPERIMENT DESIGN BEST PRACTICES

PRE-REGISTRATION

Document analysis plan before experiment starts:
- Primary metric and minimum detectable effect
- Sample size and duration
- Stopping rules and decision criteria
- Subgroup analyses planned

Pre-registration prevents data dredging:
- Cannot cherry-pick metrics showing significance
- Cannot change analysis method after seeing data
- Cannot redefine success criteria
- Maintains scientific integrity

IMPLEMENTATION QUALITY

Technical implementation determines experiment validity:
- Verify randomization is truly random
- Confirm bucketing is stable across sessions
- Check for interference between variations
- Monitor for technical errors during experiment

Quality checks before launch:
- AA test to verify null results under no treatment
- Smoke test with internal traffic
- SRM monitoring from day one
- Automated anomaly detection

INTERPRETING RESULTS

Avoid common interpretation errors:
- Stopping at first significant result inflates false positives
- Comparing non-significant p-values is not valid
- Confidence intervals, not p-values, indicate effect size
- Practical significance matters more than statistical significance

Holistic interpretation:
- Consider all pre-specified metrics together
- Weight by business importance
- Account for long-term effects not captured in test
- Consider implementation cost and risk

DOCUMENTATION

Experiment documentation enables learning:
- Hypothesis and business context
- Technical implementation details
- Results with full statistical details
- Decision made and rationale
- Learnings for future experiments

Build institutional knowledge:
- Searchable experiment repository
- Cross-experiment analysis
- Pattern recognition across tests
- Evidence-based decision culture

ERROR HANDLING AND VALIDATION

COMMON ISSUES

Data quality problems:
- Missing conversion data
- Duplicate user records
- Incorrect timestamp data
- Inconsistent metric definitions

Resolution approaches:
- Validate data pipeline before analysis
- Reconcile with source systems
- Document any data exclusions
- Assess impact on conclusions

SENSITIVITY ANALYSIS

Test robustness of conclusions:
- Alternative statistical methods
- Different covariate adjustments
- Excluding potential outliers
- Varying analysis windows

Report sensitivity findings:
- Core conclusions stable across approaches
- Note where results are sensitive to choices
- Recommend additional testing if uncertain
