ANL KNOWLEDGE BASE - COMPETITIVE INTELLIGENCE

VERSION INFORMATION

Document version 1.0 released January 2026. This knowledge base provides comprehensive methodology for competitive intelligence analysis including share of voice measurement, competitive spend estimation, competitive set definition, and data integration approaches.

OVERVIEW

Competitive intelligence transforms market signals into strategic advantage. Understanding competitor activity enables proactive positioning, budget calibration, and timing optimization. This methodology provides systematic approaches to measure, estimate, and interpret competitive dynamics in advertising and marketing.

Effective competitive intelligence requires both quantitative measurement and qualitative interpretation. Raw data without context misleads. This knowledge base provides frameworks for both measurement rigor and strategic interpretation.

SHARE OF VOICE METHODOLOGY

SHARE OF VOICE FUNDAMENTALS

Share of voice measures a brand's presence in the market relative to competitors. SOV can be calculated using different bases depending on available data and strategic objectives.

Impression-based SOV calculation:
- Brand SOV equals brand impressions divided by total category impressions
- Total category impressions equals sum of all competitors plus brand
- Express as percentage of total category presence
- Higher SOV indicates greater market presence

Spend-based SOV calculation:
- Brand SOV equals brand spend divided by total category spend
- Requires spend data which may be estimated for competitors
- More directly tied to investment decisions
- May differ from impression SOV due to efficiency differences

Voice-based SOV calculation:
- Measures mentions, coverage, or earned media
- Includes social mentions, press coverage, search interest
- Captures organic presence beyond paid media
- Useful for brands with significant earned presence

SOV CALCULATION BY CHANNEL

Digital display SOV calculation:
- Impressions from programmatic and direct sources
- Include both open exchange and private marketplace
- Segment by format such as banner video native
- Weight by viewability for quality-adjusted SOV

Search SOV calculation:
- Impression share from search platforms
- Segment by branded vs category keywords
- Include both paid and organic where trackable
- Consider search intent for strategic weighting

Social SOV calculation:
- Paid impressions plus organic impressions
- Include both owned and earned mentions
- Weight by engagement for quality-adjusted view
- Segment by platform for channel-specific insights

Video SOV calculation:
- Include streaming, connected TV, linear
- Normalize by duration for comparable units
- Segment by content type and daypart
- Consider completion rates for quality adjustment

CATEGORY DEFINITION

Category boundaries significantly impact SOV interpretation:
- Too narrow makes SOV artificially high
- Too broad dilutes competitive signals
- Must align with consumer decision journey
- Should reflect realistic competitive set

Category definition criteria:
- Products or services that consumers consider substitutes
- Competitors that target similar audiences
- Brands that compete for same purchase occasions
- Marketing messages that address same needs

Dynamic category boundaries:
- Categories evolve as markets change
- New entrants may expand category definition
- Category convergence may require redefinition
- Regular review of category boundaries recommended

TEMPORAL AGGREGATION

SOV time windows by use case:
- Weekly SOV for tactical monitoring
- Monthly SOV for planning cycles
- Quarterly SOV for strategic review
- Annual SOV for long-term trends

Temporal considerations:
- Seasonality affects absolute SOV levels
- Year-over-year comparison controls seasonality
- Rolling averages smooth short-term fluctuations
- Event periods may require separate analysis

DATA SOURCE RELIABILITY

Weighting by data source quality:
- First-party data receives highest weight
- Panel-based estimates receive medium weight
- Modeled estimates receive lower weight
- Self-reported competitor data receives scrutiny

Data source assessment criteria:
- Coverage of market and channels
- Methodology transparency
- Historical accuracy validation
- Update frequency and timeliness
- Sample size and representativeness

COMPETITIVE SPEND ESTIMATION

SIGNAL-BASED ESTIMATION METHODS

When direct spend data is unavailable, estimate from signals:
- Impression volume multiplied by estimated CPM
- Share of voice multiplied by category spend estimates
- Ad appearance frequency multiplied by unit costs
- Platform-reported impression share multiplied by benchmarks

Impression-to-spend conversion:
- Estimate CPM by channel and format
- Apply seasonal CPM adjustments
- Account for efficiency differences by advertiser size
- Build confidence intervals around estimates

Creative monitoring signals:
- New creative launches indicate investment
- Creative rotation frequency suggests spend level
- Production quality correlates with budget
- Media weight behind creative indicates priority

Platform signal interpretation:
- Auction pressure changes indicate spend shifts
- Impression share changes reveal budget moves
- Bid landscape data provides spend signals
- Competitive reports from platforms offer estimates

SEASONALITY ADJUSTMENT

Competitor spend varies seasonally:
- Identify competitor seasonal patterns
- Adjust estimates for expected seasonality
- Compare year-over-year same period
- Flag unexpected deviations from pattern

Seasonal adjustment methodology:
- Calculate seasonal indices from historical data
- Apply indices to normalize current period
- Compare seasonally-adjusted estimates
- Interpret raw vs adjusted for different purposes

EVENT-BASED ADJUSTMENTS

Major events affect competitor spending:
- Product launches trigger increased spend
- Promotional periods show spend spikes
- Competitive responses to your activity
- Category-wide events affect all players

Event identification and adjustment:
- Monitor competitor announcements and releases
- Track promotional calendars where visible
- Observe creative message changes
- Adjust estimates for known events

CONFIDENCE INTERVAL CONSTRUCTION

All competitor spend estimates require uncertainty bounds:
- Point estimate is most likely value
- Confidence interval shows plausible range
- Wider intervals for less reliable signals
- Narrower intervals with multiple confirming signals

Confidence interval methodology:
- Base interval on estimation method reliability
- Widen for sparse or conflicting signals
- Narrow when multiple sources agree
- Report as range not false precision

Interpreting confidence intervals:
- Narrow interval with 80 to 120 percent range indicates high confidence
- Medium interval with 60 to 140 percent range indicates moderate confidence
- Wide interval with 50 to 200 percent range indicates low confidence
- Very wide intervals suggest insufficient data

VALIDATION APPROACHES

Validate estimates against known data points:
- Compare to any disclosed competitor spending
- Cross-reference multiple estimation methods
- Check estimates against industry reports
- Validate against historical patterns

Validation best practices:
- Document validation methodology
- Track estimation accuracy over time
- Adjust methods based on validation results
- Flag estimates that fail validation checks

COMPETITIVE SET DEFINITION

DIRECT VS INDIRECT COMPETITORS

Direct competitors:
- Offer same product or service category
- Target same primary audience
- Compete for same purchase occasions
- Typically included in primary competitive set

Indirect competitors:
- Offer substitute products or services
- May target overlapping audiences
- Compete for share of wallet or attention
- Include in expanded competitive set for strategic analysis

Competitive set sizing:
- Primary set of 3 to 5 direct competitors for focused analysis
- Extended set of 10 to 15 for broader market view
- Monitor set of emerging or potential competitors
- Periodic review of set composition

MARKET SHARE CORRELATION

Competitive set should correlate with market dynamics:
- Brands that gain when you lose are competitors
- Negative correlation in share indicates competition
- Positive correlation may indicate market factors
- Neutral correlation suggests indirect relationship

Correlation analysis methodology:
- Calculate share correlation over rolling periods
- Test statistical significance of correlation
- Interpret economic mechanism behind correlation
- Use correlation to refine competitive set

COMPETITIVE INTENSITY SCORING

Score competitive intensity by competitor:
- Media overlap in targeting and placement
- Audience overlap in customer profiles
- Message similarity in positioning
- Price or value proposition proximity

Intensity score calculation:
- Weight each dimension by strategic importance
- Score each competitor on each dimension
- Calculate weighted composite score
- Rank competitors by intensity

DYNAMIC SET MANAGEMENT

Competitive sets evolve over time:
- New entrants emerge and grow
- Existing competitors may exit or decline
- Category disruption changes competitive dynamics
- Regular review and update of competitive set

Update triggers:
- Significant market share shifts
- New competitor launch in category
- Merger or acquisition activity
- Strategic repositioning by competitor
- Changes in your own positioning

DATA INTEGRATION

THIRD-PARTY DATA SOURCES

Common competitive intelligence data sources:
- Media measurement services for spend and SOV
- Social listening platforms for voice and sentiment
- Search intelligence tools for search presence
- Ad intelligence services for creative monitoring

Source evaluation criteria:
- Market coverage and completeness
- Measurement methodology quality
- Data freshness and update frequency
- Historical data availability
- Integration capabilities

DATA FRESHNESS REQUIREMENTS

Freshness requirements by use case:
- Strategic planning requires quarterly or annual data
- Tactical monitoring requires weekly or monthly data
- Competitive response requires daily or real-time data
- Trend analysis requires multi-year historical data

Staleness thresholds:
- Data over 7 days old is stale for tactical decisions
- Data over 30 days old is stale for planning
- Data over 90 days old is stale for strategy
- Always note data currency when presenting

CROSS-SOURCE RECONCILIATION

When using multiple data sources:
- Identify definitional differences between sources
- Normalize to common definitions where possible
- Weight by source reliability when aggregating
- Flag irreconcilable differences

Reconciliation methodology:
- Map taxonomy across sources
- Calculate conversion factors for normalization
- Test reconciliation against known values
- Document reconciliation approach

MISSING DATA IMPUTATION

When data is incomplete:
- Identify pattern of missingness
- Determine if missing at random or systematic
- Apply appropriate imputation method
- Flag imputed values in output

Imputation methods:
- Last observation carried forward for time series
- Category average for sporadic missing
- Model-based imputation for systematic patterns
- Multiple imputation for uncertainty quantification

STRATEGIC INTERPRETATION

SOV VS SHARE OF MARKET ANALYSIS

The SOV to SOM relationship provides strategic guidance:
- SOV greater than SOM suggests investment in growth
- SOV less than SOM suggests harvesting or efficiency focus
- SOV equals SOM suggests share maintenance
- Gap size indicates strategic intensity

Excess SOV implications:
- Building awareness for future share gain
- Defending against competitive attack
- Launching new products or entering new segments
- May indicate inefficient spending if sustained without share gain

Deficit SOV implications:
- Relying on brand equity and efficiency
- May indicate vulnerability to attack
- Sustainable if brand strength is high
- Risk of share erosion over time

COMPETITIVE RESPONSE PATTERNS

Identify competitor response tendencies:
- Aggressive responders increase spend when challenged
- Passive competitors maintain steady investment
- Opportunistic competitors exploit gaps
- Strategic competitors match investments selectively

Response pattern analysis:
- Track competitor reactions to your initiatives
- Build response models by competitor
- Anticipate likely responses to planned actions
- Factor expected responses into planning

TREND INTERPRETATION

Interpreting SOV trends:
- Rising SOV indicates increased investment or competitor retreat
- Falling SOV indicates reduced investment or competitor surge
- Stable SOV indicates equilibrium
- Volatile SOV indicates competitive instability

Trend significance assessment:
- Distinguish signal from noise in trend data
- Apply statistical trend tests
- Consider business context for interpretation
- Project trends with appropriate uncertainty

OUTPUT SPECIFICATIONS

SOV ANALYSIS OUTPUT

SOV analysis output structure:
- sov_summary containing overall brand SOV percentage and trend
- sov_by_channel breaking down by media channel
- sov_by_period showing temporal breakdown
- competitor_breakdown with each competitor SOV and trend
- insights highlighting key observations
- recommendations suggesting strategic actions

SPEND ESTIMATION OUTPUT

Spend estimation output structure:
- competitor as competitor identifier
- estimated_total_spend as point estimate
- confidence_interval with low and high bounds
- spend_by_channel as breakdown by media channel
- methodology_notes explaining estimation approach
- data_quality_score indicating reliability

COMPETITIVE SET OUTPUT

Competitive set output structure:
- primary_competitors as core competitive set
- secondary_competitors as extended set
- intensity_scores by competitor
- correlation_analysis showing share correlations
- set_recommendations for additions or removals

BEST PRACTICES

MEASUREMENT BEST PRACTICES

Consistent methodology over time:
- Maintain measurement approach for comparability
- Document methodology changes with impact
- Restate historical data when methodology changes
- Avoid false precision in volatile metrics

Appropriate granularity:
- Match measurement granularity to decision needs
- Higher granularity for tactical, lower for strategic
- Cost of granularity increases data requirements
- Balance detail with reliability

INTERPRETATION BEST PRACTICES

Context before conclusions:
- Always interpret data in business context
- Consider multiple explanations for patterns
- Validate interpretations against other evidence
- Avoid overreacting to short-term fluctuations

Competitive humility:
- Acknowledge uncertainty in competitor estimates
- Recognize limits of available intelligence
- Update beliefs when new information arrives
- Distinguish facts from inferences

COMMUNICATION BEST PRACTICES

Clear uncertainty communication:
- Always report confidence levels
- Use ranges not false point estimates
- Distinguish known from estimated
- Explain methodology at appropriate level

Actionable framing:
- Connect intelligence to decisions
- Highlight implications not just data
- Recommend actions based on findings
- Prioritize insights by strategic impact

ERROR HANDLING

When data is insufficient:
- Return partial analysis with clear limitations
- Indicate which competitors could not be analyzed
- Recommend data collection to fill gaps
- Provide directional guidance despite gaps

When estimates are unreliable:
- Widen confidence intervals appropriately
- Flag low-confidence estimates clearly
- Recommend validation before action
- Suggest alternative analysis approaches
